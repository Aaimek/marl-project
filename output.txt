States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3861, -1.9250],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0502,  4.3465,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0502, -4.3465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4264, -0.0697],
        [-0.1470, -0.1480],
        [ 0.0843,  0.1548]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0712],
        [0.2825],
        [0.1862]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2124],
        [ 0.0132],
        [ 0.0360]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3861, -1.9250],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0502,  4.3465,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0502, -4.3465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4264, -0.0697],
        [-0.1470, -0.1480],
        [ 0.0843,  0.1548]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1633],
        [0.4040],
        [0.2523]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1818],
        [-0.1582],
        [-0.2901]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3861, -1.9250],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0502,  4.3465,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0502, -4.3465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4264, -0.0697],
        [-0.1470, -0.1480],
        [ 0.0843,  0.1548]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1092],
        [ 0.2743],
        [-0.1804]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0511],
        [ 0.2166],
        [ 0.4873]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3902, -0.0570],
        [-0.1532, -0.1524],
        [ 0.0558,  0.1642]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0186],
        [0.2512],
        [0.1557]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1667],
        [0.0107],
        [0.0361]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3902, -0.0570],
        [-0.1532, -0.1524],
        [ 0.0558,  0.1642]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1083],
        [0.3705],
        [0.2129]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0151],
        [-0.1559],
        [-0.2921]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.9597, -1.8553],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2815,  4.6494,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2815, -4.6494,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3902, -0.0570],
        [-0.1532, -0.1524],
        [ 0.0558,  0.1642]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0606],
        [ 0.2806],
        [-0.1511]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1895],
        [0.2296],
        [0.4854]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.6643, -0.1125],
        [-0.1611, -0.1579],
        [ 0.0300,  0.1743]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4121],
        [0.2133],
        [0.1218]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1978],
        [0.0549],
        [0.0549]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.6643, -0.1125],
        [-0.1611, -0.1579],
        [ 0.0300,  0.1743]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2130],
        [ 0.3288],
        [ 0.1647]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0125],
        [-0.0556],
        [-0.0556]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9249, -2.7885,
          0.0000,  0.0000, -2.5694, -1.7983],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4904,  4.9660,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4904, -4.9660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.6643, -0.1125],
        [-0.1611, -0.1579],
        [ 0.0300,  0.1743]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.3001],
        [ 0.2948],
        [-0.1159]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2131],
        [0.1816],
        [0.1816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5813, -0.0762],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.3242],
        [-0.0198],
        [-0.0156]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2180],
        [0.0549],
        [0.0549]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5813, -0.0762],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1709],
        [-0.0707],
        [-0.0811]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0043],
        [-0.0556],
        [-0.0556]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.2606, -2.6760,
          0.0000,  0.0000, -1.9051, -1.6857],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5813, -0.0762],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2333],
        [-0.0143],
        [-0.0128]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2458],
        [0.1816],
        [0.1816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4926, -0.0552],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2537],
        [-0.0264],
        [-0.0220]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2127],
        [0.0549],
        [0.0549]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4926, -0.0552],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1025],
        [-0.0775],
        [-0.0876]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0123],
        [-0.0556],
        [-0.0556]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6793, -2.5997,
          0.0000,  0.0000, -1.3238, -1.6095],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4926, -0.0552],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1615],
        [-0.0071],
        [-0.0058]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2708],
        [0.1816],
        [0.1816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4137, -0.0414],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2051],
        [-0.0312],
        [-0.0267]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1926],
        [0.0549],
        [0.0549]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4137, -0.0414],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0457],
        [-0.0824],
        [-0.0923]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0347],
        [-0.0556],
        [-0.0556]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1867, -2.5446,
          0.0000,  0.0000, -0.8312, -1.5543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4137, -0.0414],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1386],
        [0.0008],
        [0.0019]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3036],
        [0.1816],
        [0.1816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3668,  0.0167],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1640],
        [-0.0344],
        [-0.0298]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2834],
        [ 0.0549],
        [-0.1798]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3668,  0.0167],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0042],
        [-0.0861],
        [-0.0959]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0935],
        [-0.0556],
        [ 0.0368]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7729, -2.5032,
          0.0000,  0.0000, -0.4174, -1.5129],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3668,  0.0167],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Q-values: tensor([[0.1331],
        [0.0092],
        [0.0101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2161],
        [0.1816],
        [0.7123]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Actions: tensor([[-0.4526, -0.1988],
        [ 0.0606,  0.1236],
        [-0.0222, -0.2466]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0117],
        [-0.0369],
        [-0.0179]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2059],
        [ 0.0549],
        [-0.1872]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Actions: tensor([[-0.4526, -0.1988],
        [ 0.0606,  0.1236],
        [-0.0222, -0.2466]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2058],
        [-0.0890],
        [ 0.5358]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1252],
        [-0.0556],
        [ 0.0405]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.4062,  4.4544, -0.4062, -2.5198,
          0.0000,  0.0000, -0.0507, -1.5296],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4062, -4.4544,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  1.9166,  0.0000,  0.0000]])
Actions: tensor([[-0.4526, -0.1988],
        [ 0.0606,  0.1236],
        [-0.0222, -0.2466]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Q-values: tensor([[0.2549],
        [0.0182],
        [0.2132]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2178],
        [0.1816],
        [0.7473]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Actions: tensor([[-0.4112, -0.1353],
        [ 0.0606,  0.1236],
        [-0.0319, -0.2437]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0249],
        [-0.0388],
        [-0.0197]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1956],
        [ 0.0549],
        [-0.2055]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Actions: tensor([[-0.4112, -0.1353],
        [ 0.0606,  0.1236],
        [-0.0319, -0.2437]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2062],
        [-0.0944],
        [ 0.4958]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1283],
        [-0.0556],
        [ 0.0526]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4066,  0.0000, -2.3210,
          0.0000,  0.0000,  0.3555, -1.3308],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.4066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.1632,  0.0000,  0.0000]])
Actions: tensor([[-0.4112, -0.1353],
        [ 0.0606,  0.1236],
        [-0.0319, -0.2437]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Q-values: tensor([[0.1664],
        [0.0269],
        [0.2813]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2113],
        [0.1816],
        [0.7751]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Actions: tensor([[-0.4048, -0.1375],
        [ 0.0606,  0.1236],
        [-0.0297, -0.2531]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0179],
        [-0.0404],
        [-0.0763]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1852],
        [ 0.0549],
        [-0.2233]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Actions: tensor([[-0.4048, -0.1375],
        [ 0.0606,  0.1236],
        [-0.0297, -0.2531]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1387],
        [-0.1008],
        [ 0.4451]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1313],
        [-0.0556],
        [ 0.0635]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2982,  0.0000, -2.1857,
          0.0000,  0.0000,  0.3555, -1.1955],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.2982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.4069,  0.0000,  0.0000]])
Actions: tensor([[-0.4048, -0.1375],
        [ 0.0606,  0.1236],
        [-0.0297, -0.2531]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Q-values: tensor([[0.1827],
        [0.0358],
        [0.3551]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2018],
        [0.1816],
        [0.8006]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Actions: tensor([[-0.3977, -0.1394],
        [ 0.0606,  0.1236],
        [-0.0327, -0.2645]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Q-values: tensor([[ 0.0170],
        [-0.0415],
        [-0.1335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1750],
        [ 0.0549],
        [-0.6240]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Actions: tensor([[-0.3977, -0.1394],
        [ 0.0606,  0.1236],
        [-0.0327, -0.2645]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Q-values: tensor([[ 0.0699],
        [-0.1078],
        [ 0.3900]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1336],
        [-0.0556],
        [-0.1207]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1826,  0.0000, -2.0483,
          0.0000,  0.0000,  0.3555, -1.0580],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.1826,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1568,  2.6601,  0.0000,  0.0000]])
Actions: tensor([[-0.3977, -0.1394],
        [ 0.0606,  0.1236],
        [-0.0327, -0.2645]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Q-values: tensor([[0.2009],
        [0.0447],
        [0.4388]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1933],
        [0.1816],
        [0.2120]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Actions: tensor([[-0.3912, -0.1421],
        [ 0.0606,  0.1236],
        [ 0.1254,  0.0098]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Q-values: tensor([[ 0.0172],
        [-0.0430],
        [ 0.3628]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1609],
        [ 0.0549],
        [-0.6818]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Actions: tensor([[-0.3912, -0.1421],
        [ 0.0606,  0.1236],
        [ 0.1254,  0.0098]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Q-values: tensor([[ 0.0024],
        [-0.1152],
        [ 0.3884]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1364],
        [-0.0556],
        [ 0.3350]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0575,  0.0000, -1.9089,
          0.0000,  0.0000,  0.3555, -0.9186],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9761]])
Actions: tensor([[-0.3912, -0.1421],
        [ 0.0606,  0.1236],
        [ 0.1254,  0.0098]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Q-values: tensor([[0.2119],
        [0.0523],
        [0.7324]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1794],
        [0.1816],
        [0.5643]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Actions: tensor([[-0.3964, -0.1542],
        [ 0.0606,  0.1236],
        [-0.3895, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Q-values: tensor([[ 0.0159],
        [-0.0456],
        [ 0.1361]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1431],
        [ 0.0549],
        [-0.6401]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Actions: tensor([[-0.3964, -0.1542],
        [ 0.0606,  0.1236],
        [-0.3895, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Q-values: tensor([[-0.0582],
        [-0.1221],
        [ 0.7503]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1498],
        [-0.0556],
        [-0.1296]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1254,  4.2094,  0.0000, -1.7668,
          0.0000,  0.0000,  0.3555, -0.7765],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1254, -4.2094,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0314,  2.9147,  0.2301, -4.9860]])
Actions: tensor([[-0.3964, -0.1542],
        [ 0.0606,  0.1236],
        [-0.3895, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Q-values: tensor([[0.2286],
        [0.0587],
        [1.1295]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1880],
        [0.1816],
        [0.2214]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Actions: tensor([[-0.4111, -0.1720],
        [ 0.0606,  0.1236],
        [ 0.1318,  0.0011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Q-values: tensor([[ 0.0037],
        [-0.0484],
        [ 0.3047]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1243],
        [ 0.0549],
        [-0.6903]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Actions: tensor([[-0.4111, -0.1720],
        [ 0.0606,  0.1236],
        [ 0.1318,  0.0011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Q-values: tensor([[-0.1347],
        [-0.1294],
        [ 0.3043]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1530],
        [-0.0556],
        [ 0.3422]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3315,  0.0000, -1.6126,
          0.0000,  0.0000,  0.3555, -0.6223],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.3315,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9538]])
Actions: tensor([[-0.4111, -0.1720],
        [ 0.0606,  0.1236],
        [ 0.1318,  0.0011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Q-values: tensor([[0.2526],
        [0.0616],
        [0.7664]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1894],
        [0.1816],
        [0.5693]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Actions: tensor([[-0.4206, -0.1832],
        [ 0.0606,  0.1236],
        [-0.3755, -0.0525]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Q-values: tensor([[-0.0080],
        [-0.0517],
        [ 0.0369]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1024],
        [ 0.0549],
        [-0.6569]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Actions: tensor([[-0.4206, -0.1832],
        [ 0.0606,  0.1236],
        [-0.3755, -0.0525]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Q-values: tensor([[-0.2034],
        [-0.1356],
        [ 0.6051]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1647],
        [-0.0556],
        [-0.1350]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1318,  4.5046,  0.0000, -1.4406,
          0.0000,  0.0000,  0.3555, -0.4504],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1318, -4.5046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.0250,  2.9457,  0.2237, -4.9549]])
Actions: tensor([[-0.4206, -0.1832],
        [ 0.0606,  0.1236],
        [-0.3755, -0.0525]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Q-values: tensor([[0.2644],
        [0.0639],
        [1.1040]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2034],
        [0.1816],
        [0.2401]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Actions: tensor([[-0.4493, -0.2061],
        [ 0.0606,  0.1236],
        [ 0.1426, -0.0043]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Q-values: tensor([[-0.0229],
        [-0.0550],
        [ 0.2170]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0815],
        [ 0.0549],
        [-0.6701]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Actions: tensor([[-0.4493, -0.2061],
        [ 0.0606,  0.1236],
        [ 0.1426, -0.0043]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Q-values: tensor([[-0.2758],
        [-0.1420],
        [ 0.2139]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1672],
        [-0.0556],
        [-0.1246]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.6353,  0.0000, -1.2574,
          0.0000,  0.0000,  0.3555, -0.2672],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -4.6353,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3555, -4.9025]])
Actions: tensor([[-0.4493, -0.2061],
        [ 0.0606,  0.1236],
        [ 0.1426, -0.0043]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Q-values: tensor([[0.2850],
        [0.0644],
        [0.7353]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2209],
        [0.1816],
        [0.2404]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Actions: tensor([[-0.4639, -0.2244],
        [ 0.0606,  0.1236],
        [ 0.1560, -0.0060]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8269,
          0.0000,  0.0000,  0.3555,  0.1633],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.8582,  3.0085,  0.0569, -4.8922]])
Q-values: tensor([[-0.0344],
        [-0.0585],
        [ 0.1687]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Actions: tensor([[-0.4639, -0.2244],
        [ 0.0606,  0.1236],
        [ 0.1560, -0.0060]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8269,
          0.0000,  0.0000,  0.3555,  0.1633],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.8582,  3.0085,  0.0569, -4.8922]])
Q-values: tensor([[-0.3385],
        [-0.1480],
        [ 0.1618]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1426,  4.8371,  0.0000, -1.0514,
          0.0000,  0.0000,  0.3555, -0.0611],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1426, -4.8371,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2129, -4.8982]])
Actions: tensor([[-0.4639, -0.2244],
        [ 0.0606,  0.1236],
        [ 0.1560, -0.0060]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8269,
          0.0000,  0.0000,  0.3555,  0.1633],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.8582,  3.0085,  0.0569, -4.8922]])
Q-values: tensor([[0.2824],
        [0.0640],
        [0.6987]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
Episode 1: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.3848,  0.2617],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6000, -3.3011,
          1.1207,  4.2315,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7548, -0.8715,  0.0000,  0.0000]])
Actions: tensor([[-0.1998, -0.0140],
        [ 0.0482,  0.0685],
        [ 0.1577,  0.0785]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Q-values: tensor([[-0.0567],
        [-0.1135],
        [-0.0402]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0375],
        [-0.0648],
        [-0.0085]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.3848,  0.2617],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6000, -3.3011,
          1.1207,  4.2315,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7548, -0.8715,  0.0000,  0.0000]])
Actions: tensor([[-0.1998, -0.0140],
        [ 0.0482,  0.0685],
        [ 0.1577,  0.0785]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Q-values: tensor([[-0.2016],
        [ 0.0879],
        [-0.1906]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1796],
        [ 0.0765],
        [-0.1925]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.3848,  0.2617],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6000, -3.3011,
          1.1207,  4.2315,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7548, -0.8715,  0.0000,  0.0000]])
Actions: tensor([[-0.1998, -0.0140],
        [ 0.0482,  0.0685],
        [ 0.1577,  0.0785]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Q-values: tensor([[0.0794],
        [0.0993],
        [0.0502]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0648],
        [0.2044],
        [0.0518]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Actions: tensor([[-0.1895,  0.0039],
        [ 0.0852,  0.1300],
        [ 0.1705,  0.0809]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Q-values: tensor([[-0.0354],
        [-0.0560],
        [-0.0335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0255],
        [-0.0398],
        [ 0.0015]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Actions: tensor([[-0.1895,  0.0039],
        [ 0.0852,  0.1300],
        [ 0.1705,  0.0809]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Q-values: tensor([[-0.1992],
        [ 0.0707],
        [-0.2041]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1776],
        [ 0.0666],
        [-0.2087]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1850,  0.2756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.2162, -4.2751,
          1.0724,  4.1630,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5971, -0.9500,  0.0000,  0.0000]])
Actions: tensor([[-0.1895,  0.0039],
        [ 0.0852,  0.1300],
        [ 0.1705,  0.0809]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Q-values: tensor([[0.0789],
        [0.1753],
        [0.0549]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0719],
        [0.2053],
        [0.0563]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Actions: tensor([[-0.1708,  0.0205],
        [ 0.0821,  0.1374],
        [ 0.1885,  0.0856]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Q-values: tensor([[-0.0208],
        [-0.0316],
        [-0.0257]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0157],
        [-0.1945],
        [ 0.0117]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Actions: tensor([[-0.1708,  0.0205],
        [ 0.0821,  0.1374],
        [ 0.1885,  0.0856]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Q-values: tensor([[-0.1929],
        [ 0.0549],
        [-0.2165]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1749],
        [-0.1580],
        [-0.2222]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0045,  0.2718],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1525, -4.4051,
          0.9872,  4.0330,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4266, -1.0309,  0.0000,  0.0000]])
Actions: tensor([[-0.1708,  0.0205],
        [ 0.0821,  0.1374],
        [ 0.1885,  0.0856]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Q-values: tensor([[0.0826],
        [0.1904],
        [0.0624]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0802],
        [0.3176],
        [0.0630]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Actions: tensor([[-0.1551,  0.0347],
        [ 0.2435,  0.0440],
        [ 0.2044,  0.0807]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Q-values: tensor([[-0.0092],
        [-0.1919],
        [-0.0247]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0095],
        [ 0.0020],
        [ 0.0172]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Actions: tensor([[-0.1551,  0.0347],
        [ 0.2435,  0.0440],
        [ 0.2044,  0.0807]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Q-values: tensor([[-0.1833],
        [-0.1866],
        [-0.2325]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1722],
        [ 0.0437],
        [-0.2419]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.1753,  0.2513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.9051,  3.8956,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2381, -1.1165,  0.0000,  0.0000]])
Actions: tensor([[-0.1551,  0.0347],
        [ 0.2435,  0.0440],
        [ 0.2044,  0.0807]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Q-values: tensor([[0.0924],
        [0.3393],
        [0.0722]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0890],
        [0.2127],
        [0.0739]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Actions: tensor([[-0.1420,  0.0551],
        [ 0.0675,  0.1727],
        [ 0.2207,  0.0780]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Q-values: tensor([[-0.0013],
        [ 0.0037],
        [-0.0265]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0122],
        [-0.1847],
        [ 0.0239]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Actions: tensor([[-0.1420,  0.0551],
        [ 0.0675,  0.1727],
        [ 0.2207,  0.0780]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Q-values: tensor([[-0.1723],
        [ 0.0120],
        [-0.2526]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1625],
        [-0.1767],
        [-0.2670]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3304,  0.2165],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9064, -4.5864,
          0.6615,  3.8517,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0338, -1.1972,  0.0000,  0.0000]])
Actions: tensor([[-0.1420,  0.0551],
        [ 0.0675,  0.1727],
        [ 0.2207,  0.0780]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Q-values: tensor([[0.0992],
        [0.2300],
        [0.0863]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0972],
        [0.3060],
        [0.0853]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Actions: tensor([[-0.1245,  0.0748],
        [ 0.1957,  0.0349],
        [ 0.2383,  0.0741]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Q-values: tensor([[-0.0026],
        [-0.1797],
        [-0.0279]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0161],
        [ 0.0393],
        [ 0.0274]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Actions: tensor([[-0.1245,  0.0748],
        [ 0.1957,  0.0349],
        [ 0.2383,  0.0741]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Q-values: tensor([[-0.1620],
        [-0.2122],
        [-0.2699]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1499],
        [ 0.0127],
        [-0.2878]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4724,  0.1614],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.5940,  3.6790,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1869, -1.2752,  0.0000,  0.0000]])
Actions: tensor([[-0.1245,  0.0748],
        [ 0.1957,  0.0349],
        [ 0.2383,  0.0741]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Q-values: tensor([[0.1028],
        [0.3241],
        [0.1001]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1011],
        [0.2546],
        [0.1007]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Actions: tensor([[-0.1061,  0.0934],
        [ 0.0973,  0.2609],
        [ 0.2490,  0.0708]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Q-values: tensor([[-0.0062],
        [ 0.0140],
        [-0.0265]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0181],
        [-0.2549],
        [-0.2029]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Actions: tensor([[-0.1061,  0.0934],
        [ 0.0973,  0.2609],
        [ 0.2490,  0.0708]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Q-values: tensor([[-0.1444],
        [-0.0301],
        [-0.2882]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1380],
        [-0.0783],
        [-0.4661]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.5969,  0.0866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.2483, -4.7940,
          0.3983,  3.6441,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4252, -1.3493,  0.0000,  0.0000]])
Actions: tensor([[-0.1061,  0.0934],
        [ 0.0973,  0.2609],
        [ 0.2490,  0.0708]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Q-values: tensor([[0.1037],
        [0.2975],
        [0.1188]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1067],
        [0.4152],
        [0.2108]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Actions: tensor([[-0.0909,  0.1144],
        [ 0.2821,  0.0125],
        [ 0.0797,  0.2351]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0109],
        [-0.2135],
        [-0.2521]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0220],
        [-0.1591],
        [ 0.0017]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Actions: tensor([[-0.0909,  0.1144],
        [ 0.2821,  0.0125],
        [ 0.0797,  0.2351]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1311],
        [ 0.0177],
        [-0.4103]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1270],
        [-0.2149],
        [-0.1701]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7030, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  4.8033,  0.0000,  0.0000,
          0.3010,  3.3832,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9752, -4.8033,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6742, -1.4201,  0.0000,  0.0000]])
Actions: tensor([[-0.0909,  0.1144],
        [ 0.2821,  0.0125],
        [ 0.0797,  0.2351]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1082],
        [0.4932],
        [0.2617]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1113],
        [0.2840],
        [0.1002]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0755,  0.1324],
        [ 0.1108,  0.0175],
        [ 0.1561,  0.0748]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0167],
        [-0.1509],
        [-0.0507]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0244],
        [-0.1552],
        [ 0.0055]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0755,  0.1324],
        [ 0.1108,  0.0175],
        [ 0.1561,  0.0748]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1200],
        [-0.2478],
        [-0.1686]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1146],
        [-0.2234],
        [-0.2174]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.7939, -0.1212],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0189,  3.3707,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6149,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0755,  0.1324],
        [ 0.1108,  0.0175],
        [ 0.1561,  0.0748]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1132],
        [0.2982],
        [0.1085]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1178],
        [0.2811],
        [0.1711]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0647,  0.1479],
        [ 0.0964,  0.0132],
        [ 0.1595,  0.1052]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0213],
        [-0.1446],
        [-0.0571]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0229],
        [ 0.0871],
        [ 0.2687]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0647,  0.1479],
        [ 0.0964,  0.0132],
        [ 0.1595,  0.1052]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1079],
        [-0.2569],
        [-0.1954]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1014],
        [-0.5621],
        [-0.4532]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.8694, -0.2535],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0920,  3.3532,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3127, -0.0748,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0647,  0.1479],
        [ 0.0964,  0.0132],
        [ 0.1595,  0.1052]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1172],
        [0.2938],
        [0.1784]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1232],
        [0.2960],
        [0.7692]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0602,  0.1650],
        [-0.2111,  0.5094],
        [ 0.6722,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0209],
        [ 0.0399],
        [ 0.1162]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0180],
        [-0.1430],
        [-0.0083]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0602,  0.1650],
        [-0.2111,  0.5094],
        [ 0.6722,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0964],
        [-0.6967],
        [-0.4559]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0885],
        [-0.1898],
        [-0.4235]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9340, -0.4015],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8812, -4.9181,  0.0000,  0.0000,
         -0.1884,  3.3400,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8812,  4.9181,  0.0000,  0.0000, -2.3297, -0.1800,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0602,  0.1650],
        [-0.2111,  0.5094],
        [ 0.6722,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1171],
        [0.1609],
        [0.7916]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1215],
        [0.2497],
        [0.5188]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0522,  0.1831],
        [ 0.1052,  0.0422],
        [ 0.1703,  0.2175]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0195],
        [-0.1344],
        [-0.1180]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0153],
        [-0.1384],
        [-0.0045]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0522,  0.1831],
        [ 0.1052,  0.0422],
        [ 0.1703,  0.2175]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0826],
        [-0.2230],
        [-0.3727]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0713],
        [-0.1966],
        [-0.4639]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9942, -0.5664],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0227,  2.8306,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0138, -0.1970,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0522,  0.1831],
        [ 0.1052,  0.0422],
        [ 0.1703,  0.2175]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1150],
        [0.2591],
        [0.5248]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1189],
        [0.2451],
        [0.5755]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0441,  0.1988],
        [ 0.0926,  0.0393],
        [ 0.1642,  0.2008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0208],
        [-0.1277],
        [-0.1066]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0136],
        [-0.1340],
        [ 0.0008]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0441,  0.1988],
        [ 0.0926,  0.0393],
        [ 0.1642,  0.2008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0656],
        [-0.2264],
        [-0.4086]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0544],
        [-0.2018],
        [-0.4883]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0464, -0.7495],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0825,  2.7884,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3716, -0.4144,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0441,  0.1988],
        [ 0.0926,  0.0393],
        [ 0.1642,  0.2008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1067],
        [0.2551],
        [0.5847]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1162],
        [0.2410],
        [0.6031]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0368,  0.2121],
        [ 0.0811,  0.0383],
        [ 0.1581,  0.1772]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0228],
        [-0.1212],
        [-0.0881]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0110],
        [-0.1300],
        [ 0.0058]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0368,  0.2121],
        [ 0.0811,  0.0383],
        [ 0.1581,  0.1772]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0493],
        [-0.2283],
        [-0.4341]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0410],
        [-0.2061],
        [-0.5095]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.0905, -0.9483],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1751,  2.7490,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5358, -0.6152,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0368,  0.2121],
        [ 0.0811,  0.0383],
        [ 0.1581,  0.1772]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0975],
        [0.2518],
        [0.6175]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1116],
        [0.2372],
        [0.6294]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0349,  0.2267],
        [ 0.0714,  0.0379],
        [ 0.1553,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0230],
        [-0.1157],
        [-0.0637]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0086],
        [-0.1262],
        [ 0.0102]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0349,  0.2267],
        [ 0.0714,  0.0379],
        [ 0.1553,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0374],
        [-0.2297],
        [-0.4631]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0283],
        [-0.2097],
        [-0.5293]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1273, -1.1604],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2562,  2.7108,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6940, -0.7923,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0349,  0.2267],
        [ 0.0714,  0.0379],
        [ 0.1553,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0861],
        [0.2480],
        [0.6494]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1068],
        [0.2336],
        [0.6566]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0340,  0.2444],
        [ 0.0615,  0.0374],
        [ 0.1540,  0.1390]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0241],
        [-0.1106],
        [-0.0384]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0063],
        [-0.1241],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0340,  0.2444],
        [ 0.0615,  0.0374],
        [ 0.1540,  0.1390]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0255],
        [-0.2308],
        [-0.4932]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0138],
        [-0.2126],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1623, -1.3871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3277,  2.6729,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.8493, -0.9491,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0340,  0.2444],
        [ 0.0615,  0.0374],
        [ 0.1540,  0.1390]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0731],
        [0.2446],
        [0.6829]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1001],
        [0.2308],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370,  0.2626],
        [ 0.0529,  0.0371],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0262],
        [-0.1061],
        [-0.0347]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0041],
        [-0.1222],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370,  0.2626],
        [ 0.0529,  0.0371],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0112],
        [-0.2299],
        [-0.1431]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-1.8751e-04],
        [-2.1488e-01],
        [-1.4473e-01]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.1963, -1.6315],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3891,  2.6355,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370,  0.2626],
        [ 0.0529,  0.0371],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0548],
        [0.2404],
        [0.0830]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0915],
        [0.2284],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0393,  0.2782],
        [ 0.0455,  0.0367],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0284],
        [-0.1026],
        [-0.0338]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0060],
        [-0.1205],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0393,  0.2782],
        [ 0.0455,  0.0367],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0038],
        [-0.2286],
        [-0.1420]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0126],
        [-0.2166],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2333, -1.8940],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4420,  2.5984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0393,  0.2782],
        [ 0.0455,  0.0367],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0362],
        [0.2363],
        [0.0829]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0868],
        [0.2260],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0428,  0.2926],
        [ 0.0395,  0.0373],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0323],
        [-0.1002],
        [-0.0331]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0092],
        [-0.1189],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0428,  0.2926],
        [ 0.0395,  0.0373],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0196],
        [-0.2276],
        [-0.1410]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0257],
        [-0.2182],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.2726, -2.1722],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4875,  2.5617,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0428,  0.2926],
        [ 0.0395,  0.0373],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0221],
        [0.2324],
        [0.0828]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0866],
        [0.2235],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0466,  0.3105],
        [ 0.0346,  0.0385],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0389],
        [-0.0982],
        [-0.0323]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0118],
        [-0.1173],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0466,  0.3105],
        [ 0.0346,  0.0385],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0352],
        [-0.2263],
        [-0.1401]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0423],
        [-0.2197],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3154, -2.4647],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5270,  2.5244,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0466,  0.3105],
        [ 0.0346,  0.0385],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0074],
        [0.2286],
        [0.0828]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0843],
        [0.2210],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0557,  0.3296],
        [ 0.0306,  0.0400],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0460],
        [-0.0965],
        [-0.0317]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0145],
        [-0.1158],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0557,  0.3296],
        [ 0.0306,  0.0400],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0510],
        [-0.2246],
        [-0.1392]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0569],
        [-0.2207],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.3620, -2.7752],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5616,  2.4859,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0557,  0.3296],
        [ 0.0306,  0.0400],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0079],
        [ 0.2248],
        [ 0.0828]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0817],
        [0.2181],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0644,  0.3485],
        [ 0.0271,  0.0416],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0530],
        [-0.0951],
        [-0.0311]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0165],
        [-0.1142],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0644,  0.3485],
        [ 0.0271,  0.0416],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0676],
        [-0.2230],
        [-0.1385]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0692],
        [-0.2214],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4176, -3.1048],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5922,  2.4459,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0644,  0.3485],
        [ 0.0271,  0.0416],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0233],
        [ 0.2212],
        [ 0.0829]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0795],
        [0.2147],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0737,  0.3694],
        [ 0.0239,  0.0432],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0583],
        [-0.0939],
        [-0.0305]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0177],
        [-0.1126],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0737,  0.3694],
        [ 0.0239,  0.0432],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0806],
        [-0.2212],
        [-0.1378]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0811],
        [-0.2219],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4820, -3.4533],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6193,  2.4043,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0737,  0.3694],
        [ 0.0239,  0.0432],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0374],
        [ 0.2177],
        [ 0.0830]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0820],
        [0.2113],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0831,  0.3920],
        [ 0.0211,  0.0449],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0639],
        [-0.0927],
        [-0.0299]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0269],
        [-0.1110],
        [-0.0975]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0831,  0.3920],
        [ 0.0211,  0.0449],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0913],
        [-0.2190],
        [-0.1371]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2512],
        [-0.2221],
        [-0.3146]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5557, -3.8227],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6432,  2.3611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0831,  0.3920],
        [ 0.0211,  0.0449],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0467],
        [ 0.2145],
        [ 0.0833]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1864],
        [0.2078],
        [0.0773]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4682, -0.0484],
        [ 0.0185,  0.0466],
        [ 0.1343, -0.0602]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0381],
        [-0.0916],
        [-0.1025]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0163],
        [-0.1094],
        [-0.1022]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4682, -0.0484],
        [ 0.0185,  0.0466],
        [ 0.1343, -0.0602]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1995],
        [-0.2174],
        [-0.3301]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1885],
        [-0.2215],
        [-0.2698]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.6803,  1.1367,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6642,  2.3162,  0.0000,  0.0000],
        [ 1.6803, -1.1367,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4682, -0.0484],
        [ 0.0185,  0.0466],
        [ 0.1343, -0.0602]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2397],
        [0.2117],
        [0.0978]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1450],
        [0.2041],
        [0.0851]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3703, -0.0829],
        [ 0.0163,  0.0484],
        [ 0.1302, -0.0696]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0495],
        [-0.0907],
        [-0.1098]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0174],
        [-0.1077],
        [-0.0982]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3703, -0.0829],
        [ 0.0163,  0.0484],
        [ 0.1302, -0.0696]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1450],
        [-0.2158],
        [-0.2804]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1565],
        [-0.2215],
        [-0.2282]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.0778,  1.0765,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6828,  2.2696,  0.0000,  0.0000],
        [ 1.0778, -1.0765,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3703, -0.0829],
        [ 0.0163,  0.0484],
        [ 0.1302, -0.0696]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1963],
        [0.2082],
        [0.0973]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0972],
        [0.2004],
        [0.1040]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2888, -0.0948],
        [ 0.0143,  0.0503],
        [ 0.1413, -0.0643]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0501],
        [-0.0900],
        [-0.1277]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0211],
        [-0.1060],
        [-0.0911]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2888, -0.0948],
        [ 0.0143,  0.0503],
        [ 0.1413, -0.0643]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1100],
        [-0.2140],
        [-0.2333]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1231],
        [-0.2217],
        [-0.1830]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.5773,  1.0070,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6991,  2.2211,  0.0000,  0.0000],
        [ 0.5773, -1.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2888, -0.0948],
        [ 0.0143,  0.0503],
        [ 0.1413, -0.0643]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1567],
        [0.2046],
        [0.1091]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0592],
        [0.1964],
        [0.1181]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2197, -0.0789],
        [ 0.0124,  0.0523],
        [ 0.1457, -0.0430]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0474],
        [-0.0894],
        [-0.1245]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0268],
        [-0.1042],
        [-0.0840]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2197, -0.0789],
        [ 0.0124,  0.0523],
        [ 0.1457, -0.0430]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0823],
        [-0.2119],
        [-0.1963]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1067],
        [-0.2218],
        [-0.1488]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1472,  0.9427,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7133,  2.1708,  0.0000,  0.0000],
        [ 0.1472, -0.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2197, -0.0789],
        [ 0.0124,  0.0523],
        [ 0.1457, -0.0430]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1181],
        [0.2011],
        [0.1190]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0377],
        [0.1928],
        [0.1384]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2003, -0.0564],
        [ 0.0108,  0.0541],
        [ 0.1527, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0414],
        [-0.0888],
        [-0.1204]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0322],
        [-0.1025],
        [-0.0780]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2003, -0.0564],
        [ 0.0108,  0.0541],
        [ 0.1527, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0657],
        [-0.2098],
        [-0.1638]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1045],
        [-0.2212],
        [-0.1270]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2182,  0.8997,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7257,  2.1185,  0.0000,  0.0000],
        [-0.2182, -0.8997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2003, -0.0564],
        [ 0.0108,  0.0541],
        [ 0.1527, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0954],
        [0.1975],
        [0.1356]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0344],
        [0.1894],
        [0.1611]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1924, -0.0317],
        [ 0.0093,  0.0559],
        [ 0.1636, -0.0458]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0384],
        [-0.0882],
        [-0.1240]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0363],
        [-0.1006],
        [-0.0739]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1924, -0.0317],
        [ 0.0093,  0.0559],
        [ 0.1636, -0.0458]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0626],
        [-0.2081],
        [-0.1512]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1053],
        [-0.2208],
        [-0.1261]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.5712,  0.8670,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7365,  2.0644,  0.0000,  0.0000],
        [-0.5712, -0.8670,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1924, -0.0317],
        [ 0.0093,  0.0559],
        [ 0.1636, -0.0458]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0943],
        [0.1937],
        [0.1570]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0578],
        [0.1856],
        [0.1759]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1922, -0.0085],
        [ 0.0081,  0.0577],
        [ 0.1668, -0.0527]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0354],
        [-0.0877],
        [-0.1208]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0391],
        [-0.0987],
        [-0.0649]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1922, -0.0085],
        [ 0.0081,  0.0577],
        [ 0.1668, -0.0527]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0585],
        [-0.2067],
        [-0.1452]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1118],
        [-0.2205],
        [-0.1332]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.9273,  0.8212,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7459,  2.0085,  0.0000,  0.0000],
        [-0.9273, -0.8212,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1922, -0.0085],
        [ 0.0081,  0.0577],
        [ 0.1668, -0.0527]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1123],
        [0.1900],
        [0.1708]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0761],
        [0.1817],
        [0.2005]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1785,  0.0051],
        [ 0.0076,  0.0596],
        [ 0.1690, -0.0460]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0325],
        [-0.0873],
        [-0.1085]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0370],
        [-0.0966],
        [-0.0518]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1785,  0.0051],
        [ 0.0076,  0.0596],
        [ 0.1690, -0.0460]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0622],
        [-0.2054],
        [-0.1512]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1236],
        [-0.2197],
        [-0.1411]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.2863,  0.7685,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7540,  1.9508,  0.0000,  0.0000],
        [-1.2863, -0.7685,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1785,  0.0051],
        [ 0.0076,  0.0596],
        [ 0.1690, -0.0460]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1285],
        [0.1863],
        [0.1959]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0969],
        [0.1777],
        [0.2318]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1640,  0.0177],
        [ 0.0073,  0.0616],
        [ 0.1835, -0.0448]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0312],
        [-0.0871],
        [-0.0920]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0340],
        [-0.0946],
        [-0.0343]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1640,  0.0177],
        [ 0.0073,  0.0616],
        [ 0.1835, -0.0448]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0700],
        [-0.2038],
        [-0.1573]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1402],
        [-0.2184],
        [-0.1537]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.6339,  0.7175,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7615,  1.8911,  0.0000,  0.0000],
        [-1.6339, -0.7175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1640,  0.0177],
        [ 0.0073,  0.0616],
        [ 0.1835, -0.0448]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1462],
        [0.1828],
        [0.2307]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1144],
        [0.1735],
        [0.2668]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1448,  0.0380],
        [ 0.0073,  0.0638],
        [ 0.2013, -0.0405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0256],
        [-0.0869],
        [-0.0732]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0269],
        [-0.0928],
        [-0.0157]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1448,  0.0380],
        [ 0.0073,  0.0638],
        [ 0.2013, -0.0405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0864],
        [-0.2021],
        [-0.1662]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1600],
        [-0.2170],
        [-0.1637]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.9814,  0.6550,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7688,  1.8295,  0.0000,  0.0000],
        [-1.9814, -0.6550,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1448,  0.0380],
        [ 0.0073,  0.0638],
        [ 0.2013, -0.0405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1605],
        [0.1792],
        [0.2716]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1286],
        [0.1693],
        [0.3000]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1311,  0.0551],
        [ 0.0074,  0.0660],
        [ 0.2189, -0.0362]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0230],
        [-0.0866],
        [-0.0505]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-1.8520e-02],
        [-9.0943e-02],
        [-6.5094e-05]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1311,  0.0551],
        [ 0.0074,  0.0660],
        [ 0.2189, -0.0362]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1078],
        [-0.2003],
        [-0.1768]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1852],
        [-0.2163],
        [-0.1794]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3275,  0.5764,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7761,  1.7657,  0.0000,  0.0000],
        [-2.3275, -0.5764,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1311,  0.0551],
        [ 0.0074,  0.0660],
        [ 0.2189, -0.0362]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1689],
        [0.1755],
        [0.3143]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1359],
        [0.1654],
        [0.3346]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1153,  0.0726],
        [ 0.0075,  0.0685],
        [ 0.2345, -0.0235]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0190],
        [-0.0862],
        [-0.0307]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0081],
        [-0.0894],
        [ 0.0107]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1153,  0.0726],
        [ 0.0075,  0.0685],
        [ 0.2345, -0.0235]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1337],
        [-0.1982],
        [-0.1934]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2107],
        [-0.2151],
        [-0.1987]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.6775,  0.4852,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7835,  1.6997,  0.0000,  0.0000],
        [-2.6775, -0.4852,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1153,  0.0726],
        [ 0.0075,  0.0685],
        [ 0.2345, -0.0235]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1707],
        [0.1715],
        [0.3599]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1370],
        [0.1612],
        [0.3728]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0985,  0.0913],
        [ 0.0077,  0.0712],
        [ 0.2533, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0150],
        [-0.0858],
        [-0.0170]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0045],
        [-0.0879],
        [ 0.0202]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0985,  0.0913],
        [ 0.0077,  0.0712],
        [ 0.2533, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1610],
        [-0.1964],
        [-0.2129]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2367],
        [-0.2136],
        [-0.2185]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0273,  0.3890,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7910,  1.6312,  0.0000,  0.0000],
        [-3.0273, -0.3890,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0985,  0.0913],
        [ 0.0077,  0.0712],
        [ 0.2533, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1673],
        [0.1669],
        [0.4121]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1386],
        [0.1565],
        [0.4138]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0819,  0.1103],
        [ 0.0083,  0.0743],
        [ 0.2709,  0.0041]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0117],
        [-0.0860],
        [-0.0004]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0180],
        [-0.0867],
        [ 0.0269]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0819,  0.1103],
        [ 0.0083,  0.0743],
        [ 0.2709,  0.0041]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1899],
        [-0.1949],
        [-0.2325]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2616],
        [-0.2118],
        [-0.2380]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.3790,  0.2891,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7986,  1.5600,  0.0000,  0.0000],
        [-3.3790, -0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0819,  0.1103],
        [ 0.0083,  0.0743],
        [ 0.2709,  0.0041]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1606],
        [0.1633],
        [0.4685]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1400],
        [0.1516],
        [0.4563]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0658,  0.1292],
        [ 0.0091,  0.0776],
        [ 0.2879,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0100],
        [-0.0867],
        [ 0.0213]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0317],
        [-0.0848],
        [ 0.0343]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0658,  0.1292],
        [ 0.0091,  0.0776],
        [ 0.2879,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2223],
        [-0.1932],
        [-0.2540]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2868],
        [-0.2099],
        [-0.2576]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.7318,  0.1830,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8069,  1.4857,  0.0000,  0.0000],
        [-3.7318, -0.1830,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0658,  0.1292],
        [ 0.0091,  0.0776],
        [ 0.2879,  0.0170]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1520],
        [0.1594],
        [0.5231]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1412],
        [0.1480],
        [0.4971]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0497,  0.1483],
        [ 0.0106,  0.0809],
        [ 0.3032,  0.0315]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0101],
        [-0.0878],
        [ 0.0480]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0279],
        [-0.0825],
        [ 0.0416]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0497,  0.1483],
        [ 0.0106,  0.0809],
        [ 0.3032,  0.0315]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2592],
        [-0.1916],
        [-0.2799]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.8003],
        [-0.2082],
        [-0.2777]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.0854,  0.0707,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8160,  1.4081,  0.0000,  0.0000],
        [-4.0854, -0.0707,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0497,  0.1483],
        [ 0.0106,  0.0809],
        [ 0.3032,  0.0315]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1418],
        [0.1553],
        [0.5708]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.6992],
        [0.1441],
        [0.5412]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3895, -0.0659],
        [ 0.0109,  0.0840],
        [ 0.3184,  0.0462]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1313],
        [-0.0877],
        [ 0.0743]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0001],
        [-0.0801],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3895, -0.0659],
        [ 0.0109,  0.0840],
        [ 0.3184,  0.0462]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.7376],
        [-0.1912],
        [-0.3085]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4897],
        [-0.2064],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  4.4384, -0.0461, -4.9538, -0.6676,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8266,  1.3271,  0.0000,  0.0000],
        [-4.4384,  0.0461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3895, -0.0659],
        [ 0.0109,  0.0840],
        [ 0.3184,  0.0462]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.6679],
        [0.1513],
        [0.6190]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.6063],
        [0.1389],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5870,  0.0116],
        [ 0.0103,  0.0875],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2045],
        [-0.0879],
        [-0.0297]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0054],
        [-0.0773],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5870,  0.0116],
        [ 0.0103,  0.0875],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3941],
        [-0.1891],
        [-0.1429]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4456],
        [-0.2046],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5643, -0.6017,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8375,  1.2432,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5870,  0.0116],
        [ 0.0103,  0.0875],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5917],
        [0.1453],
        [0.0853]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.5346],
        [0.1336],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5332,  0.0175],
        [ 0.0090,  0.0933],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1755],
        [-0.0877],
        [-0.0307]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0445],
        [-0.0749],
        [ 0.0893]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5332,  0.0175],
        [ 0.0090,  0.0933],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3595],
        [-0.1868],
        [-0.1436]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4750],
        [-0.2028],
        [-0.4117]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9773, -0.6133,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8478,  1.1556,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5332,  0.0175],
        [ 0.0090,  0.0933],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5259],
        [0.1386],
        [0.0846]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.4784],
        [ 0.1298],
        [-0.0388]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7495,  0.0490],
        [ 0.0080,  0.0999],
        [ 0.2303, -0.0269]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.4642, -0.0269, -2.6946, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8647,  0.9624,  0.0000,  0.0000],
        [ 2.4642,  0.0269,  0.0000,  0.0000,  0.0000,  0.0000, -0.2303, -0.6529,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1945],
        [-0.0891],
        [ 0.0963]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7495,  0.0490],
        [ 0.0080,  0.0999],
        [ 0.2303, -0.0269]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.4642, -0.0269, -2.6946, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8647,  0.9624,  0.0000,  0.0000],
        [ 2.4642,  0.0269,  0.0000,  0.0000,  0.0000,  0.0000, -0.2303, -0.6529,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3812],
        [-0.1849],
        [-0.3907]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.4441,  0.0490, -3.4441, -0.6308,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8568,  1.0623,  0.0000,  0.0000],
        [ 3.4441, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7495,  0.0490],
        [ 0.0080,  0.0999],
        [ 0.2303, -0.0269]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.4642, -0.0269, -2.6946, -0.6798,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8647,  0.9624,  0.0000,  0.0000],
        [ 2.4642,  0.0269,  0.0000,  0.0000,  0.0000,  0.0000, -0.2303, -0.6529,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3532],
        [0.1326],
        [0.0213]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
Episode 2: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.8685,  2.2877, -3.1988, -2.0751,
         -0.4217, -3.7928,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.4654,  0.9400,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8685, -2.2877,  0.0000,  0.0000,  0.0000,  0.0000, -1.3303, -4.3628,
          0.0000,  0.0000, -2.3863,  1.9492]])
Actions: tensor([[-0.7822, -0.0872],
        [-0.2651,  0.0155],
        [ 0.2270, -0.5044]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Q-values: tensor([[ 0.4965],
        [-0.0768],
        [ 0.0523]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1855],
        [-0.0365],
        [-0.3715]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.8685,  2.2877, -3.1988, -2.0751,
         -0.4217, -3.7928,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.4654,  0.9400,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8685, -2.2877,  0.0000,  0.0000,  0.0000,  0.0000, -1.3303, -4.3628,
          0.0000,  0.0000, -2.3863,  1.9492]])
Actions: tensor([[-0.7822, -0.0872],
        [-0.2651,  0.0155],
        [ 0.2270, -0.5044]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Q-values: tensor([[-0.3170],
        [-0.1749],
        [-0.8461]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.6508],
        [-0.1645],
        [-0.6466]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.8685,  2.2877, -3.1988, -2.0751,
         -0.4217, -3.7928,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.4654,  0.9400,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8685, -2.2877,  0.0000,  0.0000,  0.0000,  0.0000, -1.3303, -4.3628,
          0.0000,  0.0000, -2.3863,  1.9492]])
Actions: tensor([[-0.7822, -0.0872],
        [-0.2651,  0.0155],
        [ 0.2270, -0.5044]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Q-values: tensor([[0.7550],
        [0.0837],
        [0.3161]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.6152],
        [ 0.1122],
        [-0.0400]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Actions: tensor([[-0.7404,  0.0823],
        [-0.1772,  0.0590],
        [ 0.1782, -0.0268]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Q-values: tensor([[ 0.4708],
        [-0.0700],
        [-0.3347]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0527],
        [ 0.0186],
        [-0.3834]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Actions: tensor([[-0.7404,  0.0823],
        [-0.1772,  0.0590],
        [ 0.1782, -0.0268]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Q-values: tensor([[-0.3944],
        [-0.1975],
        [-0.5771]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3305],
        [-0.2036],
        [-0.5960]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  1.8705, -2.9908, -3.3651,
          0.3605, -3.7056,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1564, -0.4526,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.8593, -1.8705,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.6133,  2.4536]])
Actions: tensor([[-0.7404,  0.0823],
        [-0.1772,  0.0590],
        [ 0.1782, -0.0268]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Q-values: tensor([[0.7861],
        [0.1684],
        [0.0155]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2070],
        [0.1308],
        [0.0759]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Actions: tensor([[-0.4378,  0.4331],
        [-0.1490,  0.0756],
        [ 0.2563, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Q-values: tensor([[ 0.2408],
        [-0.0233],
        [-0.3535]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0451],
        [ 0.0239],
        [-0.3487]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Actions: tensor([[-0.4378,  0.4331],
        [-0.1490,  0.0756],
        [ 0.2563, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Q-values: tensor([[-0.1925],
        [-0.2271],
        [-0.5285]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.9285],
        [-0.1962],
        [-0.5693]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0594,  1.7613,  0.0000,  0.0000,
          1.1010, -3.7880,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1187, -1.4946,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0594, -1.7613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -2.7915,  2.4804]])
Actions: tensor([[-0.4378,  0.4331],
        [-0.1490,  0.0756],
        [ 0.2563, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Q-values: tensor([[0.3859],
        [0.2377],
        [0.1318]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3395],
        [0.1245],
        [0.2092]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Actions: tensor([[-0.4096, -0.0423],
        [-0.1373,  0.0882],
        [ 0.3382,  0.0017]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Q-values: tensor([[ 0.2640],
        [-0.0059],
        [-0.3303]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0772],
        [ 0.5861],
        [-0.3568]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Actions: tensor([[-0.4096, -0.0423],
        [-0.1373,  0.0882],
        [ 0.3382,  0.0017]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Q-values: tensor([[-0.7754],
        [-0.2083],
        [-0.5028]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7584],
        [-0.4093],
        [-0.5331]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.7534,  1.3024,  0.0000,  0.0000,
          1.5387, -4.2211, -2.2943,  3.8086],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9576, -1.5702,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7534, -1.3024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.0477,  2.5062]])
Actions: tensor([[-0.4096, -0.0423],
        [-0.1373,  0.0882],
        [ 0.3382,  0.0017]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Q-values: tensor([[0.5258],
        [0.2352],
        [0.2622]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0681],
        [-0.2434],
        [ 0.3267]])
States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Actions: tensor([[-0.3195, -0.0637],
        [-0.4581,  0.0285],
        [ 0.3786,  0.0386]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Q-values: tensor([[ 0.1936],
        [ 0.7691],
        [-0.3478]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1153],
        [ 0.5652],
        [-0.3544]])
States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Actions: tensor([[-0.3195, -0.0637],
        [-0.4581,  0.0285],
        [ 0.3786,  0.0386]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Q-values: tensor([[-0.5806],
        [-0.2309],
        [-0.4800]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7654],
        [-0.3839],
        [-0.5229]])
States: tensor([[ 0.0000,  0.0000, -3.6622, -3.1628,  1.5012,  1.3465,  0.0000,  0.0000,
          1.9483, -4.1788, -1.8847,  3.8509],
        [ 3.6622,  3.1628,  0.0000,  0.0000,  0.0000,  0.0000,  1.5329, -1.6584,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.5012, -1.3465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.3860,  2.5045]])
Actions: tensor([[-0.3195, -0.0637],
        [-0.4581,  0.0285],
        [ 0.3786,  0.0386]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Q-values: tensor([[0.2636],
        [0.0455],
        [0.3538]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0369],
        [-0.2257],
        [ 0.4224]])
States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Actions: tensor([[-0.1926, -0.0484],
        [-0.3738,  0.0615],
        [ 0.4082,  0.0675]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Q-values: tensor([[ 0.1060],
        [ 0.7443],
        [-0.3564]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1372],
        [ 0.5397],
        [-0.3346]])
States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Actions: tensor([[-0.1926, -0.0484],
        [-0.3738,  0.0615],
        [ 0.4082,  0.0675]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Q-values: tensor([[-0.6173],
        [-0.1874],
        [-0.4926]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7711],
        [-0.3587],
        [-0.5247]])
States: tensor([[ 0.0000,  0.0000, -3.3427, -3.0705,  2.1993,  1.4488,  0.0000,  0.0000,
          2.2678, -4.1150, -1.5653,  3.9147],
        [ 3.3427,  3.0705,  0.0000,  0.0000,  0.0000,  0.0000,  1.0610, -1.6869,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1993, -1.4488,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -3.7646,  2.4659]])
Actions: tensor([[-0.1926, -0.0484],
        [-0.3738,  0.0615],
        [ 0.4082,  0.0675]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Q-values: tensor([[0.2175],
        [0.0523],
        [0.4169]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0139],
        [-0.2092],
        [ 0.5123]])
States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Actions: tensor([[-0.0965, -0.0513],
        [-0.3040,  0.0989],
        [ 0.4267,  0.0990]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0191],
        [ 0.7072],
        [-0.3817]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1572],
        [ 0.5069],
        [ 0.0237]])
States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Actions: tensor([[-0.0965, -0.0513],
        [-0.3040,  0.0989],
        [ 0.4267,  0.0990]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.6675],
        [-0.1492],
        [-0.5163]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7803],
        [-0.3241],
        [-0.1520]])
States: tensor([[ 0.0000,  0.0000, -3.1501, -2.9606,  2.8001,  1.5647,  0.0000,  0.0000,
          2.4604, -4.0666, -1.3727,  3.9631],
        [ 3.1501,  2.9606,  0.0000,  0.0000,  0.0000,  0.0000,  0.6303, -1.7484,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8001, -1.5647,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.1728,  2.3984]])
Actions: tensor([[-0.0965, -0.0513],
        [-0.3040,  0.0989],
        [ 0.4267,  0.0990]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1633],
        [0.0607],
        [0.4790]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0097],
        [-0.1543],
        [ 0.3815]])
States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370, -0.0759],
        [-0.2443,  0.1406],
        [ 0.2434, -0.0557]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0700],
        [ 0.6552],
        [-0.0191]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1694],
        [ 0.4860],
        [ 0.0367]])
States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370, -0.0759],
        [-0.2443,  0.1406],
        [ 0.2434, -0.0557]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.7076],
        [-0.0921],
        [-0.1628]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7961],
        [-0.3358],
        [-0.1575]])
States: tensor([[ 0.0000,  0.0000, -3.0536, -2.8104,  3.3234,  1.7150,  0.0000,  0.0000,
          2.5569, -4.0153, -1.2762,  4.0144],
        [ 3.0536,  2.8104,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.8473,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3234, -1.7150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0370, -0.0759],
        [-0.2443,  0.1406],
        [ 0.2434, -0.0557]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1143],
        [0.0912],
        [0.3005]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0039],
        [-0.1359],
        [ 0.4093]])
States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0025, -0.1091],
        [-0.2272,  0.1730],
        [ 0.2537, -0.0536]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1603],
        [ 0.6271],
        [-0.0035]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1849],
        [ 0.4597],
        [ 0.0487]])
States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0025, -0.1091],
        [-0.2272,  0.1730],
        [ 0.2537, -0.0536]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.7646],
        [-0.1084],
        [-0.1635]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.8129],
        [-0.3556],
        [-0.1621]])
States: tensor([[ 0.0000,  0.0000, -3.0166, -2.5939,  3.6038,  1.7352,  0.0000,  0.0000,
          2.5939, -3.9394, -1.2392,  4.0903],
        [ 3.0166,  2.5939,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9879,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.6038, -1.7352,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.0025, -0.1091],
        [-0.2272,  0.1730],
        [ 0.2537, -0.0536]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0453],
        [0.0966],
        [0.3134]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0145],
        [-0.1155],
        [ 0.4338]])
States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0171, -0.1639],
        [-0.2079,  0.2052],
        [ 0.2625, -0.0523]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2483],
        [ 0.5979],
        [ 0.0116]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2126],
        [ 0.4167],
        [ 0.0597]])
States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0171, -0.1639],
        [-0.2079,  0.2052],
        [ 0.2625, -0.0523]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.8191],
        [-0.1299],
        [-0.1633]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.8219],
        [-0.3808],
        [-0.1654]])
States: tensor([[ 0.0000,  0.0000, -3.0142, -2.3118,  3.8599,  1.7906,  0.0000,  0.0000,
          2.5963, -3.8304, -1.2367,  4.1993],
        [ 3.0142,  2.3118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1610,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.8599, -1.7906,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0171, -0.1639],
        [-0.2079,  0.2052],
        [ 0.2625, -0.0523]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0078],
        [ 0.1072],
        [ 0.3261]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0382],
        [-0.0949],
        [ 0.4563]])
States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0227, -0.2455],
        [-0.1761,  0.2492],
        [ 0.2698, -0.0532]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3301],
        [ 0.5549],
        [ 0.0265]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2553],
        [ 0.3583],
        [ 0.0698]])
States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0227, -0.2455],
        [-0.1761,  0.2492],
        [ 0.2698, -0.0532]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.8644],
        [-0.1551],
        [-0.1620]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.8334],
        [-0.4141],
        [-0.1674]])
States: tensor([[ 0.0000,  0.0000, -3.0313, -1.9427,  4.1053,  1.9022,  0.0000,  0.0000,
          2.5792, -3.6665, -1.2538,  4.3632],
        [ 3.0313,  1.9427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3662,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.1053, -1.9022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0227, -0.2455],
        [-0.1761,  0.2492],
        [ 0.2698, -0.0532]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0367],
        [ 0.1132],
        [ 0.3412]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0775],
        [-0.0758],
        [ 0.4783]])
States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0131, -0.3391],
        [-0.1426,  0.3212],
        [ 0.2767, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3920],
        [ 0.4649],
        [ 0.0413]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1138],
        [ 0.2743],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0131, -0.3391],
        [-0.1426,  0.3212],
        [ 0.2767, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.8886],
        [-0.1779],
        [-0.1589]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.6833],
        [-0.4640],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000, -3.0540, -1.4480,  4.3524,  2.0945,  0.0000,  0.0000,
          2.5565, -3.4209, -1.2765,  4.6088],
        [ 3.0540,  1.4480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.6153,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.3524, -2.0945,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[ 0.0131, -0.3391],
        [-0.1426,  0.3212],
        [ 0.2767, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0327],
        [ 0.1195],
        [ 0.3609]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.4534],
        [-0.0525],
        [ 0.0656]])
States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7169,  0.4544],
        [-0.1006,  0.4048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0316],
        [ 0.3574],
        [-0.0183]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.3212],
        [ 0.2781],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7169,  0.4544],
        [-0.1006,  0.4048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.6016],
        [-0.2100],
        [-0.1178]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-1.4431],
        [-0.4583],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000, -3.0671, -0.7877,  0.0000,  0.0000, -3.0671, -3.7243,
          2.5434, -3.0819,  0.0000,  0.0000],
        [ 3.0671,  0.7877,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.9365,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7169,  0.4544],
        [-0.1006,  0.4048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2062],
        [0.1444],
        [0.0871]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2283],
        [-0.0040],
        [ 0.0656]])
States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7935,  0.1200],
        [-0.0581,  0.3770],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1272],
        [ 0.3161],
        [-0.0172]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1185],
        [ 0.2463],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7935,  0.1200],
        [-0.0581,  0.3770],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-1.4030],
        [-0.2589],
        [-0.1186]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-1.1780],
        [-0.4641],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000, -2.3502, -0.8373,  0.0000,  0.0000, -2.3502, -4.1787,
          3.2603, -3.5363, -0.5728,  4.4934],
        [ 2.3502,  0.8373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3413,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7935,  0.1200],
        [-0.0581,  0.3770],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0290],
        [ 0.1595],
        [ 0.0870]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2916],
        [0.0743],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7268,  0.3538],
        [-0.0690,  0.3676],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0824],
        [ 0.2607],
        [-0.0163]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1308],
        [ 0.1222],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7268,  0.3538],
        [-0.0690,  0.3676],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.9773],
        [-0.3015],
        [-0.1191]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-1.0814],
        [-1.1010],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000, -1.5567, -0.5803,  0.0000,  0.0000, -1.5567, -4.2986,
          0.0000,  0.0000,  0.2208,  4.3735],
        [ 1.5567,  0.5803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.7183,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7268,  0.3538],
        [-0.0690,  0.3676],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2060],
        [0.1897],
        [0.0872]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2346],
        [0.1728],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7389,  0.4710],
        [ 0.1619,  0.6127],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1058],
        [ 0.0782],
        [-0.0155]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0886],
        [ 0.1508],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7389,  0.4710],
        [ 0.1619,  0.6127],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.8856],
        [-0.8616],
        [-0.1204]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.6259],
        [-1.0399],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000, -0.8299, -0.5665,  0.0000,  0.0000, -0.8299, -4.6524,
          0.0000,  0.0000,  0.9476,  4.0197],
        [ 0.8299,  0.5665,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0859,
          0.0000,  0.0000,  1.7774,  4.5862],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.7389,  0.4710],
        [ 0.1619,  0.6127],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2028],
        [0.1703],
        [0.0873]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1396],
        [0.2359],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4244,  0.1967],
        [ 0.1723,  0.6048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0750],
        [ 0.0811],
        [-0.0149]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0718],
        [-0.0679],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4244,  0.1967],
        [ 0.1723,  0.6048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4894],
        [-0.8570],
        [-0.1214]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.6295],
        [-0.6184],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0709, -0.4247,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6864,  3.5487],
        [-0.0709,  0.4247,  0.0000,  0.0000,  0.0000,  0.0000, -0.1619, -4.6986,
          0.0000,  0.0000,  1.6155,  3.9735],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4244,  0.1967],
        [ 0.1723,  0.6048],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1610],
        [0.2266],
        [0.0875]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1672],
        [0.1641],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3932,  0.2739],
        [-0.0348,  0.2190],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0485],
        [-0.0881],
        [-0.0146]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0651],
        [-0.0569],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3932,  0.2739],
        [-0.0348,  0.2190],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4838],
        [-0.4849],
        [-0.1222]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5824],
        [-0.5845],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.3343, -0.0166,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.3521],
        [-0.3343,  0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4432,  3.3687],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3932,  0.2739],
        [-0.0348,  0.2190],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1819],
        [0.1540],
        [0.0875]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1660],
        [0.1669],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3710,  0.2724],
        [-0.0411,  0.2167],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0431],
        [-0.0742],
        [-0.0142]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0596],
        [-0.0460],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3710,  0.2724],
        [-0.0411,  0.2167],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4532],
        [-0.4636],
        [-0.1231]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5339],
        [-0.5509],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.2995, -0.0715,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  3.0782],
        [-0.2995,  0.0715,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4780,  3.1497],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3710,  0.2724],
        [-0.0411,  0.2167],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1824],
        [0.1618],
        [0.0875]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1651],
        [0.1704],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3487,  0.2672],
        [-0.0491,  0.2148],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0347],
        [-0.0599],
        [-0.0140]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0527],
        [-0.0360],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3487,  0.2672],
        [-0.0491,  0.2148],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4233],
        [-0.4450],
        [-0.1242]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4853],
        [-0.5176],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.2584, -0.1272,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.8058],
        [-0.2584,  0.1272,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5191,  2.9330],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3487,  0.2672],
        [-0.0491,  0.2148],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1812],
        [0.1713],
        [0.0875]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1658],
        [0.1683],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3305,  0.2623],
        [-0.0589,  0.2132],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0271],
        [-0.0447],
        [-0.0137]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0461],
        [-0.0232],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3305,  0.2623],
        [-0.0589,  0.2132],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3952],
        [-0.4273],
        [-0.1253]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4373],
        [-0.4828],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.2092, -0.1796,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.5386],
        [-0.2092,  0.1796,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5682,  2.7182],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3305,  0.2623],
        [-0.0589,  0.2132],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1800],
        [0.1767],
        [0.0874]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1634],
        [0.1622],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3141,  0.2592],
        [-0.0704,  0.2123],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0197],
        [-0.0272],
        [-0.0134]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0388],
        [-0.0079],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3141,  0.2592],
        [-0.0704,  0.2123],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3678],
        [-0.4077],
        [-0.1265]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3875],
        [-0.4484],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.1504, -0.2287,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.2762],
        [-0.1504,  0.2287,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6271,  2.5050],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3141,  0.2592],
        [-0.0704,  0.2123],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1788],
        [0.1791],
        [0.0873]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1566],
        [0.1592],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3058,  0.2547],
        [-0.0821,  0.2055],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0092],
        [-0.0059],
        [-0.0132]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0376],
        [ 0.0097],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3058,  0.2547],
        [-0.0821,  0.2055],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3380],
        [-0.3876],
        [-0.1277]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3390],
        [-0.4151],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0800, -0.2757,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0170],
        [-0.0800,  0.2757,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.6975,  2.2927],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3058,  0.2547],
        [-0.0821,  0.2055],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1786],
        [0.1790],
        [0.0872]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1562],
        [0.1552],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2995,  0.2426],
        [-0.0938,  0.1998],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0070],
        [ 0.0165],
        [-0.0131]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0343],
        [ 0.0239],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2995,  0.2426],
        [-0.0938,  0.1998],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3056],
        [-0.3683],
        [-0.1288]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2958],
        [-0.3818],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3249,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.7623],
        [ 0.0000,  0.3249,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.0871],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2995,  0.2426],
        [-0.0938,  0.1998],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1813],
        [0.1752],
        [0.0870]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1598],
        [0.1516],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2794,  0.2374],
        [-0.0888,  0.1965],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0041],
        [ 0.0318],
        [-0.0130]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0366],
        [ 0.0325],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2794,  0.2374],
        [-0.0888,  0.1965],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2734],
        [-0.3483],
        [-0.1298]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2549],
        [-0.3507],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.3676,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.5197],
        [ 0.0000,  0.3676,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.8873],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2794,  0.2374],
        [-0.0888,  0.1965],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1900],
        [0.1649],
        [0.0868]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1625],
        [0.1442],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2571,  0.2361],
        [-0.0852,  0.1935],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0068],
        [ 0.0463],
        [-0.0129]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0428],
        [ 0.0403],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2571,  0.2361],
        [-0.0852,  0.1935],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2419],
        [-0.3293],
        [-0.1307]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2140],
        [-0.3222],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4085,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.2823],
        [ 0.0000,  0.4085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.6908],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2571,  0.2361],
        [-0.0852,  0.1935],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1958],
        [0.1567],
        [0.0865]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1689],
        [0.1406],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2327,  0.2406],
        [-0.0811,  0.1919],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0147],
        [ 0.0545],
        [-0.0129]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0491],
        [ 0.0436],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2327,  0.2406],
        [-0.0811,  0.1919],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2131],
        [-0.3104],
        [-0.1316]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1805],
        [-0.2961],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4510,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.0463],
        [ 0.0000,  0.4510,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.4973],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2327,  0.2406],
        [-0.0811,  0.1919],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2002],
        [0.1501],
        [0.0862]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1798],
        [0.1364],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2051,  0.2501],
        [-0.0786,  0.1911],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Q-values: tensor([[-0.0220],
        [ 0.0555],
        [-0.0129]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1090],
        [0.1609],
        [0.2126]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2051,  0.2501],
        [-0.0786,  0.1911],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Q-values: tensor([[-0.1850],
        [-0.2934],
        [-0.1323]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3133],
        [-0.3535],
        [-0.5779]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4997,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.8056],
        [ 0.0000,  0.4997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.3054],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2051,  0.2501],
        [-0.0786,  0.1911],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Q-values: tensor([[0.2043],
        [0.1441],
        [0.0859]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3052],
        [0.2137],
        [0.2144]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Actions: tensor([[-0.2008,  0.3493],
        [-0.0180,  0.3305],
        [ 0.3428, -0.2289]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Q-values: tensor([[0.0783],
        [0.1178],
        [0.1729]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1179],
        [0.1817],
        [0.2674]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Actions: tensor([[-0.2008,  0.3493],
        [-0.0180,  0.3305],
        [ 0.3428, -0.2289]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Q-values: tensor([[-0.3598],
        [-0.3812],
        [-0.6059]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3432],
        [-0.3593],
        [-0.6586]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5587,  0.0000, -1.9165,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.5556],
        [ 0.0000,  0.5587,  0.0000,  0.0000,  0.0000, -1.3579,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  1.1142],
        [ 0.0000,  1.9165,  0.0000,  1.3579,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  2.4721]])
Actions: tensor([[-0.2008,  0.3493],
        [-0.0180,  0.3305],
        [ 0.3428, -0.2289]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Q-values: tensor([[0.3094],
        [0.2095],
        [0.1689]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3183],
        [0.2291],
        [0.3106]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Actions: tensor([[-0.1324,  0.4222],
        [ 0.0049,  0.3954],
        [ 0.4416, -0.1898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Q-values: tensor([[0.0807],
        [0.1337],
        [0.1988]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1264],
        [0.1956],
        [0.3374]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Actions: tensor([[-0.1324,  0.4222],
        [ 0.0049,  0.3954],
        [ 0.4416, -0.1898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Q-values: tensor([[-0.4043],
        [-0.4087],
        [-0.6994]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3390],
        [-0.3807],
        [-0.7652]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.5774,  0.3428, -2.4947,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.2063],
        [ 0.0000,  0.5774,  0.0000,  0.0000,  0.3428, -1.9173,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774,  0.7837],
        [-0.3428,  2.4947, -0.3428,  1.9173,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.4347,  2.7010]])
Actions: tensor([[-0.1324,  0.4222],
        [ 0.0049,  0.3954],
        [ 0.4416, -0.1898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Q-values: tensor([[0.3232],
        [0.2362],
        [0.2123]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3161],
        [0.2507],
        [0.4375]])
States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Actions: tensor([[-0.0404,  0.5026],
        [ 0.0085,  0.4732],
        [ 0.5179, -0.1060]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Q-values: tensor([[0.0748],
        [0.1533],
        [0.2300]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1356],
        [0.2030],
        [0.4038]])
States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Actions: tensor([[-0.0404,  0.5026],
        [ 0.0085,  0.4732],
        [ 0.5179, -0.1060]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Q-values: tensor([[-0.4095],
        [-0.4490],
        [-0.8010]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3230],
        [-0.3766],
        [-0.8514]])
States: tensor([[ 0.0000,  0.0000,  0.0049, -0.6042,  0.7843, -3.1067,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.2159],
        [-0.0049,  0.6042,  0.0000,  0.0000,  0.7794, -2.5025,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7725,  0.3883],
        [-0.7843,  3.1067, -0.7794,  2.5025,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.9931,  2.8908]])
Actions: tensor([[-0.0404,  0.5026],
        [ 0.0085,  0.4732],
        [ 0.5179, -0.1060]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Q-values: tensor([[0.3484],
        [0.2584],
        [0.2913]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3200],
        [0.2605],
        [0.5991]])
States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Actions: tensor([[ 0.0620,  0.5844],
        [-0.0568,  0.5712],
        [ 0.5793, -0.0093]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Q-values: tensor([[0.0745],
        [0.1786],
        [0.2725]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1541],
        [0.2095],
        [0.4942]])
States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Actions: tensor([[ 0.0620,  0.5844],
        [-0.0568,  0.5712],
        [ 0.5793, -0.0093]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Q-values: tensor([[-0.4121],
        [-0.4644],
        [-0.8871]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3045],
        [-0.3571],
        [-0.9283]])
States: tensor([[ 0.0000,  0.0000,  0.0134, -0.6337,  1.3022, -3.7153,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.7186],
        [-0.0134,  0.6337,  0.0000,  0.0000,  1.2888, -3.0817,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7641, -0.0849],
        [-1.3022,  3.7153, -1.2888,  3.0817,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.4752,  2.9968]])
Actions: tensor([[ 0.0620,  0.5844],
        [-0.0568,  0.5712],
        [ 0.5793, -0.0093]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Q-values: tensor([[0.3763],
        [0.2977],
        [0.4190]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3621],
        [0.2950],
        [0.7846]])
States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Actions: tensor([[ 0.1521,  0.6486],
        [-0.1405,  0.6422],
        [ 0.6486,  0.1068]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Q-values: tensor([[0.0929],
        [0.2039],
        [0.3757]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0263],
        [ 0.2036],
        [-0.0114]])
States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Actions: tensor([[ 0.1521,  0.6486],
        [-0.1405,  0.6422],
        [ 0.6486,  0.1068]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Q-values: tensor([[-0.4048],
        [-0.4635],
        [-0.9456]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1448],
        [-0.3135],
        [-0.9401]])
States: tensor([[ 0.0000,  0.0000, -0.0620, -0.6469,  1.8196, -4.3090,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7155, -1.3030],
        [ 0.0620,  0.6469,  0.0000,  0.0000,  1.8816, -3.6622,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -0.6561],
        [-1.8196,  4.3090, -1.8816,  3.6622,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.1041,  3.0060]])
Actions: tensor([[ 0.1521,  0.6486],
        [-0.1405,  0.6422],
        [ 0.6486,  0.1068]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Q-values: tensor([[0.4241],
        [0.3478],
        [0.5999]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0647],
        [0.3781],
        [0.5743]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Actions: tensor([[-0.1534,  0.0201],
        [-0.1588,  0.7036],
        [ 0.4709, -0.0964]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0607, -0.0201,  0.0000,  0.0000,  0.2133, -0.0201,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0607,  0.0201,  0.0000,  0.0000,  0.0000,  0.0000,  0.2740,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -1.2236,  2.9956]])
Q-values: tensor([[ 0.0132],
        [ 0.1749],
        [-0.0167]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Actions: tensor([[-0.1534,  0.0201],
        [-0.1588,  0.7036],
        [ 0.4709, -0.0964]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0607, -0.0201,  0.0000,  0.0000,  0.2133, -0.0201,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0607,  0.0201,  0.0000,  0.0000,  0.0000,  0.0000,  0.2740,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -1.2236,  2.9956]])
Q-values: tensor([[-0.1505],
        [-0.4094],
        [-0.7835]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0218,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.5301, -4.1976,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7774, -1.2984],
        [ 0.0000,  0.0000, -2.5301,  4.1976,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -0.7527,  2.8992]])
Actions: tensor([[-0.1534,  0.0201],
        [-0.1588,  0.7036],
        [ 0.4709, -0.0964]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000, -0.0607, -0.0201,  0.0000,  0.0000,  0.2133, -0.0201,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0607,  0.0201,  0.0000,  0.0000,  0.0000,  0.0000,  0.2740,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -1.2236,  2.9956]])
Q-values: tensor([[0.0896],
        [0.3992],
        [0.5891]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
Episode 3: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  1.0267, -0.9063,  0.0000,  0.0000,  0.3044,  1.1443,
         -1.7108,  3.6227,  0.0000,  0.0000],
        [-1.0267,  0.9063,  0.0000,  0.0000,  0.0000,  0.0000, -0.7223,  2.0506,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4378,  2.9330, -0.7795, -0.0491]])
Actions: tensor([[-0.0976,  0.1222],
        [-0.0132,  0.0894],
        [-0.0963, -0.0489]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Q-values: tensor([[-0.0221],
        [ 0.0533],
        [-0.1095]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1341],
        [ 0.0545],
        [-0.1594]])
States: tensor([[ 0.0000,  0.0000,  1.0267, -0.9063,  0.0000,  0.0000,  0.3044,  1.1443,
         -1.7108,  3.6227,  0.0000,  0.0000],
        [-1.0267,  0.9063,  0.0000,  0.0000,  0.0000,  0.0000, -0.7223,  2.0506,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4378,  2.9330, -0.7795, -0.0491]])
Actions: tensor([[-0.0976,  0.1222],
        [-0.0132,  0.0894],
        [-0.0963, -0.0489]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Q-values: tensor([[-0.2325],
        [-0.1864],
        [-0.0170]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3051],
        [-0.2416],
        [ 0.0665]])
States: tensor([[ 0.0000,  0.0000,  1.0267, -0.9063,  0.0000,  0.0000,  0.3044,  1.1443,
         -1.7108,  3.6227,  0.0000,  0.0000],
        [-1.0267,  0.9063,  0.0000,  0.0000,  0.0000,  0.0000, -0.7223,  2.0506,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4378,  2.9330, -0.7795, -0.0491]])
Actions: tensor([[-0.0976,  0.1222],
        [-0.0132,  0.0894],
        [-0.0963, -0.0489]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Q-values: tensor([[0.3742],
        [0.1669],
        [0.2171]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2378],
        [0.1348],
        [0.1897]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Actions: tensor([[-0.1186,  0.1517],
        [-0.1659,  0.0342],
        [-0.0983, -0.0514]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Q-values: tensor([[ 0.0383],
        [ 0.1651],
        [-0.1012]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1362],
        [ 0.0922],
        [-0.1537]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Actions: tensor([[-0.1186,  0.1517],
        [-0.1659,  0.0342],
        [-0.0983, -0.0514]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Q-values: tensor([[-0.1984],
        [-0.2152],
        [-0.0016]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1887],
        [-0.2563],
        [ 0.0686]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  1.1111e+00, -9.3915e-01,  0.0000e+00,
          0.0000e+00,  1.7793e+00,  1.8883e+00, -1.6132e+00,  3.5004e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1111e+00,  9.3915e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  6.6822e-01,  2.8274e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.5342e+00,  2.9818e+00,
         -6.8317e-01, -2.6297e-04]])
Actions: tensor([[-0.1186,  0.1517],
        [-0.1659,  0.0342],
        [-0.0983, -0.0514]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Q-values: tensor([[0.2729],
        [0.0992],
        [0.2229]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1664],
        [0.1060],
        [0.1874]])
States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Actions: tensor([[-0.1539,  0.1547],
        [-0.3391,  0.0112],
        [-0.0978, -0.0558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Q-values: tensor([[ 0.0557],
        [ 0.2470],
        [-0.0954]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1193],
        [-0.1777],
        [-0.1799]])
States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Actions: tensor([[-0.1539,  0.1547],
        [-0.3391,  0.0112],
        [-0.0978, -0.0558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Q-values: tensor([[-0.1327],
        [-0.2531],
        [ 0.0154]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2612],
        [-0.4196],
        [-0.2015]])
States: tensor([[ 0.0000,  0.0000,  1.0637, -1.0566,  0.0000,  0.0000,  3.1762,  2.3270,
         -1.4947,  3.3487,  0.0000,  0.0000],
        [-1.0637,  1.0566,  0.0000,  0.0000,  0.0000,  0.0000,  2.1125,  3.3836,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.6325,  3.0332, -0.5849,  0.0511]])
Actions: tensor([[-0.1539,  0.1547],
        [-0.3391,  0.0112],
        [-0.0978, -0.0558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Q-values: tensor([[0.2058],
        [0.0910],
        [0.2246]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3243],
        [0.3951],
        [0.2083]])
States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Actions: tensor([[-0.0782,  0.0662],
        [-0.2321, -0.0572],
        [-0.1006,  0.1930]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Q-values: tensor([[-0.0349],
        [-0.0176],
        [-0.0877]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1117],
        [-0.1703],
        [-0.1557]])
States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Actions: tensor([[-0.0782,  0.0662],
        [-0.2321, -0.0572],
        [-0.1006,  0.1930]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Q-values: tensor([[-0.1289],
        [-0.2592],
        [-0.1424]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2542],
        [-0.4011],
        [-0.2388]])
States: tensor([[ 0.0000,  0.0000,  0.8786, -1.2001,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3407,  3.1940,  0.0000,  0.0000],
        [-0.8786,  1.2001,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2193,  4.3941,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8412,  2.8198,
          3.7303,  3.0890, -0.4871,  0.1069]])
Actions: tensor([[-0.0782,  0.0662],
        [-0.2321, -0.0572],
        [-0.1006,  0.1930]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Q-values: tensor([[0.3194],
        [0.3645],
        [0.2234]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3228],
        [0.3929],
        [0.2105]])
States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Actions: tensor([[-0.0870,  0.0525],
        [-0.1970, -0.0720],
        [-0.1266,  0.2405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Q-values: tensor([[-0.0378],
        [-0.0290],
        [-0.0556]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1085],
        [-0.1652],
        [-0.1096]])
States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Actions: tensor([[-0.0870,  0.0525],
        [-0.1970, -0.0720],
        [-0.1266,  0.2405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Q-values: tensor([[-0.1324],
        [-0.2515],
        [-0.1826]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2453],
        [-0.3834],
        [-0.2744]])
States: tensor([[ 0.0000,  0.0000,  0.7247, -1.3235,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2626,  3.1278,  0.0000,  0.0000],
        [-0.7247,  1.3235,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9872,  4.4513,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7406,  3.8050,
          3.8309,  2.8960, -0.3865, -0.0861]])
Actions: tensor([[-0.0870,  0.0525],
        [-0.1970, -0.0720],
        [-0.1266,  0.2405]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Q-values: tensor([[0.3177],
        [0.3772],
        [0.2256]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3210],
        [0.3945],
        [0.2079]])
States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Actions: tensor([[-0.0944,  0.0463],
        [-0.1713, -0.0882],
        [-0.1635,  0.2890]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Q-values: tensor([[-0.0470],
        [-0.0453],
        [-0.0158]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1065],
        [-0.1621],
        [-0.1919]])
States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Actions: tensor([[-0.0944,  0.0463],
        [-0.1713, -0.0882],
        [-0.1635,  0.2890]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Q-values: tensor([[-0.1355],
        [-0.2461],
        [-0.2179]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2363],
        [-0.3699],
        [ 0.1226]])
States: tensor([[ 0.0000,  0.0000,  0.6147, -1.4479,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1755,  3.0753,  0.0000,  0.0000],
        [-0.6147,  1.4479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7903,  4.5233,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6140,  4.6352,
          3.9574,  2.6556, -0.2599, -0.3265]])
Actions: tensor([[-0.0944,  0.0463],
        [-0.1713, -0.0882],
        [-0.1635,  0.2890]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Q-values: tensor([[0.3175],
        [0.3896],
        [0.2134]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3195],
        [0.4007],
        [0.1646]])
States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Actions: tensor([[-0.1002,  0.0410],
        [-0.1520, -0.1072],
        [-0.1568,  0.0380]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Q-values: tensor([[-0.0593],
        [-0.0596],
        [-0.1788]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1066],
        [-0.1557],
        [-0.2230]])
States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Actions: tensor([[-0.1002,  0.0410],
        [-0.1520, -0.1072],
        [-0.1568,  0.0380]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Q-values: tensor([[-0.1412],
        [-0.2428],
        [ 0.0397]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2260],
        [-0.3581],
        [ 0.1006]])
States: tensor([[ 0.0000,  0.0000,  0.5379, -1.5825,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0812,  3.0290,  0.0000,  0.0000],
        [-0.5379,  1.5825,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6190,  4.6115,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.1210,  2.3666, -0.0964, -0.6155]])
Actions: tensor([[-0.1002,  0.0410],
        [-0.1520, -0.1072],
        [-0.1568,  0.0380]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Q-values: tensor([[0.3184],
        [0.4037],
        [0.1809]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3175],
        [0.4097],
        [0.3595]])
States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Actions: tensor([[-0.1083,  0.0330],
        [-0.1402, -0.1246],
        [ 0.1503, -0.0662]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Q-values: tensor([[-0.0727],
        [-0.0739],
        [-0.3334]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1074],
        [ 0.1409],
        [-0.2063]])
States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Actions: tensor([[-0.1083,  0.0330],
        [-0.1402, -0.1246],
        [ 0.1503, -0.0662]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Q-values: tensor([[-0.1495],
        [-0.2463],
        [ 0.0159]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2145],
        [-0.1840],
        [ 0.1040]])
States: tensor([[ 0.0000,  0.0000,  0.4860, -1.7307,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9809,  2.9880,  0.0000,  0.0000],
        [-0.4860,  1.7307,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4670,  4.7187,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2936, -3.1081,
          4.2778,  2.3286,  0.0605, -0.6535]])
Actions: tensor([[-0.1083,  0.0330],
        [-0.1402, -0.1246],
        [ 0.1503, -0.0662]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Q-values: tensor([[0.3212],
        [0.4235],
        [0.4144]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3201],
        [0.1684],
        [0.3804]])
States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Actions: tensor([[-0.1193,  0.0240],
        [-0.1799,  0.1954],
        [ 0.1353, -0.0903]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Q-values: tensor([[-0.0826],
        [ 0.1214],
        [-0.3148]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5948],
        [-0.1387],
        [ 0.0346]])
States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Actions: tensor([[-0.1193,  0.0240],
        [-0.1799,  0.1954],
        [ 0.1353, -0.0903]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Q-values: tensor([[-0.1539],
        [-0.2145],
        [ 0.0308]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7680],
        [-0.3287],
        [ 0.0124]])
States: tensor([[ 0.0000,  0.0000,  0.4541, -1.8883,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.8727,  2.9550,  0.0000,  0.0000],
        [-0.4541,  1.8883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4440, -3.0419,
          4.1275,  2.3948, -0.0899, -0.5873]])
Actions: tensor([[-0.1193,  0.0240],
        [-0.1799,  0.1954],
        [ 0.1353, -0.0903]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Q-values: tensor([[0.3219],
        [0.1700],
        [0.4351]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4505],
        [0.4025],
        [0.0795]])
States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Actions: tensor([[-0.8435, -0.1971],
        [-0.1003, -0.1317],
        [ 0.0859, -0.4245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Q-values: tensor([[-0.6274],
        [-0.0971],
        [ 0.2228]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5035],
        [-0.1644],
        [-0.0009]])
States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Actions: tensor([[-0.8435, -0.1971],
        [-0.1003, -0.1317],
        [ 0.0859, -0.4245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Q-values: tensor([[-0.5788],
        [-0.2497],
        [ 0.2259]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4767],
        [-0.3356],
        [ 0.0258]])
States: tensor([[ 0.0000,  0.0000,  0.3935, -1.7169, -4.7456,  0.4459,  0.0000,  0.0000,
         -0.7534,  2.9310, -4.9707, -0.0511],
        [-0.3935,  1.7169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1469,  4.6479,  0.0000,  0.0000],
        [ 4.7456, -0.4459,  0.0000,  0.0000,  0.0000,  0.0000, -0.5792, -2.9517,
          3.9922,  2.4851, -0.2251, -0.4970]])
Actions: tensor([[-0.8435, -0.1971],
        [-0.1003, -0.1317],
        [ 0.0859, -0.4245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Q-values: tensor([[0.3300],
        [0.4135],
        [0.3284]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3566],
        [0.4370],
        [0.1133]])
States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Actions: tensor([[-0.7348, -0.1058],
        [-0.1595, -0.1085],
        [ 0.0541, -0.4051]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Q-values: tensor([[-0.5607],
        [-0.1514],
        [ 0.1156]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0334],
        [-0.1427],
        [ 0.1820]])
States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Actions: tensor([[-0.7348, -0.1058],
        [-0.1595, -0.1085],
        [ 0.0541, -0.4051]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Q-values: tensor([[-0.3676],
        [-0.2905],
        [ 0.1905]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5254],
        [-0.5862],
        [-0.5243]])
States: tensor([[ 0.0000,  0.0000,  1.1368, -1.6516, -3.8162,  0.2185,  0.0000,  0.0000,
          0.0901,  3.1281, -4.1272,  0.1460],
        [-1.1368,  1.6516,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0467,  4.7796,  0.0000,  0.0000],
        [ 3.8162, -0.2185,  0.0000,  0.0000,  0.0000,  0.0000, -0.6651, -2.5272,
          3.9063,  2.9096, -0.3110, -0.0725]])
Actions: tensor([[-0.7348, -0.1058],
        [-0.1595, -0.1085],
        [ 0.0541, -0.4051]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Q-values: tensor([[0.2561],
        [0.4258],
        [0.3165]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.4701],
        [ 0.6229],
        [-0.0237]])
States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Actions: tensor([[-0.8093, -0.1997],
        [ 0.0357, -0.6153],
        [ 0.4581, -0.1463]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Q-values: tensor([[-0.0989],
        [-0.0470],
        [ 0.1282]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0512],
        [-0.1560],
        [-0.1511]])
States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Actions: tensor([[-0.8093, -0.1997],
        [ 0.0357, -0.6153],
        [ 0.4581, -0.1463]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Q-values: tensor([[-0.2957],
        [-0.5166],
        [-0.3353]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3292],
        [-0.1521],
        [-0.0490]])
States: tensor([[ 0.0000,  0.0000,  1.7120, -1.6544, -3.0274, -0.0808, -3.7466, -2.2029,
          0.8249,  3.2338, -3.3925,  0.2517],
        [-1.7120,  1.6544,  0.0000,  0.0000, -4.7394,  1.5735,  0.0000,  0.0000,
         -0.8872,  4.8882,  0.0000,  0.0000],
        [ 3.0274,  0.0808,  4.7394, -1.5735,  0.0000,  0.0000, -0.7192, -2.1221,
          0.0000,  0.0000, -0.3651,  0.3326]])
Actions: tensor([[-0.8093, -0.1997],
        [ 0.0357, -0.6153],
        [ 0.4581, -0.1463]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Q-values: tensor([[0.2902],
        [0.5470],
        [0.1594]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3871],
        [0.5165],
        [0.1933]])
States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Actions: tensor([[-0.6081, -0.1065],
        [-0.1691, -0.1945],
        [ 0.2288, -0.4020]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Q-values: tensor([[-0.1355],
        [-0.0697],
        [-0.2817]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0929],
        [-0.1411],
        [-0.1793]])
States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Actions: tensor([[-0.6081, -0.1065],
        [-0.1691, -0.1945],
        [ 0.2288, -0.4020]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Q-values: tensor([[-0.2232],
        [-0.0836],
        [-0.0410]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2084],
        [-0.1095],
        [-0.0572]])
States: tensor([[ 0.0000,  0.0000,  2.5570, -2.0032, -1.7600, -0.0274, -2.9373, -2.0032,
          1.6342,  3.4335, -2.5832,  0.4514],
        [-2.5570,  2.0032,  0.0000,  0.0000, -4.3170,  1.9758,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.7600,  0.0274,  4.3170, -1.9758,  0.0000,  0.0000, -1.1773, -1.9758,
          3.3941,  3.4609, -0.8232,  0.4788]])
Actions: tensor([[-0.6081, -0.1065],
        [-0.1691, -0.1945],
        [ 0.2288, -0.4020]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Q-values: tensor([[0.3191],
        [0.4417],
        [0.2994]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3779],
        [0.5360],
        [0.2700]])
States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Actions: tensor([[-0.5273,  0.0092],
        [-0.1537, -0.1422],
        [ 0.2275, -0.3622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Q-values: tensor([[-0.1904],
        [-0.0498],
        [-0.2937]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1312],
        [-0.1176],
        [-0.0391]])
States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Actions: tensor([[-0.5273,  0.0092],
        [-0.1537, -0.1422],
        [ 0.2275, -0.3622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Q-values: tensor([[-0.1919],
        [-0.0467],
        [-0.0713]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1428],
        [-0.1137],
        [-0.5642]])
States: tensor([[ 0.0000,  0.0000,  2.9960, -1.8967, -0.9230, -0.3229, -2.3291, -1.8967,
          2.2423,  3.5400, -1.9750,  0.5579],
        [-2.9960,  1.8967,  0.0000,  0.0000, -3.9190,  1.5739,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.9230,  0.3229,  3.9190, -1.5739,  0.0000,  0.0000, -1.4061, -1.5739,
          3.1653,  3.8629, -1.0520,  0.8808]])
Actions: tensor([[-0.5273,  0.0092],
        [-0.1537, -0.1422],
        [ 0.2275, -0.3622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Q-values: tensor([[0.3405],
        [0.4681],
        [0.3662]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3730],
        [0.5773],
        [0.2159]])
States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Actions: tensor([[-0.4964,  0.1050],
        [-0.1639, -0.0837],
        [ 0.4433,  0.0261]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Q-values: tensor([[-0.2336],
        [-0.0273],
        [-0.1013]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1737],
        [-0.0797],
        [-0.1416]])
States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Actions: tensor([[-0.4964,  0.1050],
        [-0.1639, -0.0837],
        [ 0.4433,  0.0261]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Q-values: tensor([[-0.1993],
        [-0.0490],
        [-0.4497]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0646],
        [-0.1410],
        [-0.1928]])
States: tensor([[ 0.0000,  0.0000,  3.3696, -1.9059, -0.1682, -0.6942, -1.8018, -1.9059,
          2.7696,  3.5309, -1.4477,  0.5488],
        [-3.3696,  1.9059,  0.0000,  0.0000, -3.5378,  1.2117,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1682,  0.6942,  3.5378, -1.2117,  0.0000,  0.0000, -1.6337, -1.2117,
          0.0000,  0.0000, -1.2795,  1.2430]])
Actions: tensor([[-0.4964,  0.1050],
        [-0.1639, -0.0837],
        [ 0.4433,  0.0261]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Q-values: tensor([[0.3347],
        [0.5294],
        [0.2683]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3484],
        [0.6347],
        [0.5056]])
States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Actions: tensor([[-0.4495,  0.1918],
        [-0.1585, -0.0438],
        [ 0.1973, -0.1138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Q-values: tensor([[-0.2612],
        [ 0.0376],
        [-0.2234]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1821],
        [-0.1511],
        [-0.1313]])
States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Actions: tensor([[-0.4495,  0.1918],
        [-0.1585, -0.0438],
        [ 0.1973, -0.1138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Q-values: tensor([[-0.2256],
        [-0.0772],
        [-0.2452]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0120],
        [-0.2071],
        [-0.2451]])
States: tensor([[ 0.0000,  0.0000,  3.7021, -2.0109,  0.7715, -0.7731, -1.3054, -2.0109,
          3.2660,  3.4258, -0.9513,  0.4437],
        [-3.7021,  2.0109,  0.0000,  0.0000, -2.9305,  1.2378,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7715,  0.7731,  2.9305, -1.2378,  0.0000,  0.0000, -2.0770, -1.2378,
          2.4945,  4.1990, -1.7228,  1.2169]])
Actions: tensor([[-0.4495,  0.1918],
        [-0.1585, -0.0438],
        [ 0.1973, -0.1138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Q-values: tensor([[0.3145],
        [0.6462],
        [0.5520]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3399],
        [1.0553],
        [0.5911]])
States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Actions: tensor([[-0.4246,  0.2593],
        [ 0.0452,  0.1372],
        [ 0.1586,  0.0034]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Q-values: tensor([[-0.2679],
        [-0.1568],
        [-0.2111]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1628],
        [-0.0358],
        [-0.1215]])
States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Actions: tensor([[-0.4246,  0.2593],
        [ 0.0452,  0.1372],
        [ 0.1586,  0.0034]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Q-values: tensor([[-0.2155],
        [-0.0722],
        [-0.2928]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4098],
        [-0.4307],
        [-0.2833]])
States: tensor([[ 0.0000,  0.0000,  3.9931, -2.2027,  1.4184, -1.0787, -0.8559, -2.2027,
          3.7156,  3.2340, -0.5018,  0.2520],
        [-3.9931,  2.2027,  0.0000,  0.0000, -2.5748,  1.1240, -4.8490,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4184,  1.0787,  2.5748, -1.1240,  0.0000,  0.0000, -2.2742, -1.1240,
          2.2972,  4.3128, -1.9201,  1.3307]])
Actions: tensor([[-0.4246,  0.2593],
        [ 0.0452,  0.1372],
        [ 0.1586,  0.0034]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Q-values: tensor([[0.3027],
        [0.7285],
        [0.6383]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2501],
        [0.7004],
        [0.6594]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Actions: tensor([[-0.1950,  0.3350],
        [ 0.2863,  0.2976],
        [ 0.1709,  0.0672]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Q-values: tensor([[ 0.1286],
        [-0.0532],
        [-0.1900]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1905],
        [-0.0645],
        [-0.1220]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Actions: tensor([[-0.1950,  0.3350],
        [ 0.2863,  0.2976],
        [ 0.1709,  0.0672]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Q-values: tensor([[-0.4163],
        [-0.2107],
        [-0.3387]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3942],
        [-0.3528],
        [-0.3024]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.0016, -1.3347, -0.4313, -2.4620,
          0.0000,  0.0000, -0.0771, -0.0074],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.4614,  0.9901, -4.8943, -0.1372,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.0016,  1.3347,  2.4614, -0.9901,  0.0000,  0.0000, -2.4328, -1.1274,
          2.1386,  4.3094, -2.0787,  1.3273]])
Actions: tensor([[-0.1950,  0.3350],
        [ 0.2863,  0.2976],
        [ 0.1709,  0.0672]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Q-values: tensor([[0.2098],
        [0.4472],
        [0.7068]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2804],
        [0.1993],
        [0.7193]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Actions: tensor([[-0.1637,  0.4229],
        [-0.1680,  0.0233],
        [ 0.2101,  0.0898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Q-values: tensor([[ 0.1467],
        [-0.0517],
        [-0.1534]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2241],
        [-0.1769],
        [-0.1317]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Actions: tensor([[-0.1637,  0.4229],
        [-0.1680,  0.0233],
        [ 0.2101,  0.0898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Q-values: tensor([[-0.4030],
        [-0.2753],
        [-0.3677]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3975],
        [-0.4638],
        [-0.3513]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.3675, -1.6025, -0.2362, -2.7971,
          0.0000,  0.0000,  0.1179, -0.3424],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5768,  0.7597,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.3675,  1.6025,  2.5768, -0.7597,  0.0000,  0.0000, -2.6037, -1.1946,
          1.9677,  4.2422, -2.2496,  1.2601]])
Actions: tensor([[-0.1637,  0.4229],
        [-0.1680,  0.0233],
        [ 0.2101,  0.0898]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Q-values: tensor([[0.2179],
        [0.2587],
        [0.7695]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3268],
        [0.3919],
        [0.7978]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Actions: tensor([[-0.1465,  0.5187],
        [ 0.2127, -0.3349],
        [ 0.2177,  0.1427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Q-values: tensor([[ 0.1665],
        [-0.1067],
        [-0.1373]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0015],
        [-0.0355],
        [-0.1277]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Actions: tensor([[-0.1465,  0.5187],
        [ 0.2127, -0.3349],
        [ 0.2177,  0.1427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Q-values: tensor([[-0.4046],
        [-0.5029],
        [-0.4521]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0214],
        [-0.2934],
        [-0.4184]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  2.7413, -1.9356, -0.0725, -3.2200,
          0.0000,  0.0000,  0.2816, -0.7653],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1987,  0.8261,  0.0000,  0.0000,
         -0.4410,  4.9786,  0.0000,  0.0000],
        [-2.7413,  1.9356,  2.1987, -0.8261,  0.0000,  0.0000, -2.8138, -1.2843,
          1.7576,  4.1524, -2.4597,  1.1703]])
Actions: tensor([[-0.1465,  0.5187],
        [ 0.2127, -0.3349],
        [ 0.2177,  0.1427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Q-values: tensor([[0.2441],
        [0.4627],
        [0.8483]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4085],
        [0.2276],
        [0.8558]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Actions: tensor([[-0.4412,  0.6639],
        [-0.1369, -0.0258],
        [ 0.2234,  0.1956]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Q-values: tensor([[-0.0200],
        [ 0.0049],
        [-0.1196]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0253],
        [-0.0235],
        [-0.1156]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Actions: tensor([[-0.4412,  0.6639],
        [-0.1369, -0.0258],
        [ 0.2234,  0.1956]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Q-values: tensor([[-0.2393],
        [-0.2063],
        [-0.5550]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0540],
        [-0.2444],
        [-0.4786]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.0315, -2.3116,  0.0000, -3.7386,
          4.5715,  1.6981,  0.3541, -1.2840],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1937,  1.3037,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.0315,  2.3116,  2.1937, -1.3037,  0.0000,  0.0000, -3.0315, -1.4270,
          1.5400,  4.0097, -2.6774,  1.0276]])
Actions: tensor([[-0.4412,  0.6639],
        [-0.1369, -0.0258],
        [ 0.2234,  0.1956]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Q-values: tensor([[0.3081],
        [0.3039],
        [0.9145]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4121],
        [0.2295],
        [0.9327]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Actions: tensor([[-0.4679,  0.7250],
        [-0.1062, -0.0340],
        [ 0.2118,  0.2346]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Q-values: tensor([[ 0.0040],
        [ 0.0386],
        [-0.0985]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0196],
        [-0.0814],
        [-0.0979]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Actions: tensor([[-0.4679,  0.7250],
        [-0.1062, -0.0340],
        [ 0.2118,  0.2346]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Q-values: tensor([[-0.2581],
        [-0.1552],
        [-0.6316]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0516],
        [-0.3676],
        [-0.5361]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.2549, -2.7799,  0.0000, -4.4025,
          4.5715,  1.0343,  0.3541, -1.9478],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8335,  1.5251,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.2549,  2.7799,  1.8335, -1.5251,  0.0000,  0.0000, -3.2549, -1.6226,
          1.3166,  3.8142, -2.9008,  0.8321]])
Actions: tensor([[-0.4679,  0.7250],
        [-0.1062, -0.0340],
        [ 0.2118,  0.2346]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Q-values: tensor([[0.2877],
        [0.3144],
        [0.9975]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3359],
        [0.8132],
        [1.0074]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Actions: tensor([[-0.1470,  0.8001],
        [ 0.2959,  0.2525],
        [ 0.2115,  0.2828]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Q-values: tensor([[-0.0093],
        [-0.0249],
        [-0.0852]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3292],
        [-0.0163],
        [-0.1043]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Actions: tensor([[-0.1470,  0.8001],
        [ 0.2959,  0.2525],
        [ 0.2115,  0.2828]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Q-values: tensor([[-0.1046],
        [-0.2026],
        [-0.6973]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1688],
        [-0.2097],
        [-0.5999]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  3.4667, -3.2704,  0.0000,  0.0000,
          4.5715,  0.3092,  0.3541, -2.6729],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5155,  1.7936, -4.9822, -0.0635,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.4667,  3.2704,  1.5155, -1.7936,  0.0000,  0.0000, -3.4667, -1.8572,
          1.1048,  3.5796, -3.1126,  0.5975]])
Actions: tensor([[-0.1470,  0.8001],
        [ 0.2959,  0.2525],
        [ 0.2115,  0.2828]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Q-values: tensor([[0.1841],
        [0.5939],
        [1.0767]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1283],
        [0.2425],
        [0.5147]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Actions: tensor([[-0.4975,  0.6529],
        [-0.0931, -0.0473],
        [ 0.1370, -0.1114]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Q-values: tensor([[-0.3259],
        [ 0.0699],
        [ 0.0517]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3158],
        [-0.0129],
        [-0.1139]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Actions: tensor([[-0.4975,  0.6529],
        [-0.0931, -0.0473],
        [ 0.1370, -0.1114]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Q-values: tensor([[ 0.1056],
        [-0.1182],
        [-0.5482]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1795],
        [-0.1929],
        [-0.6442]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -0.4909,  0.3541, -3.4730],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5999,  1.8239,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5999, -1.8239,  0.0000,  0.0000, -3.6782, -2.1400,
          0.8933,  3.2968, -3.3241,  0.3147]])
Actions: tensor([[-0.4975,  0.6529],
        [-0.0931, -0.0473],
        [ 0.1370, -0.1114]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Q-values: tensor([[0.0116],
        [0.3434],
        [0.5119]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1184],
        [0.2213],
        [0.5327]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Actions: tensor([[-0.5337,  0.6907],
        [-0.0818, -0.0343],
        [ 0.1278, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Q-values: tensor([[-0.3285],
        [ 0.0768],
        [ 0.0564]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2824],
        [-0.0116],
        [-0.1242]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Actions: tensor([[-0.5337,  0.6907],
        [-0.0818, -0.0343],
        [ 0.1278, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Q-values: tensor([[ 0.1488],
        [-0.1026],
        [-0.5735]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2054],
        [-0.1807],
        [-0.6859]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.1439,  0.3541, -4.1260],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3698,  1.7598,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3698, -1.7598,  0.0000,  0.0000, -3.8152, -2.0286,
          0.7563,  3.4081, -3.4611,  0.4261]])
Actions: tensor([[-0.5337,  0.6907],
        [-0.0818, -0.0343],
        [ 0.1278, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Q-values: tensor([[-0.0191],
        [ 0.3260],
        [ 0.5078]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1051],
        [0.2007],
        [0.5500]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Actions: tensor([[-0.5720,  0.7221],
        [-0.0738, -0.0204],
        [ 0.1184, -0.0863]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Q-values: tensor([[-0.3045],
        [ 0.0803],
        [ 0.0530]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0120],
        [-0.1383]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Actions: tensor([[-0.5720,  0.7221],
        [-0.0738, -0.0204],
        [ 0.1184, -0.0863]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Q-values: tensor([[ 0.1945],
        [-0.0901],
        [-0.6001]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1721],
        [-0.7225]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          4.5715, -1.8345,  0.3541, -4.8166],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1602,  1.6931,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1602, -1.6931,  0.0000,  0.0000, -3.9429, -1.9275,
          0.6285,  3.5092, -3.5888,  0.5271]])
Actions: tensor([[-0.5720,  0.7221],
        [-0.0738, -0.0204],
        [ 0.1184, -0.0863]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Q-values: tensor([[-0.0518],
        [ 0.3099],
        [ 0.5039]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1786],
        [0.5756]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0661, -0.0071],
        [ 0.1097, -0.0711]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Q-values: tensor([[0.0539],
        [0.0807],
        [0.0391]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1046],
        [-0.1489]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0661, -0.0071],
        [ 0.1097, -0.0711]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Q-values: tensor([[-0.1051],
        [-0.0810],
        [-0.6340]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3837],
        [-0.7577]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9680,  1.6272,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9680, -1.6272,  0.0000,  0.0000, -4.0613, -1.8412,
          0.5101,  3.5955, -3.7072,  0.6134]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0661, -0.0071],
        [ 0.1097, -0.0711]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Q-values: tensor([[0.1327],
        [0.2930],
        [0.5040]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.7881],
        [0.6015]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2901,  0.3181],
        [ 0.0998, -0.0600]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Q-values: tensor([[ 0.0534],
        [-0.0453],
        [ 0.0149]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2062],
        [-0.1376]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2901,  0.3181],
        [ 0.0998, -0.0600]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Q-values: tensor([[-0.1053],
        [-0.2492],
        [-0.6763]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3780],
        [-0.7577]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7922,  1.5632, -4.9632, -0.2070,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7922, -1.5632,  0.0000,  0.0000, -4.1710, -1.7701,
          0.4004,  3.6666, -3.8169,  0.6845]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2901,  0.3181],
        [ 0.0998, -0.0600]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Q-values: tensor([[0.1333],
        [0.5255],
        [0.5201]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3924],
        [0.6275]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1984, -0.2530],
        [ 0.1632, -0.0808]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Q-values: tensor([[ 0.0526],
        [-0.1289],
        [-0.0084]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0146],
        [-0.1581]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1984, -0.2530],
        [ 0.1632, -0.0808]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Q-values: tensor([[-0.1058],
        [-0.4597],
        [-0.6781]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1766],
        [-0.8304]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9825,  1.1851,  0.0000,  0.0000,
         -0.6819,  4.9117,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9825, -1.1851,  0.0000,  0.0000, -4.2708, -1.7101,
          0.3006,  3.7266, -3.9167,  0.7445]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1984, -0.2530],
        [ 0.1632, -0.0808]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Q-values: tensor([[0.1339],
        [0.4410],
        [0.5495]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1613],
        [0.6392]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0056],
        [ 0.1568, -0.0756]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Q-values: tensor([[ 0.0518],
        [ 0.0621],
        [-0.0499]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0160],
        [-0.1675]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0056],
        [ 0.1568, -0.0756]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Q-values: tensor([[-0.1062],
        [-0.1086],
        [-0.7685]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1651],
        [-0.8872]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0177,  1.3573,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0177, -1.3573,  0.0000,  0.0000, -4.4340, -1.6293,
          0.1375,  3.8074, -4.0799,  0.8253]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0056],
        [ 0.1568, -0.0756]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Q-values: tensor([[0.1343],
        [0.2673],
        [0.5662]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1356],
        [0.6739]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0533,  0.0185],
        [ 0.1459, -0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Q-values: tensor([[ 0.0507],
        [ 0.0622],
        [-0.0934]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0176],
        [-0.1726]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0533,  0.0185],
        [ 0.1459, -0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Q-values: tensor([[-0.1068],
        [-0.0987],
        [-0.8440]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1531],
        [-0.9388]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.7980,  1.2761,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.7980, -1.2761,  0.0000,  0.0000, -4.5908, -1.5537,
         -0.0193,  3.8831, -4.2366,  0.9010]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0533,  0.0185],
        [ 0.1459, -0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Q-values: tensor([[0.1346],
        [0.2448],
        [0.6074]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1137],
        [0.7088]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0424,  0.0324],
        [ 0.1422, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Q-values: tensor([[ 0.0496],
        [ 0.0606],
        [-0.1390]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0190],
        [-0.5504]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0424,  0.0324],
        [ 0.1422, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Q-values: tensor([[-0.1075],
        [-0.0877],
        [-0.9224]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1407],
        [-0.4065]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5988,  1.1937,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5988, -1.1937,  0.0000,  0.0000, -4.7367, -1.4898,
         -0.1652,  3.9469, -4.3825,  0.9648]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0424,  0.0324],
        [ 0.1422, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Q-values: tensor([[0.1349],
        [0.2198],
        [0.6545]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0927],
        [0.3390]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0328,  0.0457],
        [ 0.3285, -0.3375]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0480],
        [ 0.0580],
        [-0.4942]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0190],
        [-0.1634]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0328,  0.0457],
        [ 0.3285, -0.3375]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Q-values: tensor([[-0.1074],
        [-0.0782],
        [-0.4684]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1212],
        [-0.2669]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4142,  1.1090,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4142, -1.1090,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3074,  3.9993, -4.5247,  1.0172]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0328,  0.0457],
        [ 0.3285, -0.3375]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Q-values: tensor([[0.1348],
        [0.1956],
        [0.4690]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0475],
        [0.3427]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Actions: tensor([[-1.5542e-01,  1.9269e-02],
        [ 2.3562e-04,  7.3078e-02],
        [-9.1184e-02, -2.7120e-01]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0469],
        [ 0.0493],
        [-0.1517]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0182],
        [-0.1715]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Actions: tensor([[-1.5542e-01,  1.9269e-02],
        [ 2.3562e-04,  7.3078e-02],
        [-9.1184e-02, -2.7120e-01]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Q-values: tensor([[-0.1071],
        [-0.0634],
        [-0.3912]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1338],
        [-0.2893]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0528,  0.7259,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0528, -0.7259,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6359,  4.3367,  0.0000,  0.0000]])
Actions: tensor([[-1.5542e-01,  1.9269e-02],
        [ 2.3562e-04,  7.3078e-02],
        [-9.1184e-02, -2.7120e-01]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Q-values: tensor([[0.1346],
        [0.1355],
        [0.3120]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0526],
        [0.3523]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0263,  0.0850],
        [-0.1040, -0.2941]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[ 0.0457],
        [ 0.0337],
        [-0.1681]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1824],
        [-0.1832]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0263,  0.0850],
        [-0.1040, -0.2941]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[-0.1068],
        [-0.0803],
        [-0.4004]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3586],
        [-0.3096]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1443,  0.3816,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1443, -0.3816,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.5447,  4.6079,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0263,  0.0850],
        [-0.1040, -0.2941]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[0.1343],
        [0.1247],
        [0.3103]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3681],
        [0.3608]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0621, -0.0862],
        [-0.1044, -0.3302]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0445],
        [-0.2199],
        [-0.1842]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0246],
        [ 0.0067]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0621, -0.0862],
        [-0.1044, -0.3302]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1066],
        [-0.4212],
        [-0.4056]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2002],
        [-0.4736]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7456e-01,
          2.4740e-03,  0.0000e+00,  0.0000e+00, -7.1524e-01,  4.9045e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7456e-01, -2.4740e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4068e-01,  4.9020e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0621, -0.0862],
        [-0.1044, -0.3302]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1341],
        [0.3126],
        [0.3101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0741],
        [0.6136]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0271,  0.1126],
        [ 0.2653,  0.2358]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0437],
        [-0.0025],
        [-0.1462]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1798],
        [-0.0201]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0271,  0.1126],
        [ 0.2653,  0.2358]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1063],
        [-0.1477],
        [-0.3796]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3666],
        [-0.1335]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4410, -0.2415,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4410,  0.2415,  0.0000,  0.0000, -4.9077, -0.2045,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0271,  0.1126],
        [ 0.2653,  0.2358]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1340],
        [0.1378],
        [0.3524]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3651],
        [0.0732]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0300, -0.0663],
        [ 0.1836, -0.0091]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0429],
        [-0.2205],
        [ 0.0188]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0260],
        [-0.0241]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0300, -0.0663],
        [ 0.1836, -0.0091]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1060],
        [-0.4016],
        [-0.0830]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1542],
        [-0.1431]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2029, -0.1182,  0.0000,  0.0000,
         -0.8044,  4.8781,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2029,  0.1182,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0300, -0.0663],
        [ 0.1836, -0.0091]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1338],
        [0.2968],
        [0.1404]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0681],
        [0.0672]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0609,  0.1218],
        [ 0.1652,  0.0002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0424],
        [0.0065],
        [0.0141]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1781],
        [-0.0265]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0609,  0.1218],
        [ 0.1652,  0.0002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1054],
        [-0.0958],
        [-0.0921]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3576],
        [-0.1549]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0492, -0.0610,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0492,  0.0610,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0609,  0.1218],
        [ 0.1652,  0.0002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1333],
        [0.1285],
        [0.1300]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3618],
        [0.0691]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0085, -0.0261],
        [ 0.1706, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0417],
        [-0.2163],
        [ 0.0095]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1797],
        [-0.0294]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0085, -0.0261],
        [ 0.1706, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1051],
        [-0.3531],
        [-0.0973]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3454],
        [-0.1681]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0551, -0.1827,  0.0000,  0.0000,
         -0.8953,  4.8226,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0551,  0.1827,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0085, -0.0261],
        [ 0.1706, -0.0087]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1329],
        [0.2744],
        [0.1272]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3626],
        [0.0663]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0171, -0.0104],
        [ 0.1529, -0.0025]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0410],
        [-0.2145],
        [ 0.0068]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1793],
        [-0.0334]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0171, -0.0104],
        [ 0.1529, -0.0025]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1050],
        [-0.3242],
        [-0.1101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3319],
        [-0.1824]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2342, -0.1653,  0.0000,  0.0000,
         -0.8868,  4.8487,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2342,  0.1653,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0171, -0.0104],
        [ 0.1529, -0.0025]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1326],
        [0.2733],
        [0.1191]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3626],
        [0.0652]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0254,  0.0068],
        [ 0.1353,  0.0013]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0404],
        [-0.2120],
        [ 0.0060]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1783],
        [-0.0395]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0254,  0.0068],
        [ 0.1353,  0.0013]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1048],
        [-0.2955],
        [-0.1257]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3179],
        [-0.1970]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4042, -0.1573,  0.0000,  0.0000,
         -0.8697,  4.8591,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.4042,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0254,  0.0068],
        [ 0.1353,  0.0013]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1323],
        [0.2725],
        [0.1130]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3637],
        [0.0680]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0340,  0.0251],
        [ 0.1228,  0.0081]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0397],
        [-0.2098],
        [ 0.0003]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1771],
        [-0.0412]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0340,  0.0251],
        [ 0.1228,  0.0081]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1048],
        [-0.2675],
        [-0.1376]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3038],
        [-0.2078]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5649, -0.1629,  0.0000,  0.0000,
         -0.8443,  4.8523,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5649,  0.1629,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0340,  0.0251],
        [ 0.1228,  0.0081]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1320],
        [0.2742],
        [0.1102]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3634],
        [0.0742]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0431,  0.0464],
        [ 0.1116,  0.0138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0391],
        [-0.2075],
        [-0.0079]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1753],
        [-0.0384]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0431,  0.0464],
        [ 0.1116,  0.0138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1049],
        [-0.2408],
        [-0.1478]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2893],
        [-0.2175]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.7217, -0.1799,  0.0000,  0.0000,
         -0.8102,  4.8272,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.7217,  0.1799,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0431,  0.0464],
        [ 0.1116,  0.0138]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1318],
        [0.2774],
        [0.1118]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3601],
        [0.0819]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0525,  0.0705],
        [ 0.1004,  0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0386],
        [-0.2045],
        [-0.0104]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1762],
        [-0.0308]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0525,  0.0705],
        [ 0.1004,  0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1051],
        [-0.2154],
        [-0.1580]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2742],
        [-0.2278]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8764, -0.2125,  0.0000,  0.0000,
         -0.7671,  4.7808,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8764,  0.2125,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0525,  0.0705],
        [ 0.1004,  0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1316],
        [0.2813],
        [0.1141]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3522],
        [0.0925]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0983],
        [ 0.0930,  0.0282]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0381],
        [-0.2034],
        [-0.0082]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1768],
        [-0.0248]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0983],
        [ 0.0930,  0.0282]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1054],
        [-0.1912],
        [-0.1673]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2579],
        [-0.2385]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0293, -0.2631,  0.0000,  0.0000,
         -0.7147,  4.7103,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0293,  0.2631,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0630,  0.0983],
        [ 0.0930,  0.0282]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1314],
        [0.2840],
        [0.1154]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3423],
        [0.1068]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0739,  0.1303],
        [ 0.0859,  0.0370]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0375],
        [-0.2024],
        [-0.0053]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1766],
        [-0.0181]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0739,  0.1303],
        [ 0.0859,  0.0370]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1058],
        [-0.1669],
        [-0.1750]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2394],
        [-0.2506]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1853, -0.3331,  0.0000,  0.0000,
         -0.6516,  4.6120,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1853,  0.3331,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0739,  0.1303],
        [ 0.0859,  0.0370]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1313],
        [0.2847],
        [0.1215]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3287],
        [0.1244]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0808,  0.1642],
        [ 0.0800,  0.0479]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0371],
        [-0.2014],
        [ 0.0010]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4453],
        [-0.0095]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0808,  0.1642],
        [ 0.0800,  0.0479]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1063],
        [-0.1414],
        [-0.1810]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5272],
        [-0.2658]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3451, -0.4264,  0.0000,  0.0000,
         -0.5778,  4.4817,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.3451,  0.4264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0808,  0.1642],
        [ 0.0800,  0.0479]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1311],
        [0.2848],
        [0.1312]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3053],
        [0.1439]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3901,  0.0289],
        [ 0.0748,  0.0586]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0363],
        [-0.5217],
        [ 0.0080]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1658],
        [-0.0192]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3901,  0.0289],
        [ 0.0748,  0.0586]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1073],
        [-0.3314],
        [-0.1885]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2715],
        [-0.2428]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5059, -0.5427,  0.0000,  0.0000,
         -0.4970,  4.3175, -4.7143,  1.3354],
        [ 0.0000,  0.0000, -1.5059,  0.5427,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3901,  0.0289],
        [ 0.0748,  0.0586]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1310],
        [0.3463],
        [0.1441]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3182],
        [0.1240]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1289,  0.1573],
        [ 0.1069,  0.0285]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0361],
        [-0.1909],
        [ 0.0041]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1624],
        [-0.0082]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1289,  0.1573],
        [ 0.1069,  0.0285]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1081],
        [-0.1872],
        [-0.1712]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2461],
        [-0.2638]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.1906, -0.5130,  0.0000,  0.0000,
         -0.8870,  4.2886,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.1906,  0.5130,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1289,  0.1573],
        [ 0.1069,  0.0285]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1308],
        [0.2811],
        [0.1337]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3014],
        [0.1475]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1311,  0.2093],
        [ 0.0972,  0.0481]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0359],
        [-0.1930],
        [ 0.0114]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4304],
        [ 0.0041]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1311,  0.2093],
        [ 0.0972,  0.0481]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1090],
        [-0.1567],
        [-0.1839]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5378],
        [-0.2871]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4264, -0.6418,  0.0000,  0.0000,
         -0.7581,  4.1313,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4264,  0.6418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1311,  0.2093],
        [ 0.0972,  0.0481]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1305],
        [0.2773],
        [0.1476]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3134],
        [0.1728]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2973,  0.0883],
        [ 0.0907,  0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0354],
        [-0.5109],
        [ 0.0181]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1537],
        [ 0.0008]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2973,  0.0883],
        [ 0.0907,  0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1103],
        [-0.3500],
        [-0.1994]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2601],
        [-0.2717]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.6547, -0.8030,  0.0000,  0.0000,
         -0.6270,  3.9219, -4.8443,  0.9398],
        [ 0.0000,  0.0000, -1.6547,  0.8030,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2973,  0.0883],
        [ 0.0907,  0.0638]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1304],
        [0.3652],
        [0.1648]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2854],
        [0.1643]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1690,  0.2437],
        [ 0.1189,  0.0449]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0354],
        [-0.1889],
        [ 0.0191]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1366],
        [ 0.0132]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1690,  0.2437],
        [ 0.1189,  0.0449]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1115],
        [-0.1864],
        [-0.1876]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2390],
        [-0.2975]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.4482, -0.8276,  0.0000,  0.0000,
         -0.9243,  3.8337,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.4482,  0.8276,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1690,  0.2437],
        [ 0.1189,  0.0449]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1301],
        [0.2641],
        [0.1605]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2635],
        [0.1972]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1590,  0.3000],
        [ 0.1118,  0.0647]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0354],
        [-0.1786],
        [ 0.0276]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3848],
        [ 0.0258]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1590,  0.3000],
        [ 0.1118,  0.0647]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1126],
        [-0.1568],
        [-0.2034]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5035],
        [-0.3218]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7361, -1.0264,  0.0000,  0.0000,
         -0.7552,  3.5899,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.7361,  1.0264,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1590,  0.3000],
        [ 0.1118,  0.0647]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1298],
        [0.2452],
        [0.1832]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3060],
        [0.2333]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1374,  0.2306],
        [ 0.1079,  0.0806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0352],
        [-0.4631],
        [ 0.0349]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3951],
        [ 0.0307]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1374,  0.2306],
        [ 0.1079,  0.0806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1141],
        [-0.3315],
        [-0.2180]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5139],
        [-0.3242]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0069, -1.2617,  0.0000,  0.0000,
         -0.5963,  3.2899, -4.8136,  0.3078],
        [ 0.0000,  0.0000, -2.0069,  1.2617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1374,  0.2306],
        [ 0.1079,  0.0806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1296],
        [0.3513],
        [0.2101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3143],
        [0.2472]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0679,  0.2456],
        [ 0.1309,  0.0736]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0350],
        [-0.4654],
        [ 0.0398]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0786],
        [ 0.0368]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0679,  0.2456],
        [ 0.1309,  0.0736]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1158],
        [-0.3445],
        [-0.2162]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2857],
        [-0.3403]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.9774, -1.4117,  0.0000,  0.0000,
         -0.7337,  3.0593, -4.9510,  0.0772],
        [ 0.0000,  0.0000, -1.9774,  1.4117,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0679,  0.2456],
        [ 0.1309,  0.0736]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1294],
        [0.3502],
        [0.2235]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1941],
        [0.2655]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1793,  0.4007],
        [ 0.1457,  0.0715]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0354],
        [-0.1527],
        [ 0.0465]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3142],
        [ 0.0528]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1793,  0.4007],
        [ 0.1457,  0.0715]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1173],
        [-0.1918],
        [-0.2237]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4841],
        [-0.3877]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.0404, -1.5837,  0.0000,  0.0000,
         -0.8016,  2.8137,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.0404,  1.5837,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1793,  0.4007],
        [ 0.1457,  0.0715]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1291],
        [0.1839],
        [0.2385]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3140],
        [0.3142]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1135,  0.4098],
        [ 0.1495,  0.0874]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0355],
        [-0.3902],
        [ 0.0578]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2416],
        [ 0.0696]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1135,  0.4098],
        [ 0.1495,  0.0874]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1190],
        [-0.3299],
        [-0.2551]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4755],
        [-0.4323]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.3654, -1.9129,  0.0000,  0.0000,
         -0.6223,  2.4130, -4.8396, -0.5691],
        [ 0.0000,  0.0000, -2.3654,  1.9129,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1135,  0.4098],
        [ 0.1495,  0.0874]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1289],
        [0.2965],
        [0.2763]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3103],
        [0.3570]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2211,  0.5033],
        [ 0.1610,  0.1003]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0358],
        [-0.3207],
        [ 0.0691]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1424],
        [ 0.0908]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2211,  0.5033],
        [ 0.1610,  0.1003]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1210],
        [-0.3303],
        [-0.2834]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4576],
        [-0.4867]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.6284, -2.2353,  0.0000,  0.0000,
         -0.5088,  2.0033, -4.7261, -0.9788],
        [ 0.0000,  0.0000, -2.6284,  2.2353,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2211,  0.5033],
        [ 0.1610,  0.1003]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1288],
        [0.2758],
        [0.3138]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3179],
        [0.4135]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3361,  0.6287],
        [ 0.1720,  0.1237]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0364],
        [-0.2234],
        [ 0.0854]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0123],
        [ 0.1173]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3361,  0.6287],
        [ 0.1720,  0.1237]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1230],
        [-0.3296],
        [-0.3229]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4262],
        [-0.5513]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.0105, -2.6383,  0.0000,  0.0000,
         -0.2877,  1.5000, -4.5050, -1.4821],
        [ 0.0000,  0.0000, -3.0105,  2.6383,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3361,  0.6287],
        [ 0.1720,  0.1237]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1288],
        [0.2763],
        [0.3641]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3163],
        [0.4897]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4330,  0.7209],
        [ 0.1823,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0371],
        [-0.1016],
        [ 0.1082]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2266],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4330,  0.7209],
        [ 0.1823,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1251],
        [-0.3090],
        [-0.3746]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.0275],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  3.5186, -3.1433,  0.0000,  0.0000,
          0.0484,  0.8713, -4.1689, -2.1108],
        [ 0.0000,  0.0000, -3.5186,  3.1433,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4330,  0.7209],
        [ 0.1823,  0.1567]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1289],
        [0.3010],
        [0.4321]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2558],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1755,  0.4891],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0375],
        [-0.1582],
        [ 0.0094]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1137],
        [-0.1748],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1755,  0.4891],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1264],
        [-0.0519],
        [-0.1156]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3080],
        [-0.1224],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4815,  0.1505, -3.7359, -2.8316],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1755,  0.4891],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1285],
        [0.2163],
        [0.1242]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2712],
        [0.6982],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2722, -0.1056],
        [-0.5642,  0.6027],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0011],
        [-0.1836],
        [ 0.0096]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0899],
        [-0.2028],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2722, -0.1056],
        [-0.5642,  0.6027],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3714],
        [-0.2296],
        [-0.1169]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2561],
        [-0.0659],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  3.9145, -2.9116,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.9145,  2.9116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6570, -0.3386, -3.5603, -3.3207],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2722, -0.1056],
        [-0.5642,  0.6027],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3617],
        [0.7178],
        [0.1244]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2321],
        [0.4799],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2489, -0.0758],
        [-0.5421,  0.5895],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0125],
        [-0.1921],
        [ 0.0097]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0689],
        [ 0.0424],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2489, -0.0758],
        [-0.5421,  0.5895],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3014],
        [-0.1487],
        [-0.1178]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2050],
        [-0.3648],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  3.3502, -2.2033,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-3.3502,  2.2033,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2212, -0.9413, -2.9961, -3.9234],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2489, -0.0758],
        [-0.5421,  0.5895],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3053],
        [0.4518],
        [0.1240]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1939],
        [0.3850],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2237, -0.0550],
        [-0.3358,  0.4379],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0218],
        [0.0763],
        [0.0099]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0569],
        [ 0.0416],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2237, -0.0550],
        [-0.3358,  0.4379],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2280],
        [-0.3868],
        [-0.1188]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1772],
        [-0.3393],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.8082, -1.5379,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.8082,  1.5379,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.7633, -1.5308,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2237, -0.0550],
        [-0.3358,  0.4379],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2551],
        [0.4683],
        [0.1235]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1716],
        [0.2925],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2052, -0.0339],
        [-0.3278,  0.4381],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0290],
        [0.0824],
        [0.0100]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0464],
        [ 0.0281],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2052, -0.0339],
        [-0.3278,  0.4381],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1757],
        [-0.3783],
        [-0.1197]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1455],
        [-0.3507],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.4724, -1.0451,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.4724,  1.0451,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0991, -1.9687,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2052, -0.0339],
        [-0.3278,  0.4381],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2210],
        [0.3626],
        [0.1226]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1520],
        [0.2530],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1894,  0.0128],
        [-0.3324,  0.4451],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0367],
        [0.0661],
        [0.0100]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0308],
        [-0.0109],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1894,  0.0128],
        [-0.3324,  0.4451],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1256],
        [-0.3980],
        [-0.1204]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1108],
        [-0.3628],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.1446, -0.5731,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1446,  0.5731,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4268, -2.4068,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1894,  0.0128],
        [-0.3324,  0.4451],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1923],
        [0.2897],
        [0.1216]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1252],
        [0.2213],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1729,  0.0635],
        [-0.3845,  0.4503],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0458],
        [0.0250],
        [0.0098]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0061],
        [-0.0390],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1729,  0.0635],
        [-0.3845,  0.4503],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0857],
        [-0.4161],
        [-0.1210]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0896],
        [-0.3718],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.8122, -0.1407,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8122,  0.1407,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.7592, -2.8519,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1729,  0.0635],
        [-0.3845,  0.4503],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1706],
        [0.2522],
        [0.1206]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1071],
        [0.1914],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1624,  0.1033],
        [-0.4491,  0.4811],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0624],
        [-0.0201],
        [ 0.0096]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0115],
        [-0.0730],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1624,  0.1033],
        [-0.4491,  0.4811],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0488],
        [-0.4428],
        [-0.1215]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1053],
        [-0.1347],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.4277,  0.2461,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4277, -0.2461,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.1438, -3.3022,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1624,  0.1033],
        [-0.4491,  0.4811],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1605],
        [0.2076],
        [0.1195]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1045],
        [0.1762],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1505,  0.1552],
        [ 0.0925,  0.1310],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0757],
        [-0.0595],
        [ 0.0092]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0100],
        [-0.0704],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1505,  0.1552],
        [ 0.0925,  0.1310],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0398],
        [-0.0980],
        [-0.1218]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0996],
        [-0.1380],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.9786,  0.6240,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9786, -0.6240,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1505,  0.1552],
        [ 0.0925,  0.1310],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1641],
        [0.1986],
        [0.1184]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1044],
        [0.1846],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1522,  0.1544],
        [ 0.0918,  0.1341],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0726],
        [-0.0550],
        [ 0.0089]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0091],
        [-0.0677],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1522,  0.1544],
        [ 0.0918,  0.1341],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0379],
        [-0.1019],
        [-0.1220]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0952],
        [-0.1394],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.0711,  0.5998,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0711, -0.5998,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1522,  0.1544],
        [ 0.0918,  0.1341],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1608],
        [0.2038],
        [0.1174]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1048],
        [0.1934],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1562,  0.1553],
        [ 0.0915,  0.1376],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0697],
        [-0.0509],
        [ 0.0085]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0078],
        [-0.0650],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1562,  0.1553],
        [ 0.0915,  0.1376],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0386],
        [-0.1034],
        [-0.1224]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0922],
        [-0.1403],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.1629,  0.5794,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1629, -0.5794,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1562,  0.1553],
        [ 0.0915,  0.1376],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1580],
        [0.2100],
        [0.1163]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1056],
        [0.2026],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1594,  0.1562],
        [ 0.0898,  0.1424],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Q-values: tensor([[ 0.0666],
        [-0.0467],
        [ 0.0081]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0056],
        [-0.0620],
        [ 0.0330]])
States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1594,  0.1562],
        [ 0.0898,  0.1424],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Q-values: tensor([[-0.0390],
        [-0.1053],
        [-0.1228]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0907],
        [-0.1423],
        [-0.3494]])
States: tensor([[ 0.0000,  0.0000,  1.2544,  0.5617,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2544, -0.5617,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1594,  0.1562],
        [ 0.0898,  0.1424],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Q-values: tensor([[0.1532],
        [0.2149],
        [0.1153]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1056],
        [0.2117],
        [0.1296]])
States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Actions: tensor([[-0.1605,  0.1578],
        [ 0.0882,  0.1471],
        [ 0.1818, -0.2073]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Q-values: tensor([[ 0.0629],
        [-0.0415],
        [ 0.0541]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0031],
        [-0.0574],
        [ 0.0253]])
States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Actions: tensor([[-0.1605,  0.1578],
        [ 0.0882,  0.1471],
        [ 0.1818, -0.2073]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Q-values: tensor([[-0.0393],
        [-0.1072],
        [-0.3138]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0920],
        [-0.1438],
        [-0.3733]])
States: tensor([[ 0.0000,  0.0000,  1.3443,  0.5479,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.3443, -0.5479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6517,
          0.0000,  0.0000,  0.3541,  0.8029]])
Actions: tensor([[-0.1605,  0.1578],
        [ 0.0882,  0.1471],
        [ 0.1818, -0.2073]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Q-values: tensor([[0.1462],
        [0.2201],
        [0.1847]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1061],
        [0.2191],
        [0.1261]])
States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Actions: tensor([[-0.1624,  0.1578],
        [ 0.0856,  0.1511],
        [ 0.1567, -0.1861]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Q-values: tensor([[ 0.0587],
        [-0.0366],
        [ 0.0435]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0840],
        [ 0.0028]])
States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Actions: tensor([[-0.1624,  0.1578],
        [ 0.0856,  0.1511],
        [ 0.1567, -0.1861]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Q-values: tensor([[-0.0412],
        [-0.1089],
        [-0.3335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5788],
        [-0.4064]])
States: tensor([[ 0.0000,  0.0000,  1.4325,  0.5373,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4325, -0.5373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0405, -1.4445,
          0.0000,  0.0000,  0.1723,  1.0102]])
Actions: tensor([[-0.1624,  0.1578],
        [ 0.0856,  0.1511],
        [ 0.1567, -0.1861]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Q-values: tensor([[0.1399],
        [0.2244],
        [0.1800]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4084],
        [0.1296]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2118, -0.0632],
        [ 0.2253, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Q-values: tensor([[ 0.0341],
        [-0.0732],
        [ 0.0230]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0753],
        [-0.0130]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2118, -0.0632],
        [ 0.2253, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Q-values: tensor([[-0.1358],
        [-0.4961],
        [-0.3551]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5949],
        [-0.4414]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1795,  1.2583, -1.5181,  0.0000,
          0.0000,  0.0000, -1.1639,  2.4547],
        [ 0.0000,  0.0000,  1.1795, -1.2583,  0.0000,  0.0000, -0.3385, -1.2583,
          0.0000,  0.0000,  0.0156,  1.1963]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2118, -0.0632],
        [ 0.2253, -0.1011]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Q-values: tensor([[0.1180],
        [0.3663],
        [0.1686]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4052],
        [0.1246]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2502, -0.0614],
        [ 0.1993, -0.0841]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Q-values: tensor([[ 0.0335],
        [-0.0774],
        [ 0.0110]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0708],
        [-0.0208]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2502, -0.0614],
        [ 0.1993, -0.0841]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Q-values: tensor([[-0.1368],
        [-0.5142],
        [-0.3800]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.6199],
        [-0.4788]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1660,  1.1572, -1.7298,  0.0000,
          0.0000,  0.0000, -1.3757,  2.4547],
        [ 0.0000,  0.0000,  1.1660, -1.1572,  0.0000,  0.0000, -0.5638, -1.1572,
          0.0000,  0.0000, -0.2097,  1.2974]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2502, -0.0614],
        [ 0.1993, -0.0841]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Q-values: tensor([[0.1173],
        [0.3428],
        [0.1509]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4091],
        [0.1197]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2933, -0.0595],
        [ 0.1969, -0.0682]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Q-values: tensor([[ 0.0329],
        [-0.0832],
        [ 0.0031]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0700],
        [-0.0284]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2933, -0.0595],
        [ 0.1969, -0.0682]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Q-values: tensor([[-0.1380],
        [-0.5383],
        [-0.4084]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.6433],
        [-0.5116]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.2168,  1.0731, -1.9800,  0.0000,
          0.0000,  0.0000, -1.6259,  2.4547],
        [ 0.0000,  0.0000,  1.2168, -1.0731,  0.0000,  0.0000, -0.7632, -1.0731,
          0.0000,  0.0000, -0.4090,  1.3816]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2933, -0.0595],
        [ 0.1969, -0.0682]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Q-values: tensor([[0.1167],
        [0.3189],
        [0.1405]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4292],
        [0.1239]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3414, -0.0612],
        [ 0.2015, -0.0544]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Q-values: tensor([[ 0.0324],
        [-0.0863],
        [-0.0061]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0712],
        [-0.0340]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3414, -0.0612],
        [ 0.2015, -0.0544]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Q-values: tensor([[-0.1392],
        [-0.5737],
        [-0.4406]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.6737],
        [-0.5446]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3133,  1.0049, -2.2733,  0.0000,
          0.0000,  0.0000, -1.9192,  2.4547],
        [ 0.0000,  0.0000,  1.3133, -1.0049,  0.0000,  0.0000, -0.9600, -1.0049,
          0.0000,  0.0000, -0.6059,  1.4497]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3414, -0.0612],
        [ 0.2015, -0.0544]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Q-values: tensor([[0.1162],
        [0.2978],
        [0.1384]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4515],
        [0.1322]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3977, -0.0673],
        [ 0.2211, -0.0401]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Q-values: tensor([[ 0.0318],
        [-0.0917],
        [-0.0193]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0721],
        [-0.0398]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3977, -0.0673],
        [ 0.2211, -0.0401]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Q-values: tensor([[-0.1406],
        [-0.6148],
        [-0.4722]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.7088],
        [-0.5777]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.4532,  0.9506, -2.6147,  0.0000,
          0.0000,  0.0000, -2.2606,  2.4547],
        [ 0.0000,  0.0000,  1.4532, -0.9506,  0.0000,  0.0000, -1.1615, -0.9506,
          0.0000,  0.0000, -0.8074,  1.5041]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3977, -0.0673],
        [ 0.2211, -0.0401]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Q-values: tensor([[0.1158],
        [0.2945],
        [0.1359]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4878],
        [0.1400]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.4581, -0.0652],
        [ 0.2506, -0.0245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Q-values: tensor([[ 0.0312],
        [-0.1084],
        [-0.0346]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0814],
        [-0.0456]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.4581, -0.0652],
        [ 0.2506, -0.0245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Q-values: tensor([[-0.1420],
        [-0.6675],
        [-0.5074]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.7545],
        [-0.6027]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.6297,  0.9105, -3.0124,  0.0000,
          0.0000,  0.0000, -2.6583,  2.4547],
        [ 0.0000,  0.0000,  1.6297, -0.9105,  0.0000,  0.0000, -1.3827, -0.9105,
          0.0000,  0.0000, -1.0285,  1.5442]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.4581, -0.0652],
        [ 0.2506, -0.0245]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Q-values: tensor([[0.1155],
        [0.2964],
        [0.1305]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.5416],
        [0.1489]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5229, -0.0614],
        [ 0.2924, -0.0023]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Q-values: tensor([[ 0.0307],
        [-0.1314],
        [-0.0515]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0925],
        [-0.0522]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5229, -0.0614],
        [ 0.2924, -0.0023]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Q-values: tensor([[-0.1435],
        [-0.7352],
        [-0.5401]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.8056],
        [-0.6318]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.8372,  0.8860, -3.4705,  0.0000,
          0.0000,  0.0000, -3.1164,  2.4547],
        [ 0.0000,  0.0000,  1.8372, -0.8860,  0.0000,  0.0000, -1.6333, -0.8860,
          0.0000,  0.0000, -1.2792,  1.5687]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5229, -0.0614],
        [ 0.2924, -0.0023]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Q-values: tensor([[0.1155],
        [0.3066],
        [0.1210]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.6044],
        [0.1617]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5917, -0.0545],
        [ 0.3385,  0.0157]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Q-values: tensor([[ 0.0303],
        [-0.1621],
        [-0.0640]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1059],
        [-0.0543]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5917, -0.0545],
        [ 0.3385,  0.0157]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Q-values: tensor([[-0.1450],
        [-0.8154],
        [-0.5779]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.8629],
        [-0.6650]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.0677,  0.8836, -3.9934,  0.0000,
          0.0000,  0.0000, -3.6393,  2.4547],
        [ 0.0000,  0.0000,  2.0677, -0.8836,  0.0000,  0.0000, -1.9257, -0.8836,
          0.0000,  0.0000, -1.5716,  1.5710]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.5917, -0.0545],
        [ 0.3385,  0.0157]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Q-values: tensor([[0.1156],
        [0.3344],
        [0.1246]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.6742],
        [0.1743]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.6605, -0.0372],
        [ 0.3892,  0.0389]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Q-values: tensor([[ 0.0299],
        [-0.1985],
        [-0.0751]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0615],
        [-0.1546]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.6605, -0.0372],
        [ 0.3892,  0.0389]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Q-values: tensor([[-0.1465],
        [-0.9105],
        [-0.6211]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3452],
        [-0.3481]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.3208,  0.8993, -4.5851,  0.0000,
          0.0000,  0.0000, -4.2310,  2.4547],
        [ 0.0000,  0.0000,  2.3208, -0.8993,  0.0000,  0.0000, -2.2643, -0.8993,
          0.0000,  0.0000, -1.9101,  1.5553]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.6605, -0.0372],
        [ 0.3892,  0.0389]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Q-values: tensor([[0.1161],
        [0.3915],
        [0.1322]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2126],
        [0.4967]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1679,  0.0069],
        [ 0.2803, -0.1622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Q-values: tensor([[ 0.0297],
        [-0.1193],
        [-0.0081]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0470],
        [-0.1779]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1679,  0.0069],
        [ 0.2803, -0.1622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Q-values: tensor([[-0.1468],
        [-0.3900],
        [-0.6006]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3086],
        [-0.4347]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.5921,  0.9382,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.5921, -0.9382,  0.0000,  0.0000, -2.6535, -0.9382,
          1.9180,  4.4985, -2.2994,  1.5164]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1679,  0.0069],
        [ 0.2803, -0.1622]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Q-values: tensor([[0.1165],
        [0.2890],
        [0.4298]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1804],
        [0.5392]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1412,  0.0215],
        [ 0.2656, -0.0764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Q-values: tensor([[ 0.0293],
        [-0.0959],
        [-0.0414]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0689],
        [-0.1927]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1412,  0.0215],
        [ 0.2656, -0.0764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Q-values: tensor([[-0.1466],
        [-0.3380],
        [-0.6437]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4316],
        [-0.5133]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -2.1438,  0.7691,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.1438, -0.7691,  0.0000,  0.0000, -2.9338, -0.7760,
          1.6376,  4.6607, -2.5797,  1.6786]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1412,  0.0215],
        [ 0.2656, -0.0764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Q-values: tensor([[0.1170],
        [0.2545],
        [0.5034]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.6756],
        [0.5728]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3085,  0.3382],
        [ 0.2587, -0.0404]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Q-values: tensor([[ 0.0288],
        [-0.2600],
        [-0.0794]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0417],
        [-0.1883]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3085,  0.3382],
        [ 0.2587, -0.0404]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Q-values: tensor([[-0.1464],
        [-0.4654],
        [-0.6902]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2889],
        [-0.5589]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7370,  0.6712, -4.9365, -0.0284,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7370, -0.6712,  0.0000,  0.0000, -3.1994, -0.6996,
          1.3720,  4.7371, -2.8453,  1.7550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3085,  0.3382],
        [ 0.2587, -0.0404]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Q-values: tensor([[0.1176],
        [0.2701],
        [0.5673]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1288],
        [0.5897]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1224,  0.0657],
        [ 0.3199, -0.0444]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Q-values: tensor([[ 0.0284],
        [-0.0891],
        [-0.0934]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0291],
        [-0.1868]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1224,  0.0657],
        [ 0.3199, -0.0444]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Q-values: tensor([[-0.1457],
        [-0.3081],
        [-0.6755]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2461],
        [-0.6590]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.7868,  0.2926,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.7868, -0.2926,  0.0000,  0.0000, -3.4582, -0.6592,
          1.1133,  4.7775, -3.1040,  1.7954]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1224,  0.0657],
        [ 0.3199, -0.0444]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Q-values: tensor([[0.1180],
        [0.2034],
        [0.6169]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1084],
        [0.6232]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0880,  0.0730],
        [ 0.2989, -0.0296]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Q-values: tensor([[ 0.0279],
        [-0.0604],
        [-0.1194]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1952],
        [-0.1788]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0880,  0.0730],
        [ 0.2989, -0.0296]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Q-values: tensor([[-0.1450],
        [-0.2506],
        [-0.7228]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3786],
        [-0.7555]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.3445,  0.1825,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.3445, -0.1825,  0.0000,  0.0000, -3.7781, -0.6148,
          0.7934,  4.8219, -3.4240,  1.8398]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0880,  0.0730],
        [ 0.2989, -0.0296]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Q-values: tensor([[0.1183],
        [0.1714],
        [0.6721]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3594],
        [0.6566]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1305, -0.1672],
        [ 0.2835, -0.0205]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Q-values: tensor([[ 0.0273],
        [-0.0838],
        [-0.1394]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0190],
        [-0.1867]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1305, -0.1672],
        [ 0.2835, -0.0205]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Q-values: tensor([[-0.1445],
        [-0.6271],
        [-0.7739]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1899],
        [-0.8542]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.9575,  0.0798,  0.0000,  0.0000,
         -0.4631,  4.9314,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.9575, -0.0798,  0.0000,  0.0000, -4.0770, -0.5852,
          0.4945,  4.8516, -3.7229,  1.8695]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1305, -0.1672],
        [ 0.2835, -0.0205]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Q-values: tensor([[0.1187],
        [0.3872],
        [0.7134]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0841],
        [0.7023]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0278,  0.0792],
        [ 0.2676, -0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Q-values: tensor([[ 0.0267],
        [-0.0171],
        [-0.1740]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0202],
        [-0.1898]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0278,  0.0792],
        [ 0.2676, -0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Q-values: tensor([[-0.1437],
        [-0.1815],
        [-0.8449]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1740],
        [-0.9418]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.8045,  0.2265,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.8045, -0.2265,  0.0000,  0.0000, -4.3605, -0.5647,
          0.2110,  4.8721, -4.0064,  1.8900]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0278,  0.0792],
        [ 0.2676, -0.0200]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Q-values: tensor([[0.1187],
        [0.1480],
        [0.7647]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0748],
        [0.7585]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0082,  0.0885],
        [ 0.2711, -0.0203]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Q-values: tensor([[ 0.0261],
        [-0.0099],
        [-0.1847]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1857],
        [-0.1935]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0082,  0.0885],
        [ 0.2711, -0.0203]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Q-values: tensor([[-0.1432],
        [-0.1580],
        [-0.9055]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3466],
        [-1.0258]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5091,  0.1273,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.5091, -0.1273,  0.0000,  0.0000, -4.6281, -0.5447,
         -0.0566,  4.8921, -4.2740,  1.9100]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0082,  0.0885],
        [ 0.2711, -0.0203]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Q-values: tensor([[0.1186],
        [0.1292],
        [0.8184]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3703],
        [0.8080]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0803, -0.0900],
        [ 0.2758, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0253],
        [-0.1090],
        [-0.1970]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0255],
        [-0.1827]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0803, -0.0900],
        [ 0.2758, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Q-values: tensor([[-0.1431],
        [-0.5774],
        [-0.9758]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1410],
        [-0.3275]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2462,  0.0185,  0.0000,  0.0000,
         -0.5740,  4.9309,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2462, -0.0185,  0.0000,  0.0000, -4.8992, -0.5244,
         -0.3278,  4.9124, -4.5451,  1.9303]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0803, -0.0900],
        [ 0.2758, -0.0258]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Q-values: tensor([[0.1186],
        [0.3889],
        [0.8656]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0607],
        [0.3698]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0546,  0.1133],
        [-0.1337, -0.3151]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0247],
        [-0.0031],
        [-0.0783]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1785],
        [-0.0167]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0546,  0.1133],
        [-0.1337, -0.3151]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1412],
        [-0.1218],
        [-0.5653]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3778],
        [-0.1400]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0507,  0.0828,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0507, -0.0828,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6036,  4.9381,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0546,  0.1133],
        [-0.1337, -0.3151]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1178],
        [0.1093],
        [0.3653]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3623],
        [0.0869]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0094, -0.0566],
        [ 0.2077, -0.0326]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0241],
        [-0.1204],
        [ 0.0010]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0294],
        [-0.0240]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0094, -0.0566],
        [ 0.2077, -0.0326]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1393],
        [-0.5460],
        [-0.1096]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1773],
        [-0.1512]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2390, -0.3456,  0.0000,  0.0000,
         -0.7089,  4.9076,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.2390,  0.3456,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0094, -0.0566],
        [ 0.2077, -0.0326]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1170],
        [0.3172],
        [0.1335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0808],
        [0.0759]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0683,  0.1355],
        [ 0.1930, -0.0281]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0236],
        [-0.0140],
        [-0.0013]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1735],
        [-0.0272]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0683,  0.1355],
        [ 0.1930, -0.0281]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1371],
        [-0.1469],
        [-0.1252]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3668],
        [-0.1609]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0406, -0.3216,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0406,  0.3216,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0683,  0.1355],
        [ 0.1930, -0.0281]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1160],
        [0.1197],
        [0.1190]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3584],
        [0.0810]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0422, -0.0045],
        [ 0.2044, -0.0414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0230],
        [-0.1228],
        [-0.0012]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1725],
        [-0.0327]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0422, -0.0045],
        [ 0.2044, -0.0414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1353],
        [-0.4639],
        [-0.1344]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3496],
        [-0.1788]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0841, -0.4852,  0.0000,  0.0000,
         -0.7865,  4.8288,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0841,  0.4852,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0422, -0.0045],
        [ 0.2044, -0.0414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1152],
        [0.2498],
        [0.1202]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3607],
        [0.0816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0581,  0.0250],
        [ 0.1916, -0.0360]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0223],
        [-0.1250],
        [ 0.0031]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1714],
        [-0.0244]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0581,  0.0250],
        [ 0.1916, -0.0360]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1336],
        [-0.4171],
        [-0.1568]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3297],
        [-0.1985]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3307, -0.5221,  0.0000,  0.0000,
         -0.7442,  4.8333,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.3307,  0.5221,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0581,  0.0250],
        [ 0.1916, -0.0360]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1145],
        [0.2321],
        [0.1119]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3592],
        [0.0885]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0765,  0.0627],
        [ 0.1782, -0.0292]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0216],
        [-0.1279],
        [ 0.0050]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1685],
        [-0.0158]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0765,  0.0627],
        [ 0.1782, -0.0292]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1320],
        [-0.3705],
        [-0.1802]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3084],
        [-0.2221]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5805, -0.5831,  0.0000,  0.0000,
         -0.6861,  4.8083,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5805,  0.5831,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0765,  0.0627],
        [ 0.1782, -0.0292]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1139],
        [0.2185],
        [0.1085]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3523],
        [0.1078]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0959,  0.1089],
        [ 0.1647, -0.0099]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0208],
        [-0.1352],
        [ 0.0119]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0590],
        [-0.1655],
        [-0.0063]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0959,  0.1089],
        [ 0.1647, -0.0099]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1304],
        [-0.3238],
        [-0.2056]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5006],
        [-0.2834],
        [-0.2462]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.8351, -0.6750,  0.0000,  0.0000,
         -0.6096,  4.7456,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8351,  0.6750,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.0959,  0.1089],
        [ 0.1647, -0.0099]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1135],
        [0.2019],
        [0.1151]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0692],
        [0.3397],
        [0.1376]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3370,  0.0150],
        [-0.1046,  0.1561],
        [ 0.1517,  0.0127]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0150,
          0.0000,  0.0000,  0.3541,  2.4397],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3520, -0.9372,  0.0000,  0.0000,
         -0.4091,  4.4806, -4.6265,  1.4985],
        [ 0.0000,  0.0000, -1.3520,  0.9372,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0213],
        [-0.1410],
        [ 0.0199]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3370,  0.0150],
        [-0.1046,  0.1561],
        [ 0.1517,  0.0127]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0150,
          0.0000,  0.0000,  0.3541,  2.4397],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3520, -0.9372,  0.0000,  0.0000,
         -0.4091,  4.4806, -4.6265,  1.4985],
        [ 0.0000,  0.0000, -1.3520,  0.9372,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4364],
        [-0.2743],
        [-0.2334]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.3541,  2.4547],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0957, -0.7938,  0.0000,  0.0000,
         -0.5137,  4.6367,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -1.0957,  0.7938,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3370,  0.0150],
        [-0.1046,  0.1561],
        [ 0.1517,  0.0127]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0150,
          0.0000,  0.0000,  0.3541,  2.4397],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.3520, -0.9372,  0.0000,  0.0000,
         -0.4091,  4.4806, -4.6265,  1.4985],
        [ 0.0000,  0.0000, -1.3520,  0.9372,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1986],
        [0.1869],
        [0.1319]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
Episode 4: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6748,  0.9941,
          0.0000,  0.0000,  1.6264, -3.2966],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5753,  2.9228,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5753, -2.9228,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6751,  2.1597,  0.0000,  0.0000]])
Actions: tensor([[-0.0749,  0.4178],
        [-0.1245, -0.1062],
        [ 0.3268, -0.1526]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0558],
        [ 0.0774],
        [-0.2335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0080],
        [-0.0034],
        [-0.1665]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6748,  0.9941,
          0.0000,  0.0000,  1.6264, -3.2966],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5753,  2.9228,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5753, -2.9228,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6751,  2.1597,  0.0000,  0.0000]])
Actions: tensor([[-0.0749,  0.4178],
        [-0.1245, -0.1062],
        [ 0.3268, -0.1526]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Q-values: tensor([[-0.0042],
        [-0.0821],
        [-0.2275]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0799],
        [-0.2288],
        [-0.3774]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6748,  0.9941,
          0.0000,  0.0000,  1.6264, -3.2966],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5753,  2.9228,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.5753, -2.9228,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6751,  2.1597,  0.0000,  0.0000]])
Actions: tensor([[-0.0749,  0.4178],
        [-0.1245, -0.1062],
        [ 0.3268, -0.1526]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Q-values: tensor([[0.0216],
        [0.5101],
        [0.3549]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0898],
        [0.0773],
        [0.3948]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Actions: tensor([[-0.0671,  0.4045],
        [-0.3771, -0.2663],
        [ 0.2869, -0.1678]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Q-values: tensor([[-0.0145],
        [ 0.1520],
        [-0.2056]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0098],
        [ 0.0423],
        [-0.1687]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Actions: tensor([[-0.0671,  0.4045],
        [-0.3771, -0.2663],
        [ 0.2869, -0.1678]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0736],
        [-0.2163],
        [-0.2528]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0933],
        [-0.1959],
        [-0.4235]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7013, -3.7144],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.1240,  2.8763,  3.2662, -0.0789,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.1240, -2.8763,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.0019,  2.3123,  0.0000,  0.0000]])
Actions: tensor([[-0.0671,  0.4045],
        [-0.3771, -0.2663],
        [ 0.2869, -0.1678]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Q-values: tensor([[-0.0228],
        [ 0.3951],
        [ 0.3633]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0912],
        [0.0516],
        [0.3913]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Actions: tensor([[-0.0785,  0.4268],
        [-0.4834, -0.3031],
        [ 0.2133, -0.1690]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Q-values: tensor([[-0.0202],
        [ 0.2345],
        [-0.1669]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0101],
        [-0.0193],
        [-0.1806]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Actions: tensor([[-0.0785,  0.4268],
        [-0.4834, -0.3031],
        [ 0.2133, -0.1690]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0848],
        [-0.2215],
        [-0.2835]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1028],
        [-0.1950],
        [-0.4624]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.7684, -4.1188],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4600,  2.7874,  4.6228,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.4600, -2.7874,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2888,  2.4802,  0.0000,  0.0000]])
Actions: tensor([[-0.0785,  0.4268],
        [-0.4834, -0.3031],
        [ 0.2133, -0.1690]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Q-values: tensor([[-0.0406],
        [ 0.3774],
        [ 0.3614]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0943],
        [0.1886],
        [0.3694]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Actions: tensor([[-0.0908,  0.4507],
        [-0.0852, -0.0284],
        [ 0.1228, -0.1558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Q-values: tensor([[-0.0285],
        [ 0.1487],
        [-0.1365]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1400],
        [-0.0373],
        [-0.3725]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Actions: tensor([[-0.0908,  0.4507],
        [-0.0852, -0.0284],
        [ 0.1228, -0.1558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Q-values: tensor([[ 0.0965],
        [-0.0147],
        [-0.3219]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3895],
        [-0.5020],
        [-1.0811]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.8469, -4.5457],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2366,  2.6184,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.2366, -2.6184,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.5021,  2.6492,  0.0000,  0.0000]])
Actions: tensor([[-0.0908,  0.4507],
        [-0.0852, -0.0284],
        [ 0.1228, -0.1558]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Q-values: tensor([[-0.0556],
        [ 0.3648],
        [ 0.3401]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4821],
        [0.5069],
        [0.7092]])
States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Actions: tensor([[-0.7375, -0.0293],
        [ 0.1463,  0.2618],
        [-0.3075, -0.2764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Q-values: tensor([[-0.0312],
        [ 0.0564],
        [-0.1385]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0988],
        [-0.0309],
        [-0.0965]])
States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Actions: tensor([[-0.7375, -0.0293],
        [ 0.1463,  0.2618],
        [-0.3075, -0.2764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Q-values: tensor([[-0.3064],
        [-0.0539],
        [-0.3295]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3064],
        [-0.4592],
        [-1.0609]])
States: tensor([[ 0.0000,  0.0000, -2.7569,  0.0000, -2.3123,  2.4626,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.7569,  0.0000,  0.0000,  0.0000,  0.4447,  2.4626, -3.6175,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 2.3123, -2.4626, -0.4447, -2.4626,  0.0000,  0.0000, -4.0622, -2.4626,
         -2.6249,  2.8050,  4.2501,  2.5805]])
Actions: tensor([[-0.7375, -0.0293],
        [ 0.1463,  0.2618],
        [-0.3075, -0.2764]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Q-values: tensor([[0.6162],
        [0.5376],
        [0.7717]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3787],
        [0.5731],
        [0.5843]])
States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Actions: tensor([[-0.6514, -0.0913],
        [ 0.2123,  0.3357],
        [-0.1110, -0.3000]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0038],
        [ 0.0031],
        [-0.0772]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0360],
        [-0.0322],
        [-0.0667]])
States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Actions: tensor([[-0.6514, -0.0913],
        [ 0.2123,  0.3357],
        [-0.1110, -0.3000]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Q-values: tensor([[-0.1985],
        [-0.0662],
        [-0.5119]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3144],
        [-0.4457],
        [-0.9668]])
States: tensor([[ 0.0000,  0.0000, -1.8732,  0.2618, -1.8823,  2.1862,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8732, -0.2618,  0.0000,  0.0000, -0.0091,  1.9244, -3.7638, -0.2618,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.8823, -2.1862,  0.0091, -1.9244,  0.0000,  0.0000, -3.7547, -2.1862,
         -2.3174,  3.0813,  0.0000,  0.0000]])
Actions: tensor([[-0.6514, -0.0913],
        [ 0.2123,  0.3357],
        [-0.1110, -0.3000]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Q-values: tensor([[0.5124],
        [0.5152],
        [0.4513]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.9511],
        [0.6170],
        [0.5965]])
States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Actions: tensor([[-0.7472, -0.0913],
        [ 0.2586,  0.4233],
        [-0.0379, -0.2964]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[ 0.0679],
        [-0.0505],
        [-0.0750]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0186],
        [ 0.0039],
        [-0.0398]])
States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Actions: tensor([[-0.7472, -0.0913],
        [ 0.2586,  0.4233],
        [-0.0379, -0.2964]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[-0.0076],
        [-0.1123],
        [-0.4648]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2769],
        [-0.5048],
        [-0.8624]])
States: tensor([[ 0.0000,  0.0000, -1.0094,  0.5974, -1.3418,  1.8862, -4.9855,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.0094, -0.5974,  0.0000,  0.0000, -0.3324,  1.2888, -3.9761, -0.5974,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.3418, -1.8862,  0.3324, -1.2888,  0.0000,  0.0000, -3.6437, -1.8862,
         -2.2064,  3.3813,  0.0000,  0.0000]])
Actions: tensor([[-0.7472, -0.0913],
        [ 0.2586,  0.4233],
        [-0.0379, -0.2964]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Q-values: tensor([[0.6701],
        [0.4692],
        [0.4485]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.7555],
        [0.6738],
        [0.6024]])
States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.6772, -0.0859],
        [ 0.2940,  0.4901],
        [ 0.0656, -0.2755]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0813],
        [-0.0658],
        [-0.0849]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0219],
        [-0.0791],
        [-0.0885]])
States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.6772, -0.0859],
        [ 0.2940,  0.4901],
        [ 0.0656, -0.2755]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0250],
        [-0.2062],
        [-0.4171]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2770],
        [-0.9936],
        [-0.7545]])
States: tensor([[ 0.0000e+00,  0.0000e+00, -3.6589e-03,  1.0208e+00, -6.3249e-01,
          1.5898e+00, -4.2384e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6589e-03, -1.0208e+00,  0.0000e+00,  0.0000e+00, -6.2883e-01,
          5.6907e-01, -4.2347e+00, -1.0208e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3249e-01, -1.5898e+00,  6.2883e-01, -5.6907e-01,  0.0000e+00,
          0.0000e+00, -3.6059e+00, -1.5898e+00, -2.1686e+00,  3.6777e+00,
          0.0000e+00,  0.0000e+00]])
Actions: tensor([[-0.6772, -0.0859],
        [ 0.2940,  0.4901],
        [ 0.0656, -0.2755]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Q-values: tensor([[0.5913],
        [0.4490],
        [0.4495]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.5624],
        [0.7460],
        [0.6340]])
States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Actions: tensor([[-0.6148, -0.0289],
        [ 0.2324,  0.4152],
        [ 0.1278, -0.2591]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1110],
        [-0.2209],
        [-0.1390]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0682],
        [-0.0429],
        [-0.1326]])
States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Actions: tensor([[-0.6148, -0.0289],
        [ 0.2324,  0.4152],
        [ 0.1278, -0.2591]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0435],
        [-0.5738],
        [-0.3604]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2824],
        [-0.5656],
        [-0.6657]])
States: tensor([[ 0.0000,  0.0000,  0.9676,  1.5108,  0.1103,  1.3144, -3.5611,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9676, -1.5108,  0.0000,  0.0000, -0.8573, -0.1965, -4.5287, -1.5108,
         -3.0914,  3.7567,  0.0000,  0.0000],
        [-0.1103, -1.3144,  0.8573,  0.1965,  0.0000,  0.0000, -3.6714, -1.3144,
         -2.2341,  3.9532,  0.0000,  0.0000]])
Actions: tensor([[-0.6148, -0.0289],
        [ 0.2324,  0.4152],
        [ 0.1278, -0.2591]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Q-values: tensor([[0.5383],
        [0.5146],
        [0.4657]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4394],
        [0.4130],
        [0.6811]])
States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Actions: tensor([[-0.5260,  0.1111],
        [-0.1891,  0.1202],
        [ 0.1297, -0.2445]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1747],
        [ 0.1318],
        [-0.1612]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1171],
        [0.0314],
        [0.0789]])
States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Actions: tensor([[-0.5260,  0.1111],
        [-0.1891,  0.1202],
        [ 0.1297, -0.2445]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0546],
        [-0.2298],
        [-0.3004]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3233],
        [-0.5254],
        [-0.4234]])
States: tensor([[ 0.0000,  0.0000,  1.8148,  1.9260,  0.8529,  1.0552, -2.9463,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.8148, -1.9260,  0.0000,  0.0000, -0.9619, -0.8708,  0.0000,  0.0000,
         -3.3238,  3.3415,  3.5511,  3.1171],
        [-0.8529, -1.0552,  0.9619,  0.8708,  0.0000,  0.0000, -3.7992, -1.0552,
         -2.3619,  4.2123,  0.0000,  0.0000]])
Actions: tensor([[-0.5260,  0.1111],
        [-0.1891,  0.1202],
        [ 0.1297, -0.2445]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5040],
        [0.6816],
        [0.4963]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3414],
        [0.3911],
        [0.6896]])
States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4347,  0.2091],
        [-0.2512,  0.1954],
        [ 0.3272,  0.0770]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2277],
        [0.2008],
        [0.0014]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0590],
        [ 0.0757],
        [ 0.0671]])
States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4347,  0.2091],
        [-0.2512,  0.1954],
        [ 0.3272,  0.0770]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0076],
        [-0.2120],
        [-0.1796]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3334],
        [-0.9362],
        [-0.4504]])
States: tensor([[ 0.0000,  0.0000,  2.1517,  1.9351,  1.5087,  0.6997, -2.4203, -0.1111,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-2.1517, -1.9351,  0.0000,  0.0000, -0.6430, -1.2355,  0.0000,  0.0000,
         -3.1347,  3.2213,  3.7402,  2.9969],
        [-1.5087, -0.6997,  0.6430,  1.2355,  0.0000,  0.0000, -3.9290, -0.8108,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4347,  0.2091],
        [-0.2512,  0.1954],
        [ 0.3272,  0.0770]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4299],
        [0.6662],
        [0.4675]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.5390],
        [0.8557],
        [0.8609]])
States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3518,  0.4981],
        [-0.2097,  0.6751],
        [ 0.2766,  0.1305]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0370],
        [0.2026],
        [0.0239]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0255],
        [ 0.1374],
        [ 0.0816]])
States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3518,  0.4981],
        [-0.2097,  0.6751],
        [ 0.2766,  0.1305]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0352],
        [-0.6223],
        [-0.2515]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2684],
        [-0.4620],
        [-0.4747]])
States: tensor([[ 0.0000,  0.0000,  2.3352,  1.9215,  2.2706,  0.5676, -1.9856, -0.3202,
         -0.5483,  4.9474,  0.0000,  0.0000],
        [-2.3352, -1.9215,  0.0000,  0.0000, -0.0646, -1.3539, -4.3208, -2.2417,
         -2.8835,  3.0259,  3.9915,  2.8015],
        [-2.2706, -0.5676,  0.0646,  1.3539,  0.0000,  0.0000, -4.2562, -0.8877,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3518,  0.4981],
        [-0.2097,  0.6751],
        [ 0.2766,  0.1305]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5224],
        [1.0327],
        [0.6185]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4821],
        [0.4028],
        [1.0755]])
States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3081,  0.5246],
        [-0.2733,  0.4246],
        [ 0.3339,  0.1716]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0769],
        [0.2343],
        [0.0612]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0137],
        [0.1626],
        [0.0537]])
States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3081,  0.5246],
        [-0.2733,  0.4246],
        [ 0.3339,  0.1716]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0132],
        [-0.2652],
        [-0.2834]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2231],
        [-0.4295],
        [-0.4149]])
States: tensor([[ 0.0000,  0.0000,  2.4773,  2.0985,  2.8989,  0.2000, -1.6338, -0.8183,
         -0.1965,  4.4493,  0.0000,  0.0000],
        [-2.4773, -2.0985,  0.0000,  0.0000,  0.4217, -1.8985,  0.0000,  0.0000,
         -2.6738,  2.3507,  4.2011,  2.1263],
        [-2.8989, -0.2000, -0.4217,  1.8985,  0.0000,  0.0000, -4.5328, -1.0183,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3081,  0.5246],
        [-0.2733,  0.4246],
        [ 0.3339,  0.1716]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5009],
        [0.6608],
        [0.8019]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4226],
        [0.4188],
        [0.6364]])
States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2715,  0.5412],
        [-0.2835,  0.4821],
        [ 0.2814, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0997],
        [0.2329],
        [0.1266]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0802],
        [0.2002],
        [0.0926]])
States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2715,  0.5412],
        [-0.2835,  0.4821],
        [ 0.2814, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0009],
        [-0.2880],
        [-0.2211]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2364],
        [-0.4337],
        [-0.5388]])
States: tensor([[ 0.0000,  0.0000,  2.5120,  1.9986,  3.5409, -0.1529, -1.3258, -1.3428,
          0.1115,  3.9247,  0.0000,  0.0000],
        [-2.5120, -1.9986,  0.0000,  0.0000,  1.0289, -2.1515,  0.0000,  0.0000,
         -2.4005,  1.9261,  4.4744,  1.7017],
        [-3.5409,  0.1529, -1.0289,  2.1515,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2715,  0.5412],
        [-0.2835,  0.4821],
        [ 0.2814, -0.0327]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4578],
        [0.6412],
        [0.6652]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4013],
        [0.4665],
        [0.7520]])
States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2500,  0.5605],
        [-0.2881,  0.5584],
        [ 0.3304,  0.0503]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Q-values: tensor([[0.1396],
        [0.2322],
        [0.1708]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1435],
        [0.0789],
        [0.3032]])
States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2500,  0.5605],
        [-0.2881,  0.5584],
        [ 0.3304,  0.0503]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Q-values: tensor([[-0.0506],
        [-0.3336],
        [-0.3245]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2790],
        [-0.4440],
        [-0.8990]])
States: tensor([[ 0.0000,  0.0000,  2.5000,  1.9395,  4.0938, -0.7268, -1.0543, -1.8840,
          0.3830,  3.3835,  0.0000,  0.0000],
        [-2.5000, -1.9395,  0.0000,  0.0000,  1.5938, -2.6663,  0.0000,  0.0000,
         -2.1170,  1.4440,  4.7579,  1.2196],
        [-4.0938,  0.7268, -1.5938,  2.6663,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2500,  0.5605],
        [-0.2881,  0.5584],
        [ 0.3304,  0.0503]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Q-values: tensor([[0.4645],
        [0.6035],
        [0.7722]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4046],
        [0.2448],
        [0.8604]])
States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Actions: tensor([[-0.2424,  0.5875],
        [-0.1888,  0.5260],
        [ 0.3970, -0.3414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Q-values: tensor([[0.1913],
        [0.0564],
        [0.5658]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0244],
        [ 0.1154],
        [ 0.0905]])
States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Actions: tensor([[-0.2424,  0.5875],
        [-0.1888,  0.5260],
        [ 0.3970, -0.3414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Q-values: tensor([[-0.1277],
        [-0.2828],
        [-0.5879]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1249],
        [-0.5718],
        [-1.0480]])
States: tensor([[ 0.0000,  0.0000,  2.4619,  1.9374,  4.6742, -1.2371, -0.8043, -2.4445,
          0.6330,  2.8230,  0.0000,  0.0000],
        [-2.4619, -1.9374,  0.0000,  0.0000,  2.2123, -3.1744,  0.0000,  0.0000,
         -1.8289,  0.8856,  0.0000,  0.0000],
        [-4.6742,  1.2371, -2.2123,  3.1744,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8337,  3.8356]])
Actions: tensor([[-0.2424,  0.5875],
        [-0.1888,  0.5260],
        [ 0.3970, -0.3414]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Q-values: tensor([[0.4879],
        [0.2171],
        [1.3139]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3615],
        [0.2816],
        [0.5049]])
States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Actions: tensor([[-0.3162,  0.5336],
        [-0.2586,  0.6266],
        [ 0.4201, -0.1774]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Q-values: tensor([[0.0318],
        [0.0860],
        [0.4065]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0027],
        [ 0.0374],
        [-0.0941]])
States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Actions: tensor([[-0.3162,  0.5336],
        [-0.2586,  0.6266],
        [ 0.4201, -0.1774]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Q-values: tensor([[-0.0204],
        [-0.3677],
        [-0.6560]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1391],
        [-0.1612],
        [-0.7703]])
States: tensor([[ 0.0000,  0.0000,  2.5155,  1.8759,  0.0000,  0.0000, -0.5619, -3.0320,
          0.8754,  2.2355,  0.0000,  0.0000],
        [-2.5155, -1.8759,  0.0000,  0.0000,  2.7981, -4.0418,  0.0000,  0.0000,
         -1.6401,  0.3596,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -2.7981,  4.0418,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.4367,  4.1770]])
Actions: tensor([[-0.3162,  0.5336],
        [-0.2586,  0.6266],
        [ 0.4201, -0.1774]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Q-values: tensor([[0.4347],
        [0.1957],
        [0.9458]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3795],
        [0.3616],
        [0.1686]])
States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Actions: tensor([[-0.3978,  0.5496],
        [ 0.0640,  0.1570],
        [ 0.1268, -0.3806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0639],
        [0.0390],
        [0.3038]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0389],
        [ 0.0567],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Actions: tensor([[-0.3978,  0.5496],
        [ 0.0640,  0.1570],
        [ 0.1268, -0.3806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0354],
        [-0.2297],
        [-0.5241]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1369],
        [-0.1787],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.5730,  1.9688,  0.0000,  0.0000, -0.2458, -3.5656,
          1.1915,  1.7019,  0.0000,  0.0000],
        [-2.5730, -1.9688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3815, -0.2670,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.0167,  4.3544]])
Actions: tensor([[-0.3978,  0.5496],
        [ 0.0640,  0.1570],
        [ 0.1268, -0.3806]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4422],
        [0.4135],
        [0.6918]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3576],
        [0.3907],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4799,  0.5011],
        [ 0.0522,  0.1714],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1052],
        [0.0529],
        [0.0307]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1040],
        [ 0.0717],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4799,  0.5011],
        [ 0.0522,  0.1714],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0601],
        [-0.2608],
        [-0.1281]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2051],
        [-0.2120],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.8828,  1.5762,  0.0000,  0.0000,  0.0000, -4.1153,
          1.4373,  1.1523,  0.0000,  0.0000],
        [-2.8828, -1.5762,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4455, -0.4240,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4799,  0.5011],
        [ 0.0522,  0.1714],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4451],
        [0.4383],
        [0.1470]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3384],
        [0.3937],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5483,  0.4350],
        [ 0.0299,  0.1843],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Q-values: tensor([[0.1649],
        [0.0597],
        [0.0298]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0861],
        [ 0.0919],
        [-0.1095]])
States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5483,  0.4350],
        [ 0.0299,  0.1843],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Q-values: tensor([[-0.1376],
        [-0.2984],
        [-0.1317]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1246],
        [-0.2447],
        [-0.8301]])
States: tensor([[ 0.0000,  0.0000,  2.9350,  1.2465,  0.0000,  0.0000,  0.0000, -4.6164,
          1.4373,  0.6512,  0.0000,  0.0000],
        [-2.9350, -1.2465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4977, -0.5954,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5483,  0.4350],
        [ 0.0299,  0.1843],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Q-values: tensor([[0.4341],
        [0.4372],
        [0.1453]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0795],
        [0.4083],
        [0.1372]])
States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Actions: tensor([[-0.1991,  0.4269],
        [ 0.0127,  0.1938],
        [ 0.1272, -0.3490]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0106],
        [ 0.0685],
        [ 0.2775]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0924],
        [ 0.1124],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Actions: tensor([[-0.1991,  0.4269],
        [ 0.0127,  0.1938],
        [ 0.1272, -0.3490]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0837],
        [-0.3361],
        [-0.5697]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1232],
        [-0.2710],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.9649,  0.9959,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373,  0.2162,  0.0000,  0.0000],
        [-2.9649, -0.9959,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5276, -0.7797,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  1.5820,  4.7229]])
Actions: tensor([[-0.1991,  0.4269],
        [ 0.0127,  0.1938],
        [ 0.1272, -0.3490]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0989],
        [0.4406],
        [0.6217]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1006],
        [0.4375],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2230,  0.3865],
        [-0.0047,  0.2126],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0164],
        [ 0.0801],
        [ 0.0275]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0923],
        [ 0.1234],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2230,  0.3865],
        [-0.0047,  0.2126],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0969],
        [-0.3714],
        [-0.1382]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1437],
        [-0.2925],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.9775,  0.7628,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.2107,  0.0000,  0.0000],
        [-2.9775, -0.7628,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5402, -0.9734,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2230,  0.3865],
        [-0.0047,  0.2126],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1090],
        [0.4553],
        [0.1411]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1172],
        [0.4683],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2510,  0.3426],
        [-0.0223,  0.2365],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0188],
        [ 0.0906],
        [ 0.0264]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0806],
        [ 0.1359],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2510,  0.3426],
        [-0.0223,  0.2365],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1158],
        [-0.3978],
        [-0.1408]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1661],
        [-0.3100],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.9728,  0.5889,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.5971,  0.0000,  0.0000],
        [-2.9728, -0.5889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5355, -1.1861,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2510,  0.3426],
        [-0.0223,  0.2365],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1316],
        [0.4715],
        [0.1392]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1282],
        [0.4945],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2731,  0.3195],
        [-0.0344,  0.2516],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0100],
        [ 0.0999],
        [ 0.0254]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0682],
        [ 0.1463],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2731,  0.3195],
        [-0.0344,  0.2516],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1391],
        [-0.4180],
        [-0.1430]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1833],
        [-0.3284],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.9506,  0.4829,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -0.9397,  0.0000,  0.0000],
        [-2.9506, -0.4829,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.5133, -1.4226,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2731,  0.3195],
        [-0.0344,  0.2516],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1578],
        [0.4888],
        [0.1374]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1374],
        [0.5101],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2919,  0.3036],
        [-0.0438,  0.2576],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0044],
        [0.1067],
        [0.0244]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0573],
        [ 0.1555],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2919,  0.3036],
        [-0.0438,  0.2576],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1600],
        [-0.4362],
        [-0.1451]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2006],
        [-0.3499],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.9162,  0.4149,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.2592,  0.0000,  0.0000],
        [-2.9162, -0.4149,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4789, -1.6742,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2919,  0.3036],
        [-0.0438,  0.2576],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1845],
        [0.4991],
        [0.1357]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1398],
        [0.5226],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3087,  0.2903],
        [-0.0535,  0.2587],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0136],
        [0.1128],
        [0.0236]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0448],
        [ 0.1588],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3087,  0.2903],
        [-0.0535,  0.2587],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1832],
        [-0.4559],
        [-0.1468]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2186],
        [-0.3687],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.8724,  0.3689,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.5629,  0.0000,  0.0000],
        [-2.8724, -0.3689,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4351, -1.9318,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3087,  0.2903],
        [-0.0535,  0.2587],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2069],
        [0.5079],
        [0.1341]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1409],
        [0.5294],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3223,  0.2769],
        [-0.0698,  0.2620],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0260],
        [0.1214],
        [0.0228]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0322],
        [ 0.1615],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3223,  0.2769],
        [-0.0698,  0.2620],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2128],
        [-0.4752],
        [-0.1484]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2400],
        [-0.3893],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.8189,  0.3373,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -1.8532,  0.0000,  0.0000],
        [-2.8189, -0.3373,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3816, -2.1905,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3223,  0.2769],
        [-0.0698,  0.2620],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2250],
        [0.5092],
        [0.1327]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1460],
        [0.5345],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3325,  0.2792],
        [-0.0970,  0.2688],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0362],
        [0.1315],
        [0.0220]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0216],
        [ 0.1644],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3325,  0.2792],
        [-0.0970,  0.2688],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2360],
        [-0.4931],
        [-0.1497]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2681],
        [-0.4121],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.7491,  0.3224,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.1301,  0.0000,  0.0000],
        [-2.7491, -0.3224,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.3118, -2.4525,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3325,  0.2792],
        [-0.0970,  0.2688],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2408],
        [0.5054],
        [0.1313]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1486],
        [0.5361],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3430,  0.2988],
        [-0.1291,  0.2678],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Q-values: tensor([[0.0419],
        [0.1419],
        [0.0213]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0092],
        [ 0.1740],
        [-0.1293]])
States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3430,  0.2988],
        [-0.1291,  0.2678],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Q-values: tensor([[-0.2592],
        [-0.5070],
        [-0.1509]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2998],
        [-0.4330],
        [-0.8900]])
States: tensor([[ 0.0000,  0.0000,  2.6521,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.4093,  0.0000,  0.0000],
        [-2.6521, -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2148, -2.7212,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3430,  0.2988],
        [-0.1291,  0.2678],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Q-values: tensor([[0.2595],
        [0.5022],
        [0.1299]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1474],
        [0.5281],
        [0.0644]])
States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Actions: tensor([[-0.3502,  0.3182],
        [-0.1632,  0.2762],
        [ 0.1656, -0.2042]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0490],
        [0.1470],
        [0.2077]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0077],
        [ 0.1882],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Actions: tensor([[-0.3502,  0.3182],
        [-0.1632,  0.2762],
        [ 0.1656, -0.2042]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2893],
        [-0.5171],
        [-0.5352]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3413],
        [-0.4566],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.5230,  0.2810,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -2.7081,  0.0000,  0.0000],
        [-2.5230, -0.2810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.0857, -2.9891,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.2232,  4.9945]])
Actions: tensor([[-0.3502,  0.3182],
        [-0.1632,  0.2762],
        [ 0.1656, -0.2042]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2705],
        [0.5010],
        [0.4904]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1529],
        [0.5078],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3588,  0.3376],
        [-0.2079,  0.2855],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0646],
        [0.1629],
        [0.0197]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0308],
        [ 0.1970],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3588,  0.3376],
        [-0.2079,  0.2855],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3267],
        [-0.5226],
        [-0.1534]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3879],
        [-0.4844],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.3598,  0.2390,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.0263,  0.0000,  0.0000],
        [-2.3598, -0.2390,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9225, -3.2653,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3588,  0.3376],
        [-0.2079,  0.2855],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2795],
        [0.4903],
        [0.1270]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1631],
        [0.4785],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3810,  0.3574],
        [-0.2654,  0.2963],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0846],
        [0.1919],
        [0.0189]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0575],
        [ 0.1983],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3810,  0.3574],
        [-0.2654,  0.2963],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3712],
        [-0.5310],
        [-0.1543]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4348],
        [-0.5146],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  2.1519,  0.1869,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.3638,  0.0000,  0.0000],
        [-2.1519, -0.1869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.7146, -3.5508,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3810,  0.3574],
        [-0.2654,  0.2963],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2892],
        [0.4718],
        [0.1255]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1698],
        [0.4419],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4110,  0.3803],
        [-0.3258,  0.3035],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1056],
        [0.2110],
        [0.0181]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0948],
        [ 0.1925],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4110,  0.3803],
        [-0.3258,  0.3035],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4153],
        [-0.5426],
        [-0.1551]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4846],
        [-0.5410],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.8865,  0.1258,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -3.7212,  0.0000,  0.0000],
        [-1.8865, -0.1258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.4492, -3.8470,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4110,  0.3803],
        [-0.3258,  0.3035],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2974],
        [0.4512],
        [0.1240]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1755],
        [0.3940],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4454,  0.4227],
        [-0.4006,  0.3106],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1408],
        [0.2218],
        [0.0173]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.1280],
        [ 0.2016],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4454,  0.4227],
        [-0.4006,  0.3106],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4619],
        [-0.5579],
        [-0.1560]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.5263],
        [-0.5587],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.5607,  0.0490,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.1016,  0.0000,  0.0000],
        [-1.5607, -0.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1234, -4.1505,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4454,  0.4227],
        [-0.4006,  0.3106],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2942],
        [0.4102],
        [0.1224]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1803],
        [0.3296],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4878,  0.4688],
        [-0.4888,  0.3179],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1784],
        [0.2404],
        [0.0166]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0096],
        [-0.0261],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4878,  0.4688],
        [-0.4888,  0.3179],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.5056],
        [-0.5671],
        [-0.1568]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1000],
        [-0.1555],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  1.1601, -0.0631,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4373, -4.5242,  0.0000,  0.0000],
        [-1.1601,  0.0631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.2772, -4.4611,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4878,  0.4688],
        [-0.4888,  0.3179],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2979],
        [0.3588],
        [0.1208]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0927],
        [0.2123],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1611,  0.0266],
        [ 0.0999,  0.1825],
        [ 0.1539,  0.0061]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7712,  0.1559,  0.0000,  0.0000,  0.0000, -0.0266,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7712, -0.1559,  0.0000,  0.0000,  0.0000,  0.0000, -0.7712, -0.1825,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0443],
        [-0.0298],
        [ 0.0156]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1611,  0.0266],
        [ 0.0999,  0.1825],
        [ 0.1539,  0.0061]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7712,  0.1559,  0.0000,  0.0000,  0.0000, -0.0266,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7712, -0.1559,  0.0000,  0.0000,  0.0000,  0.0000, -0.7712, -0.1825,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1215],
        [-0.2230],
        [-0.1567]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6713,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1611,  0.0266],
        [ 0.0999,  0.1825],
        [ 0.1539,  0.0061]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.7712,  0.1559,  0.0000,  0.0000,  0.0000, -0.0266,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7712, -0.1559,  0.0000,  0.0000,  0.0000,  0.0000, -0.7712, -0.1825,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1657],
        [0.2231],
        [0.1190]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
Episode 5: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.3164,  2.8758,
          4.5671,  0.8437,  3.7015, -1.4350],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1674,  4.4509,
         -0.9167,  2.4188, -1.7823,  0.1400],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3617, -3.8304,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4728,  0.9075],
        [ 0.2138, -0.3270],
        [ 0.2266, -0.2794]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1050],
        [-0.2571],
        [ 0.1378]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.3988],
        [-0.2993],
        [ 0.0759]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.3164,  2.8758,
          4.5671,  0.8437,  3.7015, -1.4350],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1674,  4.4509,
         -0.9167,  2.4188, -1.7823,  0.1400],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3617, -3.8304,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4728,  0.9075],
        [ 0.2138, -0.3270],
        [ 0.2266, -0.2794]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1625],
        [-0.9404],
        [-0.7941]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0953],
        [-0.3820],
        [-0.4756]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.3164,  2.8758,
          4.5671,  0.8437,  3.7015, -1.4350],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.1674,  4.4509,
         -0.9167,  2.4188, -1.7823,  0.1400],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3617, -3.8304,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4728,  0.9075],
        [ 0.2138, -0.3270],
        [ 0.2266, -0.2794]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0430],
        [0.4576],
        [0.3120]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2104],
        [0.2448],
        [0.5202]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2816,  0.7807],
        [ 0.1526, -0.0618],
        [ 0.1172, -0.0880]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0401],
        [-0.1165],
        [ 0.0530]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0280],
        [-0.3177],
        [ 0.0080]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2816,  0.7807],
        [ 0.1526, -0.0618],
        [ 0.1172, -0.0880]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3366],
        [-0.4805],
        [-0.7630]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2455],
        [-0.4163],
        [-0.4640]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.9938,  3.6045,
          0.0000,  0.0000,  4.1744, -2.3425],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.1305,  2.7457, -1.9961,  0.4670],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.3837, -1.9149,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2816,  0.7807],
        [ 0.1526, -0.0618],
        [ 0.1172, -0.0880]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2105],
        [0.2397],
        [0.3988]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0703],
        [0.2509],
        [0.5595]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2694,  0.2150],
        [ 0.1566, -0.0694],
        [ 0.1551,  0.1377]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0339],
        [-0.1158],
        [-0.0796]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3367],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2694,  0.2150],
        [ 0.1566, -0.0694],
        [ 0.1551,  0.1377]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1868],
        [-0.5019],
        [-0.7883]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4516],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.6424,  3.9193,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.2832,  2.8075, -2.1488,  0.5288],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1339, -0.7314,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2694,  0.2150],
        [ 0.1566, -0.0694],
        [ 0.1551,  0.1377]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1768],
        [0.2578],
        [0.4275]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2598],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1614, -0.0775],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0652],
        [-0.1180],
        [ 0.0285]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3561],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1614, -0.0775],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1357],
        [-0.5245],
        [-0.1339]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4881],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.4397,  2.8768, -2.3053,  0.5981],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1614, -0.0775],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1364],
        [0.2742],
        [0.1282]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2690],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1661, -0.0863],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0677],
        [-0.1262],
        [ 0.0306]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2098],
        [-0.3741],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1661, -0.0863],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1303],
        [-0.5478],
        [-0.1287]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4427],
        [-0.5229],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.6011,  2.9543, -2.4667,  0.6756],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1661, -0.0863],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1380],
        [0.2906],
        [0.1297]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2123],
        [0.2810],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5976,  0.3638],
        [ 0.1713, -0.0971],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.2275],
        [-0.1401],
        [ 0.0323]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3931],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5976,  0.3638],
        [ 0.1713, -0.0971],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.4842],
        [-0.5722],
        [-0.1241]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5593],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4083, -4.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.7672,  3.0406, -2.6328,  0.7619],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5976,  0.3638],
        [ 0.1713, -0.0971],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1276],
        [0.3088],
        [0.1309]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2948],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1776, -0.1094],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0712],
        [-0.1601],
        [ 0.0333]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4129],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1776, -0.1094],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1209],
        [-0.5976],
        [-0.1197]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5997],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -1.9385,  3.1377, -2.8041,  0.8590],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1776, -0.1094],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1407],
        [0.3303],
        [0.1320]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3091],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1849, -0.1243],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0721],
        [-0.1851],
        [ 0.0339]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4341],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1849, -0.1243],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1167],
        [-0.6257],
        [-0.1158]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.6449],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1161,  3.2471, -2.9817,  0.9683],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1849, -0.1243],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1417],
        [0.3511],
        [0.1329]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3248],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1935, -0.1414],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0726],
        [-0.2158],
        [ 0.0341]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4560],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1935, -0.1414],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1130],
        [-0.6569],
        [-0.1122]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.6958],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.3010,  3.3713, -3.1666,  1.0926],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1935, -0.1414],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1426],
        [0.3708],
        [0.1335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3429],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2040, -0.1605],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0727],
        [-0.2534],
        [ 0.0340]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.4786],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2040, -0.1605],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1098],
        [-0.6931],
        [-0.1092]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.7517],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.4945,  3.5128, -3.3601,  1.2340],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2040, -0.1605],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1432],
        [0.3905],
        [0.1340]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3624],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2189, -0.1798],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0725],
        [-0.2980],
        [ 0.0335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.5038],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2189, -0.1798],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1069],
        [-0.7341],
        [-0.1065]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.8113],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.6984,  3.6733, -3.5640,  1.3946],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2189, -0.1798],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1437],
        [0.4100],
        [0.1344]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.3828],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2349, -0.1983],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0719],
        [-0.3511],
        [ 0.0328]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.5249],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2349, -0.1983],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1045],
        [-0.7823],
        [-0.1042]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.8652],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.9174,  3.8323, -3.7830,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2349, -0.1983],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1440],
        [0.4329],
        [0.1346]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.4000],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2267, -0.2031],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0712],
        [-0.4093],
        [ 0.0318]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2751],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2267, -0.2031],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1024],
        [-0.8328],
        [-0.1023]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5093],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -3.1522,  3.8323, -4.0178,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2267, -0.2031],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1442],
        [0.4593],
        [0.1346]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2201],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2732,  0.0534],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0705],
        [-0.0176],
        [ 0.0310]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2920],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2732,  0.0534],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1002],
        [-0.1744],
        [-0.1002]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5171],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.2445,  1.5536],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2732,  0.0534],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1441],
        [0.4732],
        [0.1344]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2373],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2695,  0.0674],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0696],
        [-0.0436],
        [ 0.0299]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.3090],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2695,  0.0674],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0986],
        [-0.1768],
        [-0.0987]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.5183],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.5177,  1.5002],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2695,  0.0674],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1438],
        [0.4759],
        [0.1338]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.2546],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2648,  0.0821],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[ 0.0683],
        [-0.0765],
        [ 0.0286]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2648,  0.0821],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[-0.0976],
        [-0.1865],
        [-0.0978]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000, -4.7873,  1.4328],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2648,  0.0821],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.1432],
        [0.4738],
        [0.1331]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.0656]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.0672],
        [0.0266],
        [0.0274]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0251]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[-0.0966],
        [-0.0894],
        [-0.0970]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1447]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.1423],
        [0.1327],
        [0.1321]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.0656]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.0661],
        [0.0254],
        [0.0262]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0251]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[-0.0959],
        [-0.0887],
        [-0.0963]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1447]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.1414],
        [0.1316],
        [0.1310]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.0656]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.0649],
        [0.0242],
        [0.0250]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0251]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[-0.0953],
        [-0.0882],
        [-0.0958]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1447]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Q-values: tensor([[0.1405],
        [0.1305],
        [0.1299]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.0656]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.0638],
        [0.0230],
        [0.0238]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2371],
        [ 0.4934],
        [-0.0251]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0948],
        [-0.0878],
        [-0.0955]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3225],
        [-0.3967],
        [-0.1447]])
States: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.1395],
        [0.1294],
        [0.1288]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2785],
        [0.1124],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2150,  0.0265],
        [-0.2958,  0.4047],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1233],
        [ 0.4441],
        [ 0.0228]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2227],
        [ 0.4230],
        [-0.0251]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2150,  0.0265],
        [-0.2958,  0.4047],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3356],
        [-0.5068],
        [-0.0955]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3023],
        [-0.3753],
        [-0.1447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.8668,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.8668,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6972,
          0.0000,  0.0000,  4.7477,  0.8564],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2150,  0.0265],
        [-0.2958,  0.4047],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.4473],
        [-0.1135],
        [ 0.1278]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2645],
        [0.1327],
        [0.0656]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2085,  0.0282],
        [-0.2489,  0.3979],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1166],
        [ 0.3839],
        [ 0.0217]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1963],
        [ 0.3376],
        [-0.0511]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2085,  0.0282],
        [-0.2489,  0.3979],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.3094],
        [-0.4997],
        [-0.0953]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1587],
        [-0.3463],
        [-0.3136]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.4885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.4885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1019,
          0.0000,  0.0000,  4.7477,  0.4517],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.2085,  0.0282],
        [-0.2489,  0.3979],
        [ 0.1539,  0.0061]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.4180],
        [-0.0888],
        [ 0.1270]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3833],
        [0.1568],
        [0.3681]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3329, -0.1427],
        [-0.2381,  0.4115],
        [ 0.1904, -0.1676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0597],
        [ 0.3039],
        [-0.1547]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1857],
        [ 0.2268],
        [-0.0354]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3329, -0.1427],
        [-0.2381,  0.4115],
        [ 0.1904, -0.1676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.1011],
        [-0.4792],
        [-0.3515]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1454],
        [-0.3216],
        [-0.2929]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -4.1188,  0.0000,  3.9619,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  4.1188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.4998,
          0.0000,  0.0000,  4.7477,  0.0538],
        [ 0.0000, -3.9619,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3329, -0.1427],
        [-0.2381,  0.4115],
        [ 0.1904, -0.1676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.7076],
        [-0.0518],
        [ 0.2977]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3524],
        [0.2143],
        [0.3708]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3189, -0.1517],
        [-0.2339,  0.4181],
        [ 0.1948, -0.1699]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0604],
        [ 0.1981],
        [-0.1485]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1680],
        [ 0.1159],
        [-0.0228]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3189, -0.1517],
        [-0.2339,  0.4181],
        [ 0.1948, -0.1699]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0750],
        [-0.4636],
        [-0.3358]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3001],
        [-0.2687]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -3.5646,  0.1904,  3.9370,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  3.5646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.9113,
          0.0000,  0.0000,  4.7477, -0.3577],
        [-0.1904, -3.9370,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3189, -0.1517],
        [-0.2339,  0.4181],
        [ 0.1948, -0.1699]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.6598],
        [0.0118],
        [0.2785]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3204],
        [0.2537],
        [0.3752]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3171, -0.1666],
        [-0.2480,  0.4170],
        [ 0.1988, -0.1724]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0663],
        [ 0.1039],
        [-0.1380]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1423],
        [ 0.0624],
        [-0.0091]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3171, -0.1666],
        [-0.2480,  0.4170],
        [ 0.1988, -0.1724]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0554],
        [-0.4487],
        [-0.3196]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1622],
        [-0.2542],
        [-0.2447]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.9949,  0.3852,  3.9188,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.9949,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.3294,
          0.0000,  0.0000,  4.7477, -0.7758],
        [-0.3852, -3.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3171, -0.1666],
        [-0.2480,  0.4170],
        [ 0.1988, -0.1724]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.6097],
        [0.0629],
        [0.2611]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2904],
        [0.2857],
        [0.3803]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3301, -0.1855],
        [-0.2715,  0.4128],
        [ 0.2013, -0.1747]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0826],
        [ 0.0549],
        [-0.1250]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0262],
        [0.2184],
        [0.0003]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3301, -0.1855],
        [-0.2715,  0.4128],
        [ 0.2013, -0.1747]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0571],
        [-0.3990],
        [-0.3018]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3074],
        [-0.4042],
        [-0.2211]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -2.4112,  0.5840,  3.9130,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  2.4112,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.7464,
          0.0000,  0.0000,  4.7477, -1.1928],
        [-0.5840, -3.9130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.3301, -0.1855],
        [-0.2715,  0.4128],
        [ 0.2013, -0.1747]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.5649],
        [0.0985],
        [0.2455]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1586],
        [0.2766],
        [0.3857]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4629, -0.0750],
        [-0.2340,  0.2398],
        [ 0.2046, -0.1767]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1613],
        [ 0.1257],
        [-0.1117]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0310],
        [0.2141],
        [0.0090]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4629, -0.0750],
        [-0.2340,  0.2398],
        [ 0.2046, -0.1767]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2687],
        [-0.3557],
        [-0.2829]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2834],
        [-0.4053],
        [-0.1886]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.8130,  0.7854,  3.9238,  0.0000, -4.9722,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.8130,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.1592,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7854, -3.9238,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4629, -0.0750],
        [-0.2340,  0.2398],
        [ 0.2046, -0.1767]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3769],
        [0.2453],
        [0.2316]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1579],
        [0.2561],
        [0.3829]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4486, -0.0425],
        [-0.2204,  0.2506],
        [ 0.2064, -0.1732]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1515],
        [ 0.1204],
        [-0.0929]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0307],
        [0.2147],
        [0.0156]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4486, -0.0425],
        [-0.2204,  0.2506],
        [ 0.2064, -0.1732]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2550],
        [-0.3404],
        [-0.2533]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2834],
        [-0.4172],
        [-0.1560]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.4982,  0.9899,  3.8221,  0.0687, -4.8972,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.4982,  0.0000,  0.0000,  0.0000,  0.0000,  0.0687, -3.3990,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9899, -3.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4486, -0.0425],
        [-0.2204,  0.2506],
        [ 0.2064, -0.1732]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3463],
        [0.2135],
        [0.2159]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1662],
        [0.2402],
        [0.3792]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4429, -0.0125],
        [-0.2036,  0.2809],
        [ 0.2076, -0.1659]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1340],
        [ 0.1072],
        [-0.0749]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0299],
        [ 0.0873],
        [-0.2222]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4429, -0.0125],
        [-0.2036,  0.2809],
        [ 0.2076, -0.1659]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2659],
        [-0.3434],
        [-0.2229]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2844],
        [-0.3303],
        [-0.1919]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -1.2051,  1.1963,  3.6914,  0.0000, -4.8547,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.2051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.6495,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1963, -3.6914,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4429, -0.0125],
        [-0.2036,  0.2809],
        [ 0.2076, -0.1659]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.3183],
        [0.1919],
        [0.2038]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1785],
        [0.2763],
        [0.4098]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4384,  0.0216],
        [-0.2826,  0.2277],
        [-0.1092,  0.1822]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1163],
        [ 0.1918],
        [-0.2721]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0247],
        [ 0.0690],
        [-0.2272]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4384,  0.0216],
        [-0.2826,  0.2277],
        [-0.1092,  0.1822]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2685],
        [-0.2545],
        [-0.3818]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2906],
        [-0.3205],
        [-0.1928]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.9117,  1.4039,  3.5381,  0.0000, -4.8422,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.9117,  0.0000,  0.0000,  1.4039,  4.4498,  0.0000, -3.9304,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4039, -3.5381, -1.4039, -4.4498,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4384,  0.0216],
        [-0.2826,  0.2277],
        [-0.1092,  0.1822]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2805],
        [0.4164],
        [0.3368]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1959],
        [0.2804],
        [0.4155]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4523,  0.0226],
        [-0.2664,  0.2514],
        [-0.1024,  0.1707]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1054],
        [ 0.1478],
        [-0.2798]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0250],
        [ 0.0504],
        [-0.2315]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4523,  0.0226],
        [-0.2664,  0.2514],
        [-0.1024,  0.1707]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2678],
        [-0.2497],
        [-0.3839]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2978],
        [-0.3169],
        [-0.1924]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.7056,  1.2947,  3.6987,  0.0000, -4.8638,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.7056,  0.0000,  0.0000,  1.2947,  4.4043,  0.0000, -4.1582,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.2947, -3.6987, -1.2947, -4.4043,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4523,  0.0226],
        [-0.2664,  0.2514],
        [-0.1024,  0.1707]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2800],
        [0.3761],
        [0.3416]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2156],
        [0.2736],
        [0.4187]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4691,  0.0294],
        [-0.2463,  0.2707],
        [-0.0949,  0.1594]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0926],
        [ 0.1061],
        [-0.2865]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0276],
        [ 0.0395],
        [-0.2333]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4691,  0.0294],
        [-0.2463,  0.2707],
        [-0.0949,  0.1594]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2681],
        [-0.2554],
        [-0.3747]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3080],
        [-0.3152],
        [-0.1922]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.4769,  1.1923,  3.8468,  0.0000, -4.8864,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.4769,  0.0000,  0.0000,  1.1923,  4.3236,  0.0000, -4.4095,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.1923, -3.8468, -1.1923, -4.3236,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4691,  0.0294],
        [-0.2463,  0.2707],
        [-0.0949,  0.1594]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2767],
        [0.3268],
        [0.3555]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2366],
        [0.2639],
        [0.4196]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4839,  0.0388],
        [-0.2229,  0.2910],
        [-0.0871,  0.1485]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0773],
        [ 0.0646],
        [-0.2894]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0309],
        [ 0.0303],
        [-0.2331]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4839,  0.0388],
        [-0.2229,  0.2910],
        [-0.0871,  0.1485]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2693],
        [-0.2604],
        [-0.3524]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3194],
        [-0.3189],
        [-0.1886]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.2356,  1.0974,  3.9768,  0.0000, -4.9158,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2356,  0.0000,  0.0000,  1.0974,  4.2124,  0.0000, -4.6802,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0974, -3.9768, -1.0974, -4.2124,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4839,  0.0388],
        [-0.2229,  0.2910],
        [-0.0871,  0.1485]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2767],
        [0.2875],
        [0.3710]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2593],
        [0.2577],
        [0.4222]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4998,  0.0485],
        [-0.1988,  0.3155],
        [-0.0757,  0.1361]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0606],
        [ 0.0211],
        [-0.2898]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0054],
        [-0.0242],
        [-0.2344]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4998,  0.0485],
        [-0.1988,  0.3155],
        [-0.0757,  0.1361]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2738],
        [-0.2702],
        [-0.3193]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2607],
        [-0.2344],
        [-0.1851]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0166,  1.0103,  4.0865,  0.0000, -4.9546,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0166,  0.0000,  0.0000,  1.0103,  4.0699,  0.0000, -4.9712,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-1.0103, -4.0865, -1.0103, -4.0699,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4998,  0.0485],
        [-0.1988,  0.3155],
        [-0.0757,  0.1361]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2746],
        [0.2519],
        [0.3851]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2950],
        [0.2719],
        [0.4263]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4090, -0.2144],
        [-0.1219, -0.0332],
        [-0.0629,  0.1259]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1849],
        [ 0.1369],
        [-0.2869]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0258],
        [-0.0332],
        [-0.2457]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4090, -0.2144],
        [-0.1219, -0.0332],
        [-0.0629,  0.1259]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0974],
        [-0.0975],
        [-0.2783]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3172],
        [-0.2321],
        [-0.1938]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2836,  0.9346,  4.1741,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2836,  0.0000,  0.0000,  0.9346,  3.8905,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.9346, -4.1741, -0.9346, -3.8905,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.4090, -0.2144],
        [-0.1219, -0.0332],
        [-0.0629,  0.1259]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.4764],
        [0.4395],
        [0.3996]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2936],
        [0.2958],
        [0.4570]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5220,  0.0423],
        [-0.1291, -0.0307],
        [-0.0606,  0.1218]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0383],
        [ 0.1346],
        [-0.2996]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0242],
        [-0.0287],
        [-0.2500]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5220,  0.0423],
        [-0.1291, -0.0307],
        [-0.0606,  0.1218]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2418],
        [-0.0916],
        [-0.2637]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3212],
        [-0.2434],
        [-0.2034]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.4648,  0.8717,  4.5143,  0.2082, -4.7887,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.4648,  0.0000,  0.0000,  0.8717,  4.0495,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8717, -4.5143, -0.8717, -4.0495,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5220,  0.0423],
        [-0.1291, -0.0307],
        [-0.0606,  0.1218]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2781],
        [0.4584],
        [0.4370]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2953],
        [0.3057],
        [0.4606]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5250,  0.0322],
        [-0.1332, -0.0537],
        [-0.0590,  0.1159]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0189],
        [ 0.1314],
        [-0.2982]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0235],
        [-0.0235],
        [-0.2543]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5250,  0.0322],
        [-0.1332, -0.0537],
        [-0.0590,  0.1159]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2590],
        [-0.0974],
        [-0.2494]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3230],
        [-0.2553],
        [-0.2137]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3918,  0.8110,  4.5939,  0.1316, -4.8309,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3918,  0.0000,  0.0000,  0.8110,  4.2021,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.8110, -4.5939, -0.8110, -4.2021,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5250,  0.0322],
        [-0.1332, -0.0537],
        [-0.0590,  0.1159]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2676],
        [0.4682],
        [0.4566]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2913],
        [0.3172],
        [0.4675]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5288,  0.0221],
        [-0.1386, -0.0773],
        [-0.0579,  0.1111]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[ 0.0012],
        [ 0.1273],
        [-0.2954]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0216],
        [-0.0172],
        [-0.2569]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5288,  0.0221],
        [-0.1386, -0.0773],
        [-0.0579,  0.1111]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2708],
        [-0.1051],
        [-0.2357]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3223],
        [-0.2672],
        [-0.2251]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3059,  0.7520,  4.6775,  0.1141, -4.8632,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3059,  0.0000,  0.0000,  0.7520,  4.3716,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.7520, -4.6775, -0.7520, -4.3716,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5288,  0.0221],
        [-0.1386, -0.0773],
        [-0.0579,  0.1111]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2537],
        [0.4767],
        [0.4767]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2807],
        [0.3320],
        [0.4751]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5316,  0.0117],
        [-0.1447, -0.1028],
        [-0.0580,  0.1070]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0143],
        [ 0.1224],
        [-0.2918]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0163],
        [ 0.0139],
        [-0.2602]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5316,  0.0117],
        [-0.1447, -0.1028],
        [-0.0580,  0.1070]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2735],
        [-0.1141],
        [-0.2248]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3265],
        [-0.3213],
        [-0.2429]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.2065,  0.6940,  4.7665,  0.1769, -4.8853,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.2065,  0.0000,  0.0000,  0.6940,  4.5600,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6940, -4.7665, -0.6940, -4.5600,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5316,  0.0117],
        [-0.1447, -0.1028],
        [-0.0580,  0.1070]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2358],
        [0.4844],
        [0.4965]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2857],
        [0.2761],
        [0.4830]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5284, -0.0048],
        [-0.2175,  0.2907],
        [-0.0587,  0.1031]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.0297],
        [-0.0748],
        [-0.2875]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.2042],
        [-0.0282],
        [-0.2139]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5284, -0.0048],
        [-0.2175,  0.2907],
        [-0.0587,  0.1031]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[-0.2992],
        [-0.3039],
        [-0.2189]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.4744],
        [-0.2630],
        [-0.3230]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0920,  0.6360,  4.8618,  0.0655, -4.8970,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.0920,  0.0000,  0.0000,  0.6360,  4.7698,  0.0655, -4.9890,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.6360, -4.8618, -0.6360, -4.7698,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5284, -0.0048],
        [-0.2175,  0.2907],
        [-0.0587,  0.1031]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Q-values: tensor([[0.2353],
        [0.2004],
        [0.5170]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.2743],
        [0.3515],
        [0.2448]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5763,  0.3830],
        [-0.1503, -0.0930],
        [ 0.0206,  0.1727]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Q-values: tensor([[ 0.0152],
        [ 0.1089],
        [-0.3189]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0258],
        [-0.0152],
        [-0.0626]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5763,  0.3830],
        [-0.1503, -0.0930],
        [ 0.0206,  0.1727]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Q-values: tensor([[-0.3834],
        [-0.1225],
        [-0.2023]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1417],
        [-0.1422],
        [-0.3761]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.3875,  0.0000,  0.0000,  0.0000, -4.8922,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3875,  0.0000,  0.0000,  0.5773,  4.5823,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.5773, -4.5823,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000]])
Actions: tensor([[-0.5763,  0.3830],
        [-0.1503, -0.0930],
        [ 0.0206,  0.1727]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Q-values: tensor([[0.1063],
        [0.4824],
        [0.2891]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0655],
        [0.0600],
        [0.3522]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Actions: tensor([[-0.1594,  0.0162],
        [ 0.0591,  0.1239],
        [ 0.2614, -0.2052]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0591,  0.0191,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0591, -0.0191,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  0.0000,
          0.0000,  0.0000,  3.8884,  1.5536]])
Q-values: tensor([[0.0433],
        [0.0089],
        [0.0193]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Actions: tensor([[-0.1594,  0.0162],
        [ 0.0591,  0.1239],
        [ 0.2614, -0.2052]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0591,  0.0191,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0591, -0.0191,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  0.0000,
          0.0000,  0.0000,  3.8884,  1.5536]])
Q-values: tensor([[-0.0864],
        [-0.0791],
        [-0.2335]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
States: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0885,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5979,  0.0000,
          0.0000,  0.0000,  4.1498,  1.5536]])
Actions: tensor([[-0.1594,  0.0162],
        [ 0.0591,  0.1239],
        [ 0.2614, -0.2052]])
Rewards: tensor([[ 0.],
        [ 0.],
        [10.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0591,  0.0191,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0591, -0.0191,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8593,  0.0000,
          0.0000,  0.0000,  3.8884,  1.5536]])
Q-values: tensor([[0.1362],
        [0.1215],
        [0.4052]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [ 0.],
        [10.]])
Episode 6: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.6936, -0.7653,  2.6978,  2.0446,
          0.0000,  0.0000, -2.8344, -2.4648],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.1724, -4.0353,  0.0000,  0.0000,
         -0.0568, -0.4120,  0.0000,  0.0000],
        [ 3.6936,  0.7653, -2.1724,  4.0353,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2293,  3.6233,  0.8591, -1.6995]])
Actions: tensor([[-0.6597,  0.0345],
        [-0.1197,  0.6260],
        [-0.0202, -0.3300]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Q-values: tensor([[-0.4146],
        [ 0.1117],
        [ 0.0026]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2542],
        [ 0.1007],
        [-0.0708]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.6936, -0.7653,  2.6978,  2.0446,
          0.0000,  0.0000, -2.8344, -2.4648],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.1724, -4.0353,  0.0000,  0.0000,
         -0.0568, -0.4120,  0.0000,  0.0000],
        [ 3.6936,  0.7653, -2.1724,  4.0353,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2293,  3.6233,  0.8591, -1.6995]])
Actions: tensor([[-0.6597,  0.0345],
        [-0.1197,  0.6260],
        [-0.0202, -0.3300]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Q-values: tensor([[-0.3459],
        [-0.1739],
        [-0.7014]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2661],
        [-0.3845],
        [-0.6834]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.6936, -0.7653,  2.6978,  2.0446,
          0.0000,  0.0000, -2.8344, -2.4648],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  2.1724, -4.0353,  0.0000,  0.0000,
         -0.0568, -0.4120,  0.0000,  0.0000],
        [ 3.6936,  0.7653, -2.1724,  4.0353,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.2293,  3.6233,  0.8591, -1.6995]])
Actions: tensor([[-0.6597,  0.0345],
        [-0.1197,  0.6260],
        [-0.0202, -0.3300]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Q-values: tensor([[-0.0757],
        [ 0.0503],
        [ 0.0072]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0339],
        [0.2202],
        [0.6334]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Actions: tensor([[-0.6826, -0.0671],
        [-0.0709,  0.3754],
        [ 0.0358,  0.3775]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Q-values: tensor([[-0.2593],
        [ 0.0760],
        [-0.4236]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1950],
        [ 0.1050],
        [-0.0836]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Actions: tensor([[-0.6826, -0.0671],
        [-0.0709,  0.3754],
        [ 0.0358,  0.3775]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Q-values: tensor([[-0.2759],
        [-0.2769],
        [-0.7388]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1898],
        [-0.4066],
        [-0.6506]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -3.0541, -1.1298,  0.0000,  0.0000,
          0.0000,  0.0000, -2.1747, -2.4994],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8360, -1.6797,
          0.0629, -1.0380,  0.0000,  0.0000],
        [ 3.0541,  1.1298,  0.0000,  0.0000,  0.0000,  0.0000, -3.1079,  3.3116,
         -2.2090,  3.9533,  0.8794, -1.3696]])
Actions: tensor([[-0.6826, -0.0671],
        [-0.0709,  0.3754],
        [ 0.0358,  0.3775]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Q-values: tensor([[0.0244],
        [0.2449],
        [0.4980]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0177],
        [0.2089],
        [0.6618]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Actions: tensor([[-0.5579, -0.0539],
        [-0.1332,  0.3665],
        [-0.0113,  0.3936]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Q-values: tensor([[-0.2029],
        [ 0.1031],
        [-0.4280]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1675],
        [ 0.0999],
        [-0.0771]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Actions: tensor([[-0.5579, -0.0539],
        [-0.1332,  0.3665],
        [-0.0113,  0.3936]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Q-values: tensor([[-0.2163],
        [-0.2684],
        [-0.7247]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.7004],
        [-0.3985],
        [-0.6060]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -2.3357, -0.6853,  0.0000,  0.0000,
          0.0000,  0.0000, -1.4921, -2.4323],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7651, -1.5798,
          0.1338, -1.4134,  0.0000,  0.0000],
        [ 2.3357,  0.6853,  0.0000,  0.0000,  0.0000,  0.0000, -3.1437,  3.4095,
         -2.2448,  3.5758,  0.8436, -1.7470]])
Actions: tensor([[-0.5579, -0.0539],
        [-0.1332,  0.3665],
        [-0.0113,  0.3936]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Q-values: tensor([[0.0423],
        [0.2417],
        [0.5509]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4030],
        [0.1819],
        [0.6528]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Actions: tensor([[-0.2658, -0.0773],
        [-0.1878,  0.3522],
        [-0.0489,  0.4403]])
Rewards: tensor([[ 0.],
        [10.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.5721,  0.2797,  0.0000,  0.0000,
         -3.7567,  3.0217, -0.6683, -2.3011],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4441, -0.7759,
          0.4547, -2.1322,  0.0000,  0.0000],
        [ 1.5721, -0.2797,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1846,  2.7420,  0.9038, -2.5808]])
Q-values: tensor([[-0.4915],
        [ 0.1258],
        [-0.3817]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [10.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Actions: tensor([[-0.2658, -0.0773],
        [-0.1878,  0.3522],
        [-0.0489,  0.4403]])
Rewards: tensor([[ 0.],
        [10.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.5721,  0.2797,  0.0000,  0.0000,
         -3.7567,  3.0217, -0.6683, -2.3011],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4441, -0.7759,
          0.4547, -2.1322,  0.0000,  0.0000],
        [ 1.5721, -0.2797,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1846,  2.7420,  0.9038, -2.5808]])
Q-values: tensor([[-0.8881],
        [-0.2359],
        [-0.6893]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [10.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.7890, -0.2378,  0.0000,  0.0000,
         -4.0225,  2.9444, -0.9342, -2.3784],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.6319, -1.3347,
          0.2670, -1.7800,  0.0000,  0.0000],
        [ 1.7890,  0.2378,  0.0000,  0.0000,  0.0000,  0.0000, -3.1324,  3.6276,
         -2.2335,  3.1823,  0.8549, -2.1406]])
Actions: tensor([[-0.2658, -0.0773],
        [-0.1878,  0.3522],
        [-0.0489,  0.4403]])
Rewards: tensor([[ 0.],
        [10.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -1.5721,  0.2797,  0.0000,  0.0000,
         -3.7567,  3.0217, -0.6683, -2.3011],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4441, -0.7759,
          0.4547, -2.1322,  0.0000,  0.0000],
        [ 1.5721, -0.2797,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -2.1846,  2.7420,  0.9038, -2.5808]])
Q-values: tensor([[0.2104],
        [0.2140],
        [0.5706]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.],
        [10.],
        [ 0.]])
Episode 7: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.6274,  3.2657, -0.2996,  0.7010,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5583, -1.4321, -2.4853, -3.9968,
         -0.1909, -0.0501,  0.0000,  0.0000],
        [-0.6274, -3.2657,  1.5583,  1.4321,  0.0000,  0.0000, -0.9270, -2.5647,
          1.3674,  1.3819,  0.0000,  0.0000]])
Actions: tensor([[-0.3793, -0.1822],
        [-0.0062,  0.6634],
        [ 0.4739, -0.4201]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.4805,  3.0277, -0.8024, -0.4173,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0781, -2.5156,  0.0000,  0.0000,
         -0.1846, -0.7136,  0.0000,  0.0000],
        [-1.4805, -3.0277,  1.0781,  2.5156,  0.0000,  0.0000, -2.2830, -3.4450,
          0.8935,  1.8020,  0.0000,  0.0000]])
Q-values: tensor([[ 0.1231],
        [ 0.2336],
        [-0.0620]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.6274,  3.2657, -0.2996,  0.7010,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5583, -1.4321, -2.4853, -3.9968,
         -0.1909, -0.0501,  0.0000,  0.0000],
        [-0.6274, -3.2657,  1.5583,  1.4321,  0.0000,  0.0000, -0.9270, -2.5647,
          1.3674,  1.3819,  0.0000,  0.0000]])
Actions: tensor([[-0.3793, -0.1822],
        [-0.0062,  0.6634],
        [ 0.4739, -0.4201]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.4805,  3.0277, -0.8024, -0.4173,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0781, -2.5156,  0.0000,  0.0000,
         -0.1846, -0.7136,  0.0000,  0.0000],
        [-1.4805, -3.0277,  1.0781,  2.5156,  0.0000,  0.0000, -2.2830, -3.4450,
          0.8935,  1.8020,  0.0000,  0.0000]])
Q-values: tensor([[-0.0798],
        [-0.4315],
        [-0.1157]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.6274,  3.2657, -0.2996,  0.7010,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.5583, -1.4321, -2.4853, -3.9968,
         -0.1909, -0.0501,  0.0000,  0.0000],
        [-0.6274, -3.2657,  1.5583,  1.4321,  0.0000,  0.0000, -0.9270, -2.5647,
          1.3674,  1.3819,  0.0000,  0.0000]])
Actions: tensor([[-0.3793, -0.1822],
        [-0.0062,  0.6634],
        [ 0.4739, -0.4201]])
Rewards: tensor([[10.],
        [ 0.],
        [ 0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.4805,  3.0277, -0.8024, -0.4173,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0781, -2.5156,  0.0000,  0.0000,
         -0.1846, -0.7136,  0.0000,  0.0000],
        [-1.4805, -3.0277,  1.0781,  2.5156,  0.0000,  0.0000, -2.2830, -3.4450,
          0.8935,  1.8020,  0.0000,  0.0000]])
Q-values: tensor([[0.4011],
        [0.4542],
        [0.3152]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[10.],
        [ 0.],
        [ 0.]])
Episode 8: Total Reward: 10.0
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0434, -3.6262,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4498, -2.5555],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0217,  4.6873,
         -0.4578, -0.0244, -2.1596, -1.0629],
        [-0.0434,  3.6262,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4065,  1.0708]])
Actions: tensor([[-0.1022,  0.7614],
        [-0.1229, -0.0358],
        [ 0.2613, -0.0862]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Q-values: tensor([[ 0.0727],
        [-0.1528],
        [ 0.5337]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0393],
        [-0.1861],
        [ 0.4870]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0434, -3.6262,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4498, -2.5555],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0217,  4.6873,
         -0.4578, -0.0244, -2.1596, -1.0629],
        [-0.0434,  3.6262,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4065,  1.0708]])
Actions: tensor([[-0.1022,  0.7614],
        [-0.1229, -0.0358],
        [ 0.2613, -0.0862]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Q-values: tensor([[ 0.0506],
        [-0.1911],
        [ 0.0723]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2909],
        [-0.1589],
        [-0.3579]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0434, -3.6262,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4498, -2.5555],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0217,  4.6873,
         -0.4578, -0.0244, -2.1596, -1.0629],
        [-0.0434,  3.6262,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.4065,  1.0708]])
Actions: tensor([[-0.1022,  0.7614],
        [-0.1229, -0.0358],
        [ 0.2613, -0.0862]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Q-values: tensor([[0.6042],
        [0.0303],
        [0.5200]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.4780],
        [0.1563],
        [0.1395]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Actions: tensor([[-0.0391,  0.8031],
        [-0.0118,  0.2642],
        [ 0.3061, -0.0244]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Q-values: tensor([[ 0.0900],
        [-0.1502],
        [ 0.5771]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0013],
        [-0.1576],
        [-0.0278]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Actions: tensor([[-0.0391,  0.8031],
        [-0.0118,  0.2642],
        [ 0.3061, -0.0244]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Q-values: tensor([[ 0.0985],
        [-0.0961],
        [ 0.0533]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.3754],
        [-0.1285],
        [-0.2881]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.4069, -4.4738,  0.0000,  0.0000,
          0.0000,  0.0000,  3.5520, -3.3168],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3349,  0.0114, -2.0367, -1.0271],
        [-0.4069,  4.4738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  3.1452,  1.1569]])
Actions: tensor([[-0.0391,  0.8031],
        [-0.0118,  0.2642],
        [ 0.3061, -0.0244]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Q-values: tensor([[0.6543],
        [0.1514],
        [0.5668]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.3616],
        [0.1321],
        [0.2438]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Actions: tensor([[-0.5483, -0.1304],
        [-0.0747,  0.3155],
        [ 0.2041, -0.2002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Q-values: tensor([[ 0.3461],
        [-0.1025],
        [ 0.3751]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1120],
        [-0.0974]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Actions: tensor([[-0.5483, -0.1304],
        [-0.0747,  0.3155],
        [ 0.2041, -0.2002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Q-values: tensor([[ 0.0271],
        [-0.0545],
        [ 0.1732]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.0906],
        [-0.3201]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.0050,  3.9424,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3231, -0.2528, -2.0249, -1.2913],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  2.8390,  1.1813]])
Actions: tensor([[-0.5483, -0.1304],
        [-0.0747,  0.3155],
        [ 0.2041, -0.2002]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Q-values: tensor([[0.4308],
        [0.1657],
        [0.6002]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1055],
        [0.5896]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1559,  0.3475],
        [ 0.0991,  0.0677]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Q-values: tensor([[ 0.1183],
        [-0.0349],
        [ 0.5563]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0692],
        [-0.0949]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1559,  0.3475],
        [ 0.0991,  0.0677]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Q-values: tensor([[-0.0117],
        [-0.0014],
        [ 0.2954]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.0402],
        [-0.3248]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.2484, -0.5683, -1.9502, -1.6068],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -3.9612, -0.1615,
          4.3367,  2.4200,  2.6349,  1.3815]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.1559,  0.3475],
        [ 0.0991,  0.0677]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Q-values: tensor([[0.1931],
        [0.1690],
        [0.8613]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0549],
        [0.6087]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2343,  0.3732],
        [ 0.0928,  0.0784]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Q-values: tensor([[0.1255],
        [0.0150],
        [0.5649]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0647],
        [-0.0946]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2343,  0.3732],
        [ 0.0928,  0.0784]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Q-values: tensor([[-0.0040],
        [ 0.0653],
        [ 0.3004]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [ 0.0019],
        [-0.3253]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.0925, -0.9158, -1.7943, -1.9543],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.0603, -0.2292,
          4.2376,  2.3524,  2.5358,  1.3139]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.2343,  0.3732],
        [ 0.0928,  0.0784]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Q-values: tensor([[0.1991],
        [0.1398],
        [0.8918]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0123],
        [0.6287]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3053,  0.3928],
        [ 0.0874,  0.0875]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Q-values: tensor([[0.1314],
        [0.0244],
        [0.5481]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0771],
        [-0.0961]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3053,  0.3928],
        [ 0.0874,  0.0875]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Q-values: tensor([[0.0021],
        [0.1226],
        [0.2835]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [ 0.0407],
        [-0.3216]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1419, -1.2890, -1.5600, -2.3275],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.1531, -0.3076,
          4.1448,  2.2740,  2.4429,  1.2355]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3053,  0.3928],
        [ 0.0874,  0.0875]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Q-values: tensor([[0.2042],
        [0.1243],
        [0.9101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0656],
        [-0.0099],
        [ 0.6442]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3487,  0.3917],
        [ 0.0835,  0.0944]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Q-values: tensor([[0.1361],
        [0.0441],
        [0.5108]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0675],
        [-0.0986]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3487,  0.3917],
        [ 0.0835,  0.0944]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Q-values: tensor([[0.0070],
        [0.1863],
        [0.2507]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [ 0.0680],
        [-0.3157]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4471, -1.6818, -1.2547, -2.7203],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.2406, -0.3951,
          4.0573,  2.1864,  2.3555,  1.1479]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3487,  0.3917],
        [ 0.0835,  0.0944]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Q-values: tensor([[0.2084],
        [0.1467],
        [0.9116]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0656],
        [-0.0170],
        [ 0.6574]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3903,  0.3878],
        [ 0.0796,  0.1014]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Q-values: tensor([[0.1396],
        [0.0837],
        [0.4569]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0523],
        [-0.1052]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3903,  0.3878],
        [ 0.0796,  0.1014]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Q-values: tensor([[0.0107],
        [0.2486],
        [0.2088]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [ 0.0846],
        [-0.3075]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.7958, -2.0736, -0.9060, -3.1121],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.3241, -0.4895,
          3.9738,  2.0920,  2.2720,  1.0535]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.3903,  0.3878],
        [ 0.0796,  0.1014]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Q-values: tensor([[0.2119],
        [0.1723],
        [0.9028]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[ 0.0656],
        [-0.0151],
        [ 0.6704]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4253,  0.3868],
        [ 0.0779,  0.1074]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Q-values: tensor([[0.1423],
        [0.1345],
        [0.3910]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0365],
        [-0.1120]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4253,  0.3868],
        [ 0.0779,  0.1074]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Q-values: tensor([[0.0134],
        [0.2992],
        [0.1572]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [ 0.0947],
        [-0.2968]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1861, -2.4614, -0.5157, -3.4999],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4036, -0.5910,
          3.8943,  1.9906,  2.1924,  0.9521]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4253,  0.3868],
        [ 0.0779,  0.1074]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Q-values: tensor([[0.2147],
        [0.2057],
        [0.8883]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[6.5611e-02],
        [4.4605e-04],
        [6.8456e-01]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4539,  0.3843],
        [ 0.0769,  0.1092]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Q-values: tensor([[0.1441],
        [0.1830],
        [0.3173]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0497],
        [-0.1210],
        [-0.1166]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4539,  0.3843],
        [ 0.0769,  0.1092]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Q-values: tensor([[0.0152],
        [0.3501],
        [0.0993]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2534],
        [-0.0137],
        [-0.2859]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6114, -2.8481, -0.0904, -3.8866],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.4816, -0.6983,
          3.8163,  1.8832,  2.1145,  0.8447]])
Actions: tensor([[-0.1554,  0.0193],
        [-0.4539,  0.3843],
        [ 0.0769,  0.1092]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Q-values: tensor([[0.2169],
        [0.2460],
        [0.8722]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1811],
        [0.4817],
        [0.7003]])
States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Actions: tensor([[-0.2507,  0.3100],
        [-0.3682,  0.4804],
        [ 0.0757,  0.1063]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Q-values: tensor([[0.1767],
        [0.2290],
        [0.2402]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0489],
        [-0.1266],
        [-0.1209]])
States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Actions: tensor([[-0.2507,  0.3100],
        [-0.3682,  0.4804],
        [ 0.0757,  0.1063]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Q-values: tensor([[0.2529],
        [0.4578],
        [0.0375]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2485],
        [ 0.0159],
        [-0.2755]])
States: tensor([[ 0.0000,  0.0000,  4.8638,  0.1465,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.8638, -0.1465,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.0653, -3.2324,  0.3635, -4.2709],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.5584, -0.8075,
          3.7395,  1.7740,  2.0376,  0.7355]])
Actions: tensor([[-0.2507,  0.3100],
        [-0.3682,  0.4804],
        [ 0.0757,  0.1063]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Q-values: tensor([[0.2343],
        [0.5193],
        [0.8536]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1769],
        [0.4638],
        [0.7156]])
States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Actions: tensor([[-0.2467,  0.3278],
        [-0.3827,  0.4700],
        [ 0.0739,  0.1037]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Q-values: tensor([[0.1686],
        [0.2095],
        [0.1627]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0454],
        [ 0.0581],
        [-0.1268]])
States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Actions: tensor([[-0.2467,  0.3278],
        [-0.3827,  0.4700],
        [ 0.0739,  0.1037]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Q-values: tensor([[ 0.2356],
        [ 0.4821],
        [-0.0259]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2430],
        [-0.2579],
        [-0.2625]])
States: tensor([[ 0.0000,  0.0000,  4.7463,  0.3169,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7463, -0.3169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          2.4335, -3.7128,  0.7317, -4.7513],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.6341, -0.9138,
          3.6638,  1.6678,  1.9619,  0.6293]])
Actions: tensor([[-0.2467,  0.3278],
        [-0.3827,  0.4700],
        [ 0.0739,  0.1037]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Q-values: tensor([[0.2315],
        [0.5248],
        [0.8340]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1703],
        [0.5310],
        [0.7303]])
States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Actions: tensor([[-0.2427,  0.3407],
        [-0.0840,  0.3139],
        [ 0.0720,  0.1019]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Q-values: tensor([[0.1569],
        [0.3062],
        [0.0838]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0472],
        [ 0.0608],
        [-0.1356]])
States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Actions: tensor([[-0.2427,  0.3407],
        [-0.0840,  0.3139],
        [ 0.0720,  0.1019]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Q-values: tensor([[ 0.2042],
        [ 0.0641],
        [-0.0898]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2510],
        [-0.2656],
        [-0.2552]])
States: tensor([[ 0.0000,  0.0000,  4.6102,  0.4591,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.6102, -0.4591,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7080, -1.0175,
          3.5899,  1.5641,  1.8881,  0.5256]])
Actions: tensor([[-0.2427,  0.3407],
        [-0.0840,  0.3139],
        [ 0.0720,  0.1019]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Q-values: tensor([[0.2273],
        [0.5204],
        [0.8139]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1747],
        [0.5484],
        [0.7442]])
States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Actions: tensor([[-0.2474,  0.3463],
        [-0.0951,  0.3242],
        [ 0.0700,  0.1008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Q-values: tensor([[0.1412],
        [0.2851],
        [0.0072]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0482],
        [ 0.0633],
        [-0.1445]])
States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Actions: tensor([[-0.2474,  0.3463],
        [-0.0951,  0.3242],
        [ 0.0700,  0.1008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Q-values: tensor([[ 0.1737],
        [ 0.0355],
        [-0.1538]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.2591],
        [-0.2728],
        [-0.0771]])
States: tensor([[ 0.0000,  0.0000,  4.7689,  0.4323,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.7689, -0.4323,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -4.7800, -1.1194,
          3.5179,  1.4622,  1.8161,  0.4237]])
Actions: tensor([[-0.2474,  0.3463],
        [-0.0951,  0.3242],
        [ 0.0700,  0.1008]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Q-values: tensor([[0.2232],
        [0.5354],
        [0.7945]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.1788],
        [0.5645],
        [0.1573]])
States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Actions: tensor([[-0.2520,  0.3521],
        [-0.1057,  0.3341],
        [ 0.0932, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Q-values: tensor([[0.1203],
        [0.2562],
        [0.0133]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1374]])
States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Actions: tensor([[-0.2520,  0.3521],
        [-0.1057,  0.3341],
        [ 0.0932, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Q-values: tensor([[0.1349],
        [0.0015],
        [0.0695]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0732]])
States: tensor([[ 0.0000,  0.0000,  4.9212,  0.4102,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [-4.9212, -0.4102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          3.4479,  1.3614,  1.7461,  0.3229]])
Actions: tensor([[-0.2520,  0.3521],
        [-0.1057,  0.3341],
        [ 0.0932, -0.0321]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Q-values: tensor([[0.2161],
        [0.5476],
        [0.3598]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1552]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0939, -0.0349]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Q-values: tensor([[ 0.1416],
        [ 0.0958],
        [-0.0194]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1294]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0939, -0.0349]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Q-values: tensor([[0.0066],
        [0.0092],
        [0.0371]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0693]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3547,
         1.3935, 1.6529, 0.3550]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0939, -0.0349]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Q-values: tensor([[0.2239],
        [0.2155],
        [0.3269]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1526]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0946, -0.0379]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Q-values: tensor([[ 0.1402],
        [ 0.0942],
        [-0.0466]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1216]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0946, -0.0379]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Q-values: tensor([[0.0039],
        [0.0062],
        [0.0066]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0648]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2609,
         1.4284, 1.5591, 0.3899]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0946, -0.0379]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Q-values: tensor([[0.2237],
        [0.2150],
        [0.2943]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1487]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0954, -0.0412]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Q-values: tensor([[ 0.1387],
        [ 0.0927],
        [-0.0714]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1162]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0954, -0.0412]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Q-values: tensor([[ 0.0012],
        [ 0.0033],
        [-0.0212]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0608]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1663,
         1.4663, 1.4645, 0.4278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0954, -0.0412]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Q-values: tensor([[0.2233],
        [0.2143],
        [0.2606]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1443]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0958, -0.0439]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Q-values: tensor([[ 0.1371],
        [ 0.0911],
        [-0.0951]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1111]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0958, -0.0439]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Q-values: tensor([[-0.0013],
        [ 0.0005],
        [-0.0459]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0578]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0710,
         1.5075, 1.3691, 0.4690]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0958, -0.0439]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Q-values: tensor([[0.2226],
        [0.2134],
        [0.2294]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1397]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0956, -0.0478]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Q-values: tensor([[ 0.1356],
        [ 0.0895],
        [-0.1153]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1061]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0956, -0.0478]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Q-values: tensor([[-0.0038],
        [-0.0022],
        [-0.0676]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0552]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9751,
         1.5513, 1.2733, 0.5128]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0956, -0.0478]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Q-values: tensor([[0.2219],
        [0.2124],
        [0.2014]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1364]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0931, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Q-values: tensor([[ 0.1341],
        [ 0.0880],
        [-0.1320]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0994]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0931, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Q-values: tensor([[-0.0061],
        [-0.0047],
        [-0.0849]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0523]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8795,
         1.5991, 1.1777, 0.5606]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0931, -0.0524]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Q-values: tensor([[0.2210],
        [0.2113],
        [0.1759]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1367]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0923, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Q-values: tensor([[ 0.1327],
        [ 0.0865],
        [-0.1466]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0922]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0923, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Q-values: tensor([[-0.0084],
        [-0.0071],
        [-0.0972]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0503]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7863,
         1.6515, 1.0845, 0.6130]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0923, -0.0550]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Q-values: tensor([[0.2201],
        [0.2102],
        [0.1532]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1408]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0933, -0.0599]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Q-values: tensor([[ 0.1312],
        [ 0.0850],
        [-0.1579]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0843]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0933, -0.0599]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Q-values: tensor([[-0.0105],
        [-0.0094],
        [-0.1069]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0460]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6941,
         1.7065, 0.9923, 0.6680]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0933, -0.0599]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Q-values: tensor([[0.2191],
        [0.2090],
        [0.1355]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1445]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0957, -0.0687]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Q-values: tensor([[ 0.1299],
        [ 0.0836],
        [-0.1643]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0749]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0957, -0.0687]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Q-values: tensor([[-0.0125],
        [-0.0116],
        [-0.1142]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0406]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6008,
         1.7663, 0.8990, 0.7278]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0957, -0.0687]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Q-values: tensor([[0.2181],
        [0.2077],
        [0.1212]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1463]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0973, -0.0789]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Q-values: tensor([[ 0.1285],
        [ 0.0822],
        [-0.1681]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0667]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0973, -0.0789]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Q-values: tensor([[-0.0145],
        [-0.0136],
        [-0.1222]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0356]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5051,
         1.8351, 0.8033, 0.7966]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0973, -0.0789]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Q-values: tensor([[0.2170],
        [0.2065],
        [0.1095]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1480]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0965, -0.0883]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Q-values: tensor([[ 0.1272],
        [ 0.0809],
        [-0.1694]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0677]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0965, -0.0883]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Q-values: tensor([[-0.0163],
        [-0.0156],
        [-0.1303]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0393]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4079,
         1.9140, 0.7060, 0.8755]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0965, -0.0883]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Q-values: tensor([[0.2159],
        [0.2052],
        [0.1010]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1480]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0983, -0.0925]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Q-values: tensor([[ 0.1260],
        [ 0.0796],
        [-0.1761]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0690]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0983, -0.0925]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Q-values: tensor([[-0.0180],
        [-0.0174],
        [-0.1414]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0487]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3113,
         2.0023, 0.6095, 0.9638]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.0983, -0.0925]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Q-values: tensor([[0.2149],
        [0.2040],
        [0.0891]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1503]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1028, -0.0970]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Q-values: tensor([[ 0.1248],
        [ 0.0784],
        [-0.1818]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0723]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1028, -0.0970]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Q-values: tensor([[-0.0196],
        [-0.0191],
        [-0.1519]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0646]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2130,
         2.0948, 0.5112, 1.0563]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1028, -0.0970]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Q-values: tensor([[0.2138],
        [0.2028],
        [0.0779]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1506]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1096, -0.0996]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Q-values: tensor([[ 0.1236],
        [ 0.0772],
        [-0.1871]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0796]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1096, -0.0996]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Q-values: tensor([[-0.0211],
        [-0.0207],
        [-0.1626]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.0849]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1102,
         2.1918, 0.4084, 1.1532]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1096, -0.0996]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Q-values: tensor([[0.2127],
        [0.2015],
        [0.0686]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1516]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1183, -0.1006]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Q-values: tensor([[ 0.1225],
        [ 0.0761],
        [-0.1977]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.0923]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1183, -0.1006]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Q-values: tensor([[-0.0226],
        [-0.0223],
        [-0.1742]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1086]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0007,
         2.2913, 0.2988, 1.2528]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1183, -0.1006]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Q-values: tensor([[0.2116],
        [0.2003],
        [0.0600]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1580]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1304, -0.0973]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Q-values: tensor([[ 0.1214],
        [ 0.0751],
        [-0.2172]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1049]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1304, -0.0973]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Q-values: tensor([[-0.0240],
        [-0.0237],
        [-0.1886]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1328]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8823,
         2.3919, 0.1805, 1.3534]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1304, -0.0973]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Q-values: tensor([[0.2106],
        [0.1992],
        [0.0533]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1635]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1371, -0.0949]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Q-values: tensor([[ 0.1204],
        [ 0.0740],
        [-0.2355]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1204]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1371, -0.0949]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Q-values: tensor([[-0.0253],
        [-0.0252],
        [-0.2031]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1560]])
States: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7519,
         2.4892, 0.0501, 1.4507]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1371, -0.0949]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Q-values: tensor([[0.2095],
        [0.1980],
        [0.0461]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1616]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1350, -0.0914]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Q-values: tensor([[ 0.1194],
        [ 0.0730],
        [-0.2473]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0251],
        [-0.1307]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1350, -0.0914]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Q-values: tensor([[-0.0266],
        [-0.0265],
        [-0.2107]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1447],
        [-0.1682]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.6148,  2.5815, -0.0870,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1350, -0.0914]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Q-values: tensor([[0.2085],
        [0.1969],
        [0.0364]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.0656],
        [0.1597]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1328, -0.0892]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Q-values: tensor([[ 0.1184],
        [ 0.0721],
        [-0.2509]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1093],
        [-0.1325]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1328, -0.0892]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Q-values: tensor([[-0.0279],
        [-0.0279],
        [-0.2101]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.0770],
        [-0.1957]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.4799,  2.5815, -0.2219,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.0606,  0.1236],
        [ 0.1328, -0.0892]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Q-values: tensor([[0.2076],
        [0.1958],
        [0.0370]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1254],
        [0.1586]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3317,  0.2048],
        [ 0.0545, -0.0427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Q-values: tensor([[ 0.1174],
        [-0.2854],
        [-0.2502]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.0991],
        [-0.1438]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3317,  0.2048],
        [ 0.0545, -0.0427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Q-values: tensor([[-0.0291],
        [-0.1937],
        [-0.2294]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1277],
        [-0.2074]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.6024,  0.0000,  0.0000,  0.0000,
          1.9495,  2.5815,  0.2476,  1.5430],
        [ 0.0000,  0.0000, -0.6024,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          1.3471,  2.5815, -0.3548,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3317,  0.2048],
        [ 0.0545, -0.0427]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Q-values: tensor([[ 0.2066],
        [-0.0027],
        [ 0.0260]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1314],
        [0.1633]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3013,  0.1640],
        [ 0.1133, -0.0676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Q-values: tensor([[ 0.1166],
        [-0.2610],
        [-0.2502]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1012],
        [-0.1540]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3013,  0.1640],
        [ 0.1133, -0.0676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Q-values: tensor([[-0.0302],
        [-0.2138],
        [-0.2260]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.1726],
        [-0.2227]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3252, -0.2048,  0.0000,  0.0000,
          1.6178,  2.3767, -0.0840,  1.3382],
        [ 0.0000,  0.0000, -0.3252,  0.2048,  0.0000,  0.0000,  0.0000,  0.0000,
          1.2926,  2.5815, -0.4093,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.3013,  0.1640],
        [ 0.1133, -0.0676]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Q-values: tensor([[0.2058],
        [0.0012],
        [0.0313]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1330],
        [0.1655]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2905,  0.1359],
        [ 0.1541, -0.0918]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Q-values: tensor([[ 0.1160],
        [-0.2444],
        [-0.2456]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1026],
        [-0.1668]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2905,  0.1359],
        [ 0.1541, -0.0918]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Q-values: tensor([[-0.0312],
        [-0.2284],
        [-0.2280]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2065],
        [-0.2369]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1372, -0.3688,  0.0000,  0.0000,
          1.3165,  2.2127, -0.3853,  1.1742],
        [ 0.0000,  0.0000, -0.1372,  0.3688,  0.0000,  0.0000,  0.0000,  0.0000,
          1.1793,  2.5815, -0.5225,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2905,  0.1359],
        [ 0.1541, -0.0918]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Q-values: tensor([[ 0.2052],
        [-0.0013],
        [ 0.0347]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1280],
        [0.1575]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2779,  0.1054],
        [ 0.1839, -0.1133]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Q-values: tensor([[ 0.1154],
        [-0.2238],
        [-0.2427]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1139],
        [-0.1826]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2779,  0.1054],
        [ 0.1839, -0.1133]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Q-values: tensor([[-0.0322],
        [-0.2302],
        [-0.2308]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2306],
        [-0.2545]])
States: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.0978e-04,
         -5.0470e-01,  0.0000e+00,  0.0000e+00,  1.0260e+00,  2.0768e+00,
         -6.7581e-01,  1.0383e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0978e-04,  5.0470e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0252e+00,  2.5815e+00,
         -6.7662e-01,  1.5430e+00]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2779,  0.1054],
        [ 0.1839, -0.1133]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Q-values: tensor([[0.2047],
        [0.0002],
        [0.0344]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1148],
        [0.1475]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2739,  0.0849],
        [ 0.2104, -0.1319]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Q-values: tensor([[ 0.1149],
        [-0.2098],
        [-0.2413]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1458],
        [-0.1978]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2739,  0.0849],
        [ 0.2104, -0.1319]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Q-values: tensor([[-0.0332],
        [-0.2257],
        [-0.2359]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2471],
        [-0.2741]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0933, -0.6101,  0.0000,  0.0000,
          0.7481,  1.9715, -0.9537,  0.9330],
        [ 0.0000,  0.0000,  0.0933,  0.6101,  0.0000,  0.0000,  0.0000,  0.0000,
          0.8413,  2.5815, -0.8605,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2739,  0.0849],
        [ 0.2104, -0.1319]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Q-values: tensor([[ 0.2042],
        [-0.0054],
        [ 0.0362]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1046],
        [0.1360]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2671,  0.0710],
        [ 0.2295, -0.1511]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Q-values: tensor([[ 0.1145],
        [-0.1972],
        [-0.2391]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.1755],
        [-0.2088]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2671,  0.0710],
        [ 0.2295, -0.1511]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Q-values: tensor([[-0.0341],
        [-0.2188],
        [-0.2409]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.2803],
        [-0.2922]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1568, -0.6949,  0.0000,  0.0000,
          0.4741,  1.8866, -1.2277,  0.8481],
        [ 0.0000,  0.0000,  0.1568,  0.6949,  0.0000,  0.0000,  0.0000,  0.0000,
          0.6310,  2.5815, -1.0709,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2671,  0.0710],
        [ 0.2295, -0.1511]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Q-values: tensor([[ 0.2038],
        [-0.0078],
        [ 0.0363]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1061],
        [0.1250]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2542,  0.0633],
        [ 0.2468, -0.1706]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Q-values: tensor([[ 0.1141],
        [-0.1964],
        [-0.2393]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2005],
        [-0.2234]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2542,  0.0633],
        [ 0.2468, -0.1706]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Q-values: tensor([[-0.0351],
        [-0.2269],
        [-0.2388]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3123],
        [-0.3084]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1945, -0.7660,  0.0000,  0.0000,
          0.2070,  1.8156, -1.4948,  0.7771],
        [ 0.0000,  0.0000,  0.1945,  0.7660,  0.0000,  0.0000,  0.0000,  0.0000,
          0.4015,  2.5815, -1.3003,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2542,  0.0633],
        [ 0.2468, -0.1706]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Q-values: tensor([[0.2035],
        [0.0015],
        [0.0354]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1178],
        [0.1169]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2294,  0.0614],
        [ 0.2593, -0.1912]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Q-values: tensor([[ 0.1136],
        [-0.2007],
        [-0.2387]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2197],
        [-0.2464]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2294,  0.0614],
        [ 0.2593, -0.1912]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Q-values: tensor([[-0.0362],
        [-0.2365],
        [-0.2301]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3429],
        [-0.3284]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2019, -0.8293,  0.0000,  0.0000,
         -0.0472,  1.7523, -1.7490,  0.7138],
        [ 0.0000,  0.0000,  0.2019,  0.8293,  0.0000,  0.0000,  0.0000,  0.0000,
          0.1547,  2.5815, -1.5471,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2294,  0.0614],
        [ 0.2593, -0.1912]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Q-values: tensor([[0.2033],
        [0.0229],
        [0.0425]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1262],
        [0.1291]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2077,  0.0572],
        [ 0.2725, -0.2125]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Q-values: tensor([[ 0.1131],
        [-0.2021],
        [-0.2380]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2310],
        [-0.2733]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2077,  0.0572],
        [ 0.2725, -0.2125]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Q-values: tensor([[-0.0374],
        [-0.2525],
        [-0.2203]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3700],
        [-0.3703]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1720, -0.8907,  0.0000,  0.0000,
         -0.2766,  1.6909, -1.9785,  0.6523],
        [ 0.0000,  0.0000,  0.1720,  0.8907,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.1046,  2.5815, -1.8064,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.2077,  0.0572],
        [ 0.2725, -0.2125]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Q-values: tensor([[0.2031],
        [0.0405],
        [0.0602]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1295],
        [0.1651]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1934,  0.0614],
        [ 0.2882, -0.2169]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Q-values: tensor([[ 0.1125],
        [-0.2003],
        [-0.2437]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2446],
        [-0.3002]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1934,  0.0614],
        [ 0.2882, -0.2169]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Q-values: tensor([[-0.0387],
        [-0.2669],
        [-0.2279]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.3973],
        [-0.4245]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1072, -0.9479,  0.0000,  0.0000,
         -0.4843,  1.6337, -2.1861,  0.5952],
        [ 0.0000,  0.0000,  0.1072,  0.9479,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.3771,  2.5815, -2.0790,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1934,  0.0614],
        [ 0.2882, -0.2169]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Q-values: tensor([[0.2030],
        [0.0527],
        [0.1004]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1298],
        [0.2055]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1768,  0.0680],
        [ 0.3082, -0.2254]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1190, -1.0773,  0.0000,  0.0000,
         -0.8544,  1.5043, -2.5563,  0.4658],
        [ 0.0000,  0.0000, -0.1190,  1.0773,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9735,  2.5815, -2.6753,  1.5430]])
Q-values: tensor([[ 0.1118],
        [-0.2005],
        [-0.2542]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.0251],
        [-0.2590],
        [-0.3290]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1768,  0.0680],
        [ 0.3082, -0.2254]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1190, -1.0773,  0.0000,  0.0000,
         -0.8544,  1.5043, -2.5563,  0.4658],
        [ 0.0000,  0.0000, -0.1190,  1.0773,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9735,  2.5815, -2.6753,  1.5430]])
Q-values: tensor([[-0.0403],
        [-0.2853],
        [-0.2552]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[-0.1447],
        [-0.4277],
        [-0.4816]])
States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0124, -1.0093,  0.0000,  0.0000,
         -0.6777,  1.5723, -2.3795,  0.5338],
        [ 0.0000,  0.0000,  0.0124,  1.0093,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.6653,  2.5815, -2.3671,  1.5430]])
Actions: tensor([[-0.1554,  0.0193],
        [ 0.1768,  0.0680],
        [ 0.3082, -0.2254]])
Rewards: tensor([[0.],
        [0.],
        [0.]])
Next States: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1190, -1.0773,  0.0000,  0.0000,
         -0.8544,  1.5043, -2.5563,  0.4658],
        [ 0.0000,  0.0000, -0.1190,  1.0773,  0.0000,  0.0000,  0.0000,  0.0000,
         -0.9735,  2.5815, -2.6753,  1.5430]])
Q-values: tensor([[0.2029],
        [0.0596],
        [0.1501]], grad_fn=<AddmmBackward0>)
Target Q-values: tensor([[0.0656],
        [0.1314],
        [0.2418]])
